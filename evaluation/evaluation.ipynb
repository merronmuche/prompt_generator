{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation with RAGAS and Advanced Retrieval Methods Using LangChain\n",
    "API connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "# openai.api_key = getpass(\"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\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "use papers from Arxiv with the ArxivLoader document loader from LangChain.\n",
    "\n",
    "Let's grab and load 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
    "len(base_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Survey on Retrieval-Augmented Text Generation\\nHuayang Li♥,∗\\nYixuan Su♠,∗\\nDeng Cai♦,∗\\nYan Wang♣,∗\\nLemao Liu♣,∗\\n♥Nara Institute of Science and Technology\\n♠University of Cambridge\\n♦The Chinese University of Hong Kong\\n♣Tencent AI Lab\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community.\\nCompared\\nwith conventional generation models, retrieval-\\naugmented text generation has remarkable ad-\\nvantages and particularly has achieved state-of-\\nthe-art performance in many NLP tasks. This\\npaper aims to conduct a survey about retrieval-\\naugmented text generation. It ﬁrstly highlights\\nthe generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable ap-\\nproaches according to different tasks including\\ndialogue response generation, machine trans-\\nlation, and other generation tasks. Finally, it\\npoints out some promising directions on top of\\nrecent methods to facilitate future research.\\n1\\nIntroduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-\\nnology, has achieved state-of-the-art (SOTA) per-\\nformance in many NLP tasks and attracted the at-\\ntention of the computational linguistics community\\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\\n2021). Compared with generation-based counter-\\npart, this new paradigm has some remarkable ad-\\nvantages: 1) The knowledge is not necessary to be\\nimplicitly stored in model parameters, but is explic-\\nitly acquired in a plug-and-play manner, leading\\nto great scalibility; 2) Instead of generating from\\nscratch, the paradigm generating text from some re-\\ntrieved human-written reference, which potentially\\nalleviates the difﬁculty of text generation.\\nThis paper aims to review many representative\\napproaches for retrieval-augmented text generation\\ntasks including dialogue response generation (We-\\nston et al., 2018), machine translation (Gu et al.,\\n2018) and others (Hashimoto et al., 2018). We\\n∗All authors contributed equally.\\nﬁrstly present the generic paradigm of retrieval-\\naugmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-\\nnized with respect to different tasks. Speciﬁcally,\\non the dialogue response generation task, exem-\\nplar/template retrieval as an intermediate step has\\nbeen shown beneﬁcial to informative response gen-\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\net al., 2019a,b). In addition, there has been growing\\ninterest in knowledge-grounded generation explor-\\ning different forms of knowledge such as knowl-\\nedge bases and external documents (Dinan et al.,\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\n2021). On the machine translation task, we summa-\\nrize the early work on how the retrieved sentences\\n(called translation memory) are used to improve\\nstatistical machine translation (SMT) (Koehn et al.,\\n2003) models (Simard and Isabelle, 2009; Koehn\\nand Senellart, 2010) and in particular, we inten-\\nsively highlight several popular methods to inte-\\ngrating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications\\nof retrieval-augmented generation in other genera-\\ntion tasks such as abstractive summarization (Peng\\net al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm\\nIn this section, we ﬁrst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-\\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\\nInput\\nSources \\n(Sec. 2.2):\\nTraining \\nCorpus\\nExternal Data\\nUnsupervised \\nData\\nMetrics\\n(Sec. 2.3):\\nSparse-vector \\nRetrieval\\nDense-vector \\nRetrieval\\nTask-specific \\nRetrieval\\nRetrieval Memory\\nGeneration Model\\nSec. 4: Machine \\nTranslation\\nSec. 5: Other \\nTasks\\nData \\nAugmentation\\nAttention \\nMechanism\\nSkeleton & \\nTemplates\\nInformation Retrieval\\nTasks:\\nSec. 3: Dialogue \\nGeneration\\nModels \\n(Sec 2.4):\\nOutput\\nFigure 1: The overview of this survey.\\ntrieval source, retrieval metric and integration meth-\\nods.\\n2.1\\nFormulation\\nMost text generation tasks can be formulated as a\\nmapping from input sequence x to output sequence\\ny : y = f(x). For instance, x and y could be the\\ndialogue history and the corresponding response\\nfor dialogue response generation, the text in the\\nsource language and the translation in the target\\nlanguage for machine translation, and so on.\\nRecently, some researchers suggest to endow\\nmodels the capability to access external memory\\nvia some information retrieval techniques, so that\\nthey can acquire more information in the generation\\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\\net al., 2019b). The retrieval-augmented generation\\ncan be further formulated as:\\ny = f(x, z)\\n(1)\\nwhere z = {⟨xr, yr⟩} is a set of relevant instances\\nretrieved from the original training set or external\\ndatasets. The main idea of this paradigm is that yr\\nmay beneﬁt the response generation, if xr (or yr)\\nis similar (or relevant) to the input x. It is worth\\nnoting that xr = ∅ when unsupervised retrieval\\nsources are used. In general, the retrieval mem-\\nory can be retrieved from three kinds of sources:\\nthe training corpus, external datasets in the same\\nformat with the training corpus, and large-scale\\nunsupervised corpus (§2.2). Metrics that evaluate\\nthe relevance between text are varied as well, in\\n§2.3 we divided them into three categories: sparse-\\nvector retrieval, dense-vector retrieval, and training-\\nbased retrieval. Finally, how to integrate the re-\\ntrieval memory to the generation model is also sig-\\nniﬁcant, we also introduce some popular integra-\\ntion approaches in §2.4.\\n2.2\\nRetrieval Sources\\nTraining Corpus\\nMost previous studies search\\nthe external memory from its training corpus (Song\\net al., 2016; Gu et al., 2018; Weston et al., 2018).\\nIn the inference time, retrieved examples with high\\nrelevant scores could be regarded as extra refer-\\nences and reduce model’s uncertainty in generation.\\nThe main motivation of those works is to to store\\nknowledge not only in the model parameters but\\nalso in an explicit and accessible form, making the\\nmodel be able to re-access it during inference.\\nExternal Data\\nSome researchers also propose to\\nretrieval relevant samples from external datasets\\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\\nies, the retrieval pool is different with the training\\ncorpus, which can further provide additional infor-\\nmation that are not contained in the training corpus.\\nThis is especially beneﬁcial for applications such\\nas domain adaptation and knowledge update. For\\nexample, Khandelwal et al. (2020a); Zheng et al.\\n(2021a) employ the in-domain dataset as the exter-\\nnal memory to achieve fast domain adaptation for\\nmachine translation.\\nUnsupervised Data\\nOne limitation for previous\\ntwo sources is that the datasets have to be super-\\nvised datasets consisting of aligned input-output\\npairs. For machine translation, Cai et al. (2021) pro-\\npose a cross-lingual retriever to directly retrieve tar-\\nget sentence from unsupervised corpus (i.e., mono-\\nlingual corpus in the target language). The main\\nidea is aligning source-side sentences and the corre-\\nsponding target-side translations in a dense vector\\nspace, i.e., aligning x and yr when xr is absent.\\nAs a result, the retriever directly connects the dots\\nbetween the source-side input and target-side trans-\\nlations, enabling monolingual data in the target\\nlanguage to be used alone as memories.\\n2.3\\nRetrieval Metrics\\nSparse-vector Retrieval\\nGiven an input se-\\nquence x and a retrieval corpus, retrieval model\\naims to retrieve a set of relevant examples z =\\n{⟨xr, yr⟩} from the corpus. When a supervised\\ncorpus is used, {⟨xr, yr⟩} is retrieved by measur-\\ning the similarity between x and xr. For simi-\\nlarity measurement, sparse-vector retrieval meth-\\nods such as TF-IDF and BM25 (Robertson and\\nZaragoza, 2009) are widely used. They match key-\\nwords efﬁciently with an inverted index.\\nDense-vector Retrieval\\nHowever, these meth-\\nods prefer examples with similar surfaces, and may\\nfail to retrieve examples that are only semantically\\nrelevant. To alleviate above problem, some stud-\\nies (Cao and Xiong, 2018) attempt to retrieve in\\ndense-vector space instead of the lexical overlap.\\nRecent work (Lee et al., 2019) makes use of pre-\\ntrained language models, which encodes the text to\\nlow-dimensional dense vectors via BERT-based en-\\ncoders. The retrieval score are computed via inner\\nproducts between vectors.\\nTask-speciﬁc\\nRetrieval\\nSimilarity-based\\nre-\\ntrieval is based on a simple heuristic. That is, the\\nmore xr resembles with x, the more likely xr\\nand yr will help the generation. However, the\\nmost similar one by universal textual similarity\\ndoes not necessarily serve the best for downstream\\nmodels.\\nIdeally, the retrieval metric would be\\nlearned from the data in a task-dependent way: we\\nwish to consider a memory only if it can indeed\\nboost the quality of ﬁnal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciﬁc objectives.\\n2.4\\nIntegration\\nData Augmentation\\nThere are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation, which constructs some augmented inputs\\nby concatenating spans from {⟨xr, yr⟩} with the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efﬁciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).\\nAttention\\nMechanisms\\nAnother\\nintegration\\nmethod\\nis\\nbased\\non\\nattention\\nmechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various\\narchitectures) to encode retrieved target sentences,\\nand integrate them through attention (Cao and\\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\\n2019). Since the attention mechanism is becoming\\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\\nkey module in lots of NLP models, integrating\\nretrieved memory through attention becomes a\\nvery nature and efﬁcient way.\\nSkeleton Extraction\\nIn the previous two meth-\\nods, the downstream generation model learns how\\nto ﬁlter out irrelevant or even harmful informa-\\ntion from the retrieved examples implicitly. There\\nalso exist some works that try to explicitly extract\\nuseful information, i.e., skeleton extraction, from\\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\\n2019; Cai et al., 2019b). For example, one skeleton\\nshould be a part of a whole utterance with irrelevant\\ncontent masked, and the generation model only in-\\ntegrate this skeleton in the generation process.\\n3\\nDialogue Response Generation\\nBackground\\nDialogue systems can be grouped\\ninto two categories: chit-chat systems and task-\\noriented systems. While task-oriented dialogue\\nsystems are designed to accomplish speciﬁc user\\ntasks such as air tickets booking, chit-chat dialogue\\nsystems aim at giving a meaningful and ﬂuent re-\\nsponse for any dialogue history in the open domain.\\nDialogue response generation in chit-chat dialogue\\nsystem is challenging partly due to the diversity\\nof possible responses to a single dialogue history\\n(i.e., the one-to-many problem). The dialogue his-\\ntory alone cannot decide a meaningful and speciﬁc\\nresponse. Also, external knowledge that is not\\npresent in the dialogue history are often necessary\\nfor avoiding safe but boring responses. We focus\\non recent efforts tackling the challenges to develop\\nchit-chat dialogue systems.\\nMost modern chit-chat dialogue systems can\\nbe categorized into two classes, namely, retrieval-\\nbased models and generation-based models. The\\nretrieval-based models (Ji et al., 2014; Hu et al.,\\n2014) directly copy an existing response from cu-\\nrated dialogue corpora (i.e., the retrieval pool)\\nwhen receiving a response request. The retrieved\\nresponses are often informative and grammatical\\nas they are collected from real-world conversa-\\ntions and possibly post-edited by a human. How-\\never, such systems perform poorly when a given\\ndialogue history is substantially different from\\nthose in the retrieval pool. On the other hand,\\nthe generation-based models (Shang et al., 2015;\\nVinyals and Le, 2015; Li et al., 2016a) generate\\na new utterance from scratch. Those generation-\\nbased models have better generalization capacity\\nwhen handling unseen dialogue contexts. Never-\\ntheless, the generated utterances are inclined to be\\ndull and non-informative (e.g., “I don’t know”, “I\\nthink so”, “Me too” etc.) (Li et al., 2016a).\\nShallow Integration\\nAs discussed, retrieval-\\nbased models may give informative but inappro-\\npriate responses while generation-based models\\noften do the opposite. It is desirable to combine the\\nbest of both worlds. Early work (Qiu et al., 2017)\\nattempts to re-rank the output from both models.\\nFor a deep integration, Song et al. (2016) and Yang\\net al. (2019) extend the standard SEQ2SEQ encoder-\\ndecoder model (Bahdanau et al., 2014) with an ex-\\ntra encoder for encoding the retrieval result. The\\noutput of the extra encoder, along with the output\\nfrom the original encoder for dialogue history, is\\nused to feed the decoder. Weston et al. (2018) use\\na single encoder that takes the concatenation of\\nthe original dialogue history and the retrieved as\\ninput. Wu et al. (2019) note that the retrieved infor-\\nmation should be used in awareness of the context\\ndifference, and further proposed to construct an\\nedit vector by explicitly encoding the lexical differ-\\nences between the input dialogue history and the\\nretrieved dialogue history. Pandey et al. (2018) fur-\\nther propose to weight different training instances\\nby context similarity.\\nDeep Integration\\nTo prevent the inﬂow of er-\\nroneous information, Cai et al. (2019a) propose\\na general framework that ﬁrst extracts a skeleton\\nfrom the retrieved response and then generates the\\nresponse based on the extracted skeleton. This\\nframework is also adopted for stylistic response\\ngeneration (Su et al., 2021c). Gupta et al. (2021)\\nsuggest to use the semantic structure of an exem-\\nplar response, instead of the tokens of the exem-\\nplar response, to guide generation. Despite their\\ndifferences, a common issue is that the genera-\\ntion model easily learns to ignore the retrieved re-\\nsponse entirely and collapses to a vanilla seq2seq\\nmodel. This happens with improper training in-\\nstances. Due to the one-to-many nature, it hap-\\npens frequently that a retrieved response (extracted\\nskeleton) is suitable for responding to the query,\\nbut inconsistent with the current target response.\\nEarlier studies (Weston et al., 2018; Wu et al.,\\n2019; Cai et al., 2019a) alleviate the above prob-\\nlems by putting hard constraints on the data (e.g.,\\ndiscarding data with low similarity of the retrieved\\nresponse and the target response), which, however,\\ngreatly reduces the amount of usable data. Cai\\net al. (2019b) employ a random mechanism for\\ngenerating the skeletons used for training, which\\nextract skeletons from the corresponding responses\\nwith some deliberate disturbance. Paranjape et al.\\n(2021) propose to model the retriever after the pos-\\nterior distribution of retrieval given the input and\\nthe target output and train it jointly with the stan-\\ndard retriever and the generator by maximizing the\\nevidence lower bound (ELBo) in expectation over\\nretrieval.\\nKnowledge-Enhanced Generation\\nThe afore-\\nmentioned work demonstrates that retrieval-based\\ndialogue systems can be used for building bet-\\nter generation-based models. In general, this is\\ndone by conditioning the generation on some re-\\ntrieved responses. More traditionally, to infuse\\nthe response with external knowledge, the retrieval\\npool is not necessarily a dialogue corpus. In fact,\\nknowledge-grounded dialogue response generation\\nexploring different forms of knowledge such as\\nknowledge bases and external documents (Dinan\\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\\nZhang et al., 2021; Komeili et al., 2021) has been\\nactively explored.\\nLimitations\\nWe note that there are three major\\nlimitations in existing work for dialogue response\\ngeneration. First, current methods only use one\\nretrieved response for generation. It can be more\\nbeneﬁcial to combine multiple retrieval responses.\\nHowever, this can be difﬁcult due to the one-to-\\nmany nature of dialogue response generation. Sec-\\nond, current methods use universal relevance score\\nfor retrieval. It can be more effective if we can\\nuse more customized retrieval metric especially\\nfor controlled dialogue response generation (e.g.,\\npersona, emotion, etc). Third, the retrieval pool\\nof existing methods is limited to dialogue corpora\\n(context-response pairs) or documents. It might\\nbe useful to enlarge the retrieval pool by including\\nmore corpora in other domains or in other modali-\\nties. As discussed, there leaves plenty of possible\\ndirections to explore in the future.\\n4\\nMachine Translation\\nRetrieval augmented translation originates from hu-\\nman translation scenarios (Somers, 2003). When\\ntranslating ˆy from an input source sentence x, a hu-\\nman translator typically involves a search engine to\\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\\ngual database. Such a technique called translation\\nmemory is helpful to improve the translation qual-\\nity and efﬁciency for human translators (Dillon\\nand Fraser, 2006). As the development of ma-\\nchine translation techniques, there is a surge of\\ninterests in improving machine translation models\\nwith translation memory. In the rest of this section,\\nwe will review translation memory for both statisti-\\ncal machine translation (SMT) and neural machine\\ntranslation (NMT).\\n4.1\\nTranslation Memory in SMT\\nGenerally, SMT includes three key components in\\na pipeline manner such as phrase table extraction,\\nparameter tuning and decoding (Koehn et al., 2003;\\nChiang, 2007). As a result, many efforts have been\\nmade to make use of translation memory (TM) on\\ntop of each component.\\nConstrained Decoding with TM\\nConstrained\\ndecoding is the most straightforward way to in-\\ntegrating TM into SMT (Smith and Clark, 2009;\\nKoehn and Senellart, 2010; Zhechev and Van Gen-\\nabith, 2010; Ma et al., 2011). Its basic idea is\\nto reuse the useful segments in yr while trans-\\nlate other segments by SMT. Speciﬁcally, the ap-\\nproach consists of three steps: 1) identify the un-\\nmatched segments in both xr and x through the\\nedit-distance algorithm; 2) identify the unmatched\\nsegments in yr, each of which is aligned to one\\nunmatched segment in xr by a word alignment\\nalgorithm; 3) decode each unmatched segment in\\nx by SMT and then use the result to replace its\\ncorresponding unmatched segment in yr. Li et al.\\n(2016b) further extend this approach from sentence\\nlevel to phrase level. The advantage in constrained\\ndecoding is that it does not require to change the\\ntranslation model (including phrase table and pa-\\nrameters) and can be applied in a plug-and-play\\nway. This approach is successful when x is highly\\nsimilar to xr; otherwise its performance is de-\\ngraded largely, because it explicitly isolates TM\\nmatching and SMT decoding and reuses the results\\nin xr or not in a deterministic way.\\nPhrase Table Aggregation with TM\\nThere are\\nalso notable efforts to augment the phrase table\\nfor SMT by extracting translation rules from the\\nretrieved bilingual sentences {⟨xr, yr⟩}.\\nThen\\nthey re-tune the parameters for the SMT model\\nwhich makes use of translation knowledge from\\n{⟨xr, yr⟩} in a implicit way when translating x.\\nFor example, Biçici and Dymetman (2008); Simard\\nand Isabelle (2009) directly combine the extracted\\ntranslation rules into the phrase table in a shallow\\ncombination way. They introduce an additional fea-\\nture to indicate that whether translation rule is from\\n{⟨xr, yr⟩} or not and then train all feature weights\\nwith MERT (Och, 2003). One characteristic of\\nthese work is that a translation rule extracted from\\n{⟨xr, yr⟩} which can not exactly match any seg-\\nments in x is useless even if it may contain some\\nuseful words in its target side. To remedy this ob-\\nservation, Wang et al. (2013, 2014) resort to a deep\\ncombination way to using the extracted translation\\nrules. For each rule in the phrase table, it designs\\na generative model to reward the rules which are\\nsimilar to those extracted from {⟨xr, yr⟩}. Then\\nthis generative model is used as a feature in the log-\\nlinear based SMT model whose weight is tuned\\ntogether with other features by MERT. In addition,\\nLi et al. (2014) employ a similar way to reward\\nthe rules but it relies on a discriminative model\\nwhich is easy to integrate potential features from\\n{⟨xr, yr⟩}.\\nParameter Tuning with TM\\nUnlike the above\\ntwo research lines, Liu et al. (2012, 2014) make use\\nof translation memory only in tuning parameters.\\nTo be speciﬁc, when translating an input sentence\\nx, they ﬁrstly retrieve many similar bilingual sen-\\ntences {⟨xr, yr⟩}, and then tune the parameters on\\ntop of the retrieved sentences as well as a given de-\\nvelopment dataset in a sentence-wise manner, i.e.,\\nit performs an independent tuning for each input\\nsentence. To improve the efﬁciency of each tuning\\nstep, it propose a local update on top of {⟨xr, yr⟩}\\nfrom a baseline model.\\nDespite the successes of translation memory in\\nSMT, there are still some limitations for the above\\nthree kinds of methods. Firstly, all these methods\\nemploy fuzzy score for retrieval which is highly de-\\npendent on word matching and thus can not recall\\nsuch examples which are similar in word seman-\\ntics but different in surface form. Secondly, these\\nmethods integrate the retrieved examples into a\\nmodule of SMT in the ways which can not make\\nfull use of the knowledge in retrieved examples.\\nFor example, the integration ways in the ﬁrst two\\nkinds (constrained decoding and phrase table ag-\\ngregation) are heuristic and not optimized towards\\ntranslation quality; the parameter tuning method\\nﬁne-tunes few parameters for log-linear based SMT\\nwhich are not enough to preserve sufﬁcient knowl-\\nedge from retrieved examples. Thirdly, since SMT\\nperforms in a pipeline manner, it is intractable to\\njointly optimize retrieval metrics as well as SMT\\nmodels. Consequently, all these methods adopt an\\noff-the-shelf metric for retrieval, leading to sub-\\noptimal performance.\\n4.2\\nTranslation Memory in NMT\\nTranslation memory has been widely explored in\\nNeural Machine Translation (NMT). Depending\\non when retrieval is involved, we can categorize\\nprevious works into two classes: 1) an NMT model\\nleans how to cooperate with the retrieval model in\\nthe training phase; 2) an NMT model is only aware\\nof the retrieved data in the inference phase.\\nInference Phase\\nThe key point of literature in\\nthis line is to reward some target words based on\\nwords in yr in the inference process. Thus, a de-\\ncision can be made based on both the distribution\\nof generation model and the additional reward of\\nretrieval model. Some previous works propose to\\nreward target words based on the sentence-level\\nsimilarity between x and xr, and the word align-\\nment between xr and yr. Given the input sentence\\nx, Zhang et al. (2018) try to assign target words\\nin ˆy with higher rewards, when they appear in yr\\nand the aligned source words are in both xr and\\nx. He et al. (2019) follow a similar framework\\nand consider the position information of those tar-\\nget words when rewarding. Those works reward\\nthe target words in an explicit way, however, the\\none-sentence-one-model approach (Li et al., 2016c;\\nTurchi et al., 2017) propose to reward target word\\nimplicitly. For each testing input x, their approach\\nwill ﬁrst ﬁnetune the translation model on retrieved\\nmemory {⟨xr, yr⟩} and then translate x.\\nOthers try to reward target words based on token-\\nlevel similarity score. Most works in this line are\\nbased on the dense retriever (Khandelwal et al.,\\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\\na key-value datastore, where key h(xr, yr\\n<t) is the\\nhidden state at each time step when translating yr\\nfrom xr, and value is its golden-truth target word\\nyr\\nt. Therefore, in the inference time, they can use\\nthe h(x, ˆy<t) as query and reward target words\\nwith similar hidden representations in the datas-\\ntore. Although this method achieves signiﬁcant\\nperformance gain, one drawback of it is the high la-\\ntency. To address this issue, Meng et al. (2021) use\\nsome heuristics, e.g., pre-ﬁltering, to avoid search-\\ning on the entire datastore. The reward score of\\nprevious works is got from some non-parametric\\napproaches, however, Zheng et al. (2021a) propose\\na light-weight network to learn the reward score.\\nSince dense retrieval has the potential of cross-\\nlingual retrieval, Zheng et al. (2021b) use a similar\\napproach to achieve unsupervised domain adapta-\\ntion, where a main change is to create the datastore\\nbased on synthetic sources sentence and the real\\ntarget sentences.\\nTraining Phase\\nDifferent from those model-\\nagnostic approaches, previous works in this line\\naim to train the generation model to learn how\\nto cooperate with the retrieval model. It is also\\nworth noting that most works in this line adopt\\nthe sentence-level retrieval, when integrating the\\nretrieval information in the training process. To\\nachieve its goal, Bulte and Tezcan (2019) and\\nHossain et al. (2020) propose a data augmenta-\\ntion method to integrate the retrieved information,\\nwhere x is concatenated with yr before feeding\\ninto the model . Following the data augmentation\\napproach, Xu et al. (2020) propose more matching\\nmethods to determine including which retrieved\\nexample in the source is better.\\nThere also exist some works that propose new\\narchitectures to integrate the retrieval information.\\nUnder the RNN-based framework, Cao and Xiong\\n(2018) and Gu et al. (2018) use the gating and at-\\ntention mechanism to incorporate the retrieved tar-\\nget sentences. When Transformer (Vaswani et al.,\\n2017) becomes the backbone of NMT, some works\\nalso use additional transformer encoders to en-\\ncode retrieved target sentences, and integrate them\\nthrough attention mechanism (Bapna and Firat,\\n2019; Cao et al., 2019). Xia et al. (2019) repre-\\nsent the retrieved target sentences in a different\\ndata structure, i.e., a graph structure, and integrate\\nit through attention mechanism. He et al. (2021)\\npropose a light-weight method to encode the re-\\ntrieved target sentences and leverage the alignment\\ninformation to ﬁlter out irrelevant information. Dif-\\nferent from previous works that rely on bilingual\\nmemories, Cai et al. (2021) propose a framework\\nthat can retrieve the most similar target sentence in\\na monolingual dataset, using a source sentence as\\nquery.\\nLimitations\\nIn the section of SMT, we have\\nshowed some limitations of the retrieval augmented\\napproaches. There also exist some limitations in\\nthe line of NMT. First, the information used for\\nderiving reward scores is limited. The similarity\\nbetween an input and retrieved examples is the\\nprimary feature to derive reward scores.\\nHow-\\never, some information, e.g., frequencies of words\\nand context, may also be beneﬁcial for integrating\\nthe translation memory. Second, it remains to be\\nan open question that when should we use the re-\\ntrieved information and when not. In the inference\\nphase, approaches tend to integrate the translation\\nmemory excessively, e.g., at each time step, which\\nnot only reduces the translation efﬁciency but may\\nalso dampen the ﬂuency of generated results.\\n5\\nOther Tasks\\nIn addition to dialogue system and machine trans-\\nlation, retrieval-augmented generation techniques\\nhave shown to be beneﬁcial in many other tasks. In\\nthe following, we highlight several key tasks that\\napply retrieval-augmented generation approaches.1\\nLanguage Modelling\\nIt has been shown that\\nproperly leveraging information from retrieval\\nmemory could improve the performance of large\\npre-trained language model. To build a more accu-\\nrate language model, Khandelwal et al. (2020b) pro-\\npose to incorporate a soft memory module into the\\nsystem. Speciﬁcally, an index is built by caching\\nthe hidden states of the training corpus. Then, the\\nlanguage model accesses the index via k-NN search\\nand displays a greatly improved performance. As\\nanother example, Guu et al. (2020) propose a new\\nparadigm that applies retrieval-augmented tech-\\nnique into the pre-training of generative language\\nmodel. During learning, they train a neural se-\\nlector that dynamically samples a relevant text to\\nguide the reconstruction of a corrupted input se-\\nquence. In this way, the pre-trained model deliv-\\ners better results by explicitly grounding on the\\nretrieval memory. Lewis et al. (2020a) combine\\nlanguage model pre-training with a paraphrasing\\n1Here, we focus on tasks other than question answering.\\nWe refer readers interested in QA to Chen and Yih (2020).\\napproach. During learning, an input sequence to\\nthe model is ﬁrst corrupted. In the meantime, a set\\nof multi-lingual texts are retrieved based on which\\nthe model learns to reconstruct the original input\\nsequence. Recently, Borgeaud et al. (2021) pro-\\npose RETRO, a large pre-trained language model\\nenhanced with retrieved documents, and obtained\\ncomparable performances with GPT-3 using 25×\\nfewer parameters.\\nSummarization\\nText summarization is another\\nresearch\\narea\\nthat\\nbeneﬁts\\nfrom\\nretrieval-\\naugmented text generation.\\nPeng et al. (2019)\\npropose an adaptive decoding framework which\\nﬁrst retrieves an exemplar document given the\\nsource document. Then, the summarization of the\\nsource document is derived through an adaptive\\ngeneration process based on the retrieved template.\\nDifferent from Peng et al. (2019), Cao et al.\\n(2018) and Hossain et al. (2020) introduce an\\nintermediate re-ranking stage into the generation\\npipeline.\\nSpeciﬁcally, before generating the\\ndocument summary, the retrieval documents are\\nﬁrst re-ranked based on their similarity scores\\nwith respect to the source document. Then, the\\ndocument summarization is produced by re-writing\\nthe selected templates.\\nParaphrase Generation\\nTo address the lack of\\nquality as well as diversity in the generation of para-\\nphrases, Kazemnejad et al. (2020) propose a gen-\\neration framework which ﬁrst retrieves a sentence\\nthat is similar to input sentence. Then, based on\\nthe retrieved sentence, a neural editor produces the\\nresulting paraphrased sentence. Chen et al. (2019)\\ninvestigate a different aspect of paraphrasing, i.e.\\nhow to control the linguistic syntax displayed in\\nthe generated text. To achieve this goal, Chen et al.\\n(2019) propose to ﬁrst extract a sentential exem-\\nplar that serves as the syntax template. A neural\\nmodel then generates the paraphrase with desired\\nlinguistic syntax following the retrieved exemplar.\\nText Style Transfer\\nTo improve the quality of\\ngenerated text, Li et al. (2018) propose a retrieval-\\naugmented framework which ﬁrst retrieves texts\\nthat are similar to the input based on lexical-level\\nsimilarity. Then, the retrieved tokens that are irrel-\\nevant to the source are deleted, and the output is\\nderived from the edited template. Xiao et al. (2021)\\nalso adopte this framework by incorporating re-\\ntrieval information from two sources (i.e. sparse\\nand dense memories) and obtained an improved\\nmodel performance.\\nData-to-Text Generation\\nRecently, retrieval-\\naugmented generation has been adapted to the task\\nof data-to-text generation. To bridge the gap be-\\ntween the structured data and natural language\\ntext, Su et al. (2021a) propose a novel retrieval-\\naugmented framework.\\nSpeciﬁcally, given the\\nsource data, a set of candidate texts are ﬁrst re-\\ntrieved from a large unlabelled corpus. Then, a\\nneural selector is applied to measure the similari-\\nties between the source data and candidate texts,\\nand extract a set of more ﬁne-grained prototypes\\nfrom the candidates. Lastly, a generation model\\ntakes the prototypes as input to produce the text\\nthat describes the given structured data.\\nWhile retrieval-augmented generation has been\\nwidely explored in the NLP community, we sug-\\ngest that future research could extend this approach\\nto tasks that involve data from multiple modali-\\nties. For instance, with recent advancements in\\nimage-text retrieval (Jia et al., 2021; Radford et al.,\\n2021), the structural gap between images and texts\\nis largely bridged. Some early studies (Zhang et al.,\\n2020) have shown that information retrieved from\\nimages could improve the performance of neural\\nmachine translation model. Naturally, such meth-\\nods could be extended to other multi-modal tasks,\\nsuch as image captioning (Karpathy and Li, 2015).\\nA similar idea could also be applied to tasks be-\\nyond images, such as speech-to-text transcription\\n(Gales and Young, 2007).\\n6\\nFuture Directions\\nDespite the current success of retrieval augmented\\ntext generation, there is still a long way to go as\\ndiscussed in previous sections. We highlight some\\ndirections to facilitate the future research as fol-\\nlows:\\nRetrieval Sensitivity\\nThe performance of re-\\ntrieval augmented text generation is very sensitive\\nto the retrieval quality, i.e., the similarity between\\nthe query and the retrieved examples. Currently, re-\\ntrieval augmented text generation models perform\\nwell when the retrieved examples are very simi-\\nlar to the query. However, they are even worse\\nthan the generation models without retrieval when\\nthe retrieval examples are less similar. Therefore,\\nit would be important to exploit new methods to\\naddress such an issue on similarity.\\nRetrieval Efﬁciency\\nGenerally, if one enlarges\\nthe retrieval memory to some extent, it would be\\npossible to retrieve an example which is very simi-\\nlar to the query.Unfortunately, the downside is that\\nthe overall inference for the retrieval augmented\\ngeneration models is less efﬁcient due the consid-\\nerable retrieval overhead. In this sense, it is urgent\\nto consider some methods to trade off the retrieval\\nmemory size and retrieval efﬁciency, for example,\\ndata compression for the retrieval memory.\\nLocal vs. Global Optimization\\nTheoretically, it\\nseems promising to jointly learn retrieval metrics\\nand generation models. However, in practice, there\\nis an essential gap about the retrieval metric be-\\ntween the training and inference phrases. In the\\ntraining phase, the loss is locally back-propagated\\nto only a few retrieved examples while in the infer-\\nence phase the metric is globally conducted among\\nall examples in the memory. It would be interesting\\nto narrow such a gap when learning a better metric\\nfor generation tasks.\\nMulti-Modalities\\nWith recent advancement in\\nimage-text retrieval, directly associating images\\nwith relevant text becomes possible. This urges\\nresearchers to investigate the possibility of retrieval-\\nbased text generation in tasks that involve data from\\ndifferent modalities. One typical task is image\\ncaptioning. Beyond images, other tasks like speech-\\nto-text transcription could potentially beneﬁt from\\nretrieval-based generation methods as well.\\nDiverse & Controllable Retrieval\\nMost of the\\nexisting approaches adopt a universal metric for\\nretrieval, such as lexical similarities of sentences.\\nFuture work should explore how to use customized\\nmetrics for retrieval. This can be beneﬁcial for\\nmore controlled text generation. For example, in-\\nstances with emotions and styles may be more de-\\nsirable in the personalized dialogue generation, par-\\nallel data that contains speciﬁc terminologies is\\nmore helpful in machine translation, and so on. On\\nthe other hand, using a universal metric for retrieval\\nmay lead to the lack of diversity of the retrieval re-\\nsults. Collecting a diverse set of retrieval results\\ncan improve the coverage of useful information.\\nThus, considering multiple different metrics for re-\\ntrieval may lead to generation with higher quality\\nin the future.\\n7\\nConclusion\\nIn this paper, we surveyed recent approaches for\\nretrieval-augmented text generation. We reviewed\\nand summarized the development of different com-\\nponents of retrieval-augmented text generation in-\\ncluding retrieval metrics, retrieval sources, and in-\\ntegration paradigms. We gave in-depth discussions\\nwhen retrieval-augmented text generation comes to\\ndifferent applications including dialogue response\\ngeneration, machine translation, and other genera-\\ntion tasks. We also pointed out some future direc-\\ntions for retrieval-augmented text generation.\\nReferences\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2014.\\nNeural machine translation by jointly\\nlearning to align and translate.\\narXiv preprint\\narXiv:1409.0473.\\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\\nadaptation for neural machine translation. In Pro-\\nceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long and Short Papers), pages 1921–1931.\\nErgun Biçici and Marc Dymetman. 2008.\\nDynamic\\ntranslation memory: Using statistical machine trans-\\nlation to improve translation memory fuzzy matches.\\nIn International Conference on Intelligent Text Pro-\\ncessing and Computational Linguistics, pages 454–\\n465. Springer.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\\nSifre. 2021. Improving language models by retriev-\\ning from trillions of tokens. CoRR, abs/2112.04426.\\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\\npair: Integrating fuzzy matches into neural machine\\ntranslation. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 1800–1809.\\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\\naojiang Liu, Wai Lam, and Shuming Shi. 2019a.\\nSkeleton-to-response: Dialogue generation guided\\nby retrieval memory.\\nIn Proceedings of the 2019\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short\\nPapers), pages 1219–1228.\\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\\njiang Liu, and Shuming Shi. 2019b.\\nRetrieval-\\nguided dialogue response generation via a matching-\\nto-generation framework.\\nIn Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pages 1866–1875.\\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\\nLemao Liu. 2021. Neural machine translation with\\nmonolingual translation memory. In Proceedings of\\nthe 59th Annual Meeting of the Association for Com-\\nputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing\\n(Volume 1: Long Papers), pages 7307–7318, Online.\\nAssociation for Computational Linguistics.\\nQian Cao, Shaohui Kuang, and Deyi Xiong. 2019.\\nLearning to reuse translations: Guiding neural ma-\\nchine translation with examples.\\narXiv preprint\\narXiv:1911.10732.\\nQian Cao and Deyi Xiong. 2018.\\nEncoding gated\\ntranslation memory into neural machine translation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n3042–3047.\\nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\\n2018. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the\\n56th Annual Meeting of the Association for Com-\\nputational Linguistics, ACL 2018, Melbourne, Aus-\\ntralia, July 15-20, 2018, Volume 1: Long Papers,\\npages 152–161. Association for Computational Lin-\\nguistics.\\nDanqi Chen and Wen-tau Yih. 2020.\\nOpen-domain\\nquestion answering. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics: Tutorial Abstracts, pages 34–37, On-\\nline. Association for Computational Linguistics.\\nMingda Chen, Qingming Tang, Sam Wiseman, and\\nKevin Gimpel. 2019. Controllable paraphrase gen-\\neration with a syntactic exemplar. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers, pages\\n5972–5984. Association for Computational Linguis-\\ntics.\\nDavid Chiang. 2007. Hierarchical phrase-based trans-\\nlation. computational linguistics, 33(2):201–228.\\nSarah Dillon and Janet Fraser. 2006. Translators and\\ntm: An investigation of translators’ perceptions of\\ntranslation memory adoption. Machine Translation,\\n20(2):67–79.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2018. Wizard\\nof wikipedia: Knowledge-powered conversational\\nagents. arXiv preprint arXiv:1811.01241.\\nMark J. F. Gales and Steve J. Young. 2007. The applica-\\ntion of hidden markov models in speech recognition.\\nFound. Trends Signal Process., 1(3):195–304.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\\ntor OK Li. 2018. Search engine guided neural ma-\\nchine translation. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence, volume 32.\\nPrakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and\\nAmy Pavel. 2021. Controlling dialogue generation\\nwith semantic exemplars.\\nIn Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, pages 3018–3029, On-\\nline. Association for Computational Linguistics.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\naugmented language model pre-training.\\nCoRR,\\nabs/2002.08909.\\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\\nand Percy S Liang. 2018. A retrieve-and-edit frame-\\nwork for predicting structured outputs. In Advances\\nin Neural Information Processing Systems, pages\\n10052–10062.\\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\\nLemao Liu. 2021. Fast and accurate neural machine\\ntranslation with translation memory.\\nIn Proceed-\\nings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (Volume 1: Long Papers), pages 3170–3180.\\nQiuxiang He, Guoping Huang, Lemao Liu, and Li Li.\\n2019. Word position aware translation memory for\\nneural machine translation.\\nIn CCF International\\nConference on Natural Language Processing and\\nChinese Computing, pages 367–379. Springer.\\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\\nmoyer. 2020.\\nSimple and effective retrieve-edit-\\nrerank text generation. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics, pages 2532–2538.\\nBaotian Hu, Zhengdong Lu, Hang Li, and Qingcai\\nChen. 2014. Convolutional neural network architec-\\ntures for matching natural language sentences. In\\nNIPS, pages 2042–2050.\\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\\ninformation retrieval approach to short text conver-\\nsation. arXiv preprint arXiv:1408.6988.\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\\nParekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,\\nZhen Li, and Tom Duerig. 2021. Scaling up visual\\nand vision-language representation learning with\\nnoisy text supervision. In Proceedings of the 38th In-\\nternational Conference on Machine Learning, ICML\\n2021, 18-24 July 2021, Virtual Event, volume 139 of\\nProceedings of Machine Learning Research, pages\\n4904–4916. PMLR.\\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In IEEE Conference on Computer Vision and\\nPattern Recognition, CVPR 2015, Boston, MA, USA,\\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\\nSociety.\\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\\nMahdieh Soleymani Baghshah. 2020.\\nParaphrase\\ngeneration by learning how to edit from samples. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, pages 6010–\\n6021, Online. Association for Computational Lin-\\nguistics.\\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\\nZettlemoyer,\\nand Mike Lewis. 2020a.\\nNear-\\nest neighbor machine translation.\\narXiv preprint\\narXiv:2010.00710.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020b. Generaliza-\\ntion through memorization: Nearest neighbor lan-\\nguage models. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis Ababa,\\nEthiopia, April 26-30, 2020. OpenReview.net.\\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\\nStatistical phrase-based translation. In Proceedings\\nof the 2003 Human Language Technology Confer-\\nence of the North American Chapter of the Associa-\\ntion for Computational Linguistics, pages 127–133.\\nPhilipp Koehn and Jean Senellart. 2010. Convergence\\nof translation memory and statistical machine trans-\\nlation. In Proceedings of AMTA Workshop on MT\\nResearch and the Translation Industry, pages 21–31.\\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\\n2021.\\nInternet-augmented dialogue generation.\\narXiv preprint arXiv:2107.07566.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019.\\nLatent retrieval for weakly supervised\\nopen domain question answering.\\narXiv preprint\\narXiv:1906.00300.\\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\\n2020a. Pre-training via paraphrasing. In Advances\\nin Neural Information Processing Systems 33: An-\\nnual Conference on Neural Information Processing\\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\\nvirtual.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020b.\\nRetrieval-augmented gen-\\neration for knowledge-intensive nlp tasks.\\narXiv\\npreprint arXiv:2005.11401.\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\\nand Bill Dolan. 2016a. A diversity-promoting ob-\\njective function for neural conversation models. In\\nNAACL, pages 110–119.\\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\\nDelete, retrieve, generate: a simple approach to sen-\\ntiment and style transfer. In Proceedings of the 2018\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, NAACL-HLT 2018, New\\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\\n1 (Long Papers), pages 1865–1874. Association for\\nComputational Linguistics.\\nLiangyou Li, Andy Way, and Qun Liu. 2014.\\nA\\ndiscriminative framework of integrating translation\\nmemory features into smt.\\nIn Proceedings of the\\n11th Conference of the Association for Machine\\nTranslation in the Americas, volume 1, pages 249–\\n260.\\nLiangyou Li, Andy Way, and Qun Liu. 2016b. Phrase-\\nlevel combination of smt and tm using constrained\\nword lattice.\\nAssociation for Computational Lin-\\nguistics (ACL).\\nXiaoqing Li, Jiajun Zhang, and Chengqing Zong.\\n2016c. One sentence one model for neural machine\\ntranslation. arXiv preprint arXiv:1609.06490.\\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\\nQian Li, and Jie Zhou. 2019.\\nIncremental trans-\\nformer with deliberation decoder for document\\ngrounded conversations. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics, pages 12–21.\\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\\nand Hua Wu. 2019. Learning to select knowledge\\nfor response generation in dialog systems.\\narXiv\\npreprint arXiv:1902.04911.\\nLemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,\\nMo Yu, and Conghui Zhu. 2012. Locally training\\nthe log-linear model for smt. In Proceedings of the\\n2012 Joint Conference on Empirical Methods in Nat-\\nural Language Processing and Computational Natu-\\nral Language Learning, pages 402–411.\\nLemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,\\nand Conghui Zhu. 2014. Discriminative training for\\nlog-linear based smt: Global or local methods. ACM\\nTransactions on Asian Language Information Pro-\\ncessing (TALIP), 13(4):1–25.\\nYanjun Ma, Yifan He, Andy Way, and Josef van Gen-\\nabith. 2011.\\nConsistent translation using discrim-\\ninative learning-a translation memory-inspired ap-\\nproach.\\nIn Proceedings of the 49th Annual Meet-\\ning of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 1239–1248.\\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\\naofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\\nFast nearest neighbor machine translation.\\narXiv\\npreprint arXiv:2105.14528.\\nFranz Josef Och. 2003. Minimum error rate training in\\nstatistical machine translation. In Proceedings of the\\n41st Annual Meeting of the Association for Compu-\\ntational Linguistics, pages 160–167, Sapporo, Japan.\\nAssociation for Computational Linguistics.\\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\\nSachindra Joshi. 2018. Exemplar encoder-decoder\\nfor neural conversation generation. In ACL, pages\\n1329–1338.\\nAshwin Paranjape, Omar Khattab, Christopher Potts,\\nMatei Zaharia, and Christopher D Manning. 2021.\\nHindsight: Posterior-guided training of retrievers for\\nimproved open-ended generation.\\narXiv preprint\\narXiv:2110.07752.\\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\\nDhingra, and Das Dipanjan. 2019. Text generation\\nwith exemplar-based adaptive decoding. In Proceed-\\nings of the Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies.\\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\\nJianfeng Gao. 2019. Conversing by reading: Con-\\ntentful neural conversation with on-demand machine\\nreading. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 5427–5436.\\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\\nand Wei Chu. 2017. Alime chat: A sequence to se-\\nquence and rerank based chatbot engine. In ACL,\\npages 498–503.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\\ning transferable visual models from natural lan-\\nguage supervision. In Proceedings of the 38th In-\\nternational Conference on Machine Learning, ICML\\n2021, 18-24 July 2021, Virtual Event, volume 139 of\\nProceedings of Machine Learning Research, pages\\n8748–8763. PMLR.\\nStephen Robertson and Hugo Zaragoza. 2009.\\nThe\\nprobabilistic relevance framework: BM25 and be-\\nyond. Now Publishers Inc.\\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\\nral responding machine for short-text conversation.\\nIn ACL, pages 1577–1586.\\nMichel Simard and Pierre Isabelle. 2009. Phrase-based\\nmachine translation in a computer-assisted transla-\\ntion environment. Proceedings of the Twelfth Ma-\\nchine Translation Summit (MT Summit XII), pages\\n120–127.\\nJames Smith and Stephen Clark. 2009. Ebmt for smt:\\na new ebmt-smt hybrid. In Proceedings of the 3rd\\nInternational Workshop on Example-Based Machine\\nTranslation, pages 3–10. Citeseer.\\nHarold Somers. 2003.\\nTranslation memory systems.\\nBenjamins Translation Library, 35:31–48.\\nYiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and\\nMing Zhang. 2016. Two are better than one: An en-\\nsemble of retrieval-and generation-based dialog sys-\\ntems. arXiv preprint arXiv:1610.07149.\\nYixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-\\nlier. 2021a. Few-shot table-to-text generation with\\nprototype memory. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 16-\\n20 November, 2021, pages 910–917. Association for\\nComputational Linguistics.\\nYixuan Su, David Vandyke, Simon Baker, Yan Wang,\\nand Nigel Collier. 2021b. Keep the primary, rewrite\\nthe secondary: A two-stage approach for paraphrase\\ngeneration. In Findings of the Association for Com-\\nputational Linguistics: ACL-IJCNLP 2021, pages\\n560–569, Online. Association for Computational\\nLinguistics.\\nYixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna\\nKorhonen, and Nigel Collier. 2021c. PROTOTYPE-\\nTO-STYLE: dialogue generation with style-aware\\nediting on retrieval memory. IEEE ACM Trans. Au-\\ndio Speech Lang. Process., 29:2152–2161.\\nMarco Turchi, Matteo Negri, M Farajian, and Marcello\\nFederico. 2017. Continuous learning from human\\npost-edits for neural machine translation.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information pro-\\ncessing systems, pages 5998–6008.\\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\\ntional model. In ICML (Deep Learning Workshop).\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2013.\\nIntegrating translation memory into phrase-based\\nmachine translation during decoding.\\nIn Proceed-\\nings of the 51st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Pa-\\npers), pages 11–21.\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\\nDynamically integrating cross-domain translation\\nmemory into phrase-based machine translation dur-\\ning decoding.\\nIn Proceedings of COLING 2014,\\nthe 25th International Conference on Computational\\nLinguistics: Technical Papers, pages 398–408.\\nJason Weston, Emily Dinan, and Alexander Miller.\\n2018. Retrieve and reﬁne: Improved sequence gen-\\neration models for dialogue. In Proceedings of the\\n2018 EMNLP Workshop SCAI: The 2nd Interna-\\ntional Workshop on Search-Oriented Conversational\\nAI, pages 87–92.\\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\\njun Li, and Ming Zhou. 2019. Response generation\\nby context-aware prototype editing. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence,\\nvolume 33, pages 7281–7288.\\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\\net al. 2021. A controllable model of grounded re-\\nsponse generation. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence, volume 35, pages\\n14085–14093.\\nMengzhou Xia, Guoping Huang, Lemao Liu, and\\nShuming Shi. 2019. Graph based translation mem-\\nory for neural machine translation. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence,\\nvolume 33, pages 7297–7304.\\nFei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei\\nShen, and Xueqi Cheng. 2021. Transductive learn-\\ning for unsupervised text style transfer. In Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021, pages 2510–2521. Association for\\nComputational Linguistics.\\nJitao Xu, Josep M Crego, and Jean Senellart. 2020.\\nBoosting neural machine translation with similar\\ntranslations.\\nIn Proceedings of the 58th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, pages 1580–1590.\\nLiu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jian-\\nfeng Gao, W Bruce Croft, Xiaodong Liu, Yelong\\nShen, and Jingjing Liu. 2019.\\nA hybrid retrieval-\\ngeneration neural conversation model. In Proceed-\\nings of the 28th ACM international conference on in-\\nformation and knowledge management, pages 1341–\\n1350.\\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\\nham Neubig, and Satoshi Nakamura. 2018. Guiding\\nneural machine translation with retrieved translation\\npieces. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long Papers), pages 1325–1335.\\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\\nBrockett, Michel Galley, Jianfeng Gao, and Bill\\nDolan. 2021.\\nJoint retrieval and generation train-\\ning for grounded text generation.\\narXiv preprint\\narXiv:2105.06597.\\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\\n2020.\\nNeural machine translation with universal\\nvisual representation. In 8th International Confer-\\nence on Learning Representations, ICLR 2020, Ad-\\ndis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\\nview.net.\\nVentsislav Zhechev and Josef Van Genabith. 2010.\\nSeeding statistical machine translation with trans-\\nlation memory output through tree-based structural\\nalignment.\\nIn Proceedings of the 4th Workshop\\non Syntax and Structure in Statistical Translation,\\npages 43–51.\\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian\\nHuang, Boxing Chen, Weihua Luo, and Jiajun Chen.\\n2021a. Adaptive nearest neighbor machine transla-\\ntion. arXiv preprint arXiv:2105.13022.\\nXin Zheng, Zhirui Zhang, Shujian Huang, Boxing\\nChen, Jun Xie, Weihua Luo, and Jiajun Chen. 2021b.\\nNon-parametric unsupervised domain adaptation for\\nneural machine translation. In Findings of the As-\\nsociation for Computational Linguistics: EMNLP\\n2021, pages 4234–4241.\\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\\nBlack. 2018. A dataset for document grounded con-\\nversations. arXiv preprint arXiv:1809.07358.\\n', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
       " Document(page_content='Context Tuning for Retrieval Augmented Generation\\nRaviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi\\nApple\\nAbstract\\nLarge language models (LLMs) have the re-\\nmarkable ability to solve new tasks with just a\\nfew examples, but they need access to the right\\ntools. Retrieval Augmented Generation (RAG)\\naddresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG’s\\ntool retrieval step requires all the required in-\\nformation to be explicitly present in the query.\\nThis is a limitation, as semantic search, the\\nwidely adopted tool retrieval method, can fail\\nwhen the query is incomplete or lacks context.\\nTo address this limitation, we propose Context\\nTuning for RAG, which employs a smart con-\\ntext retrieval system to fetch relevant informa-\\ntion that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval\\nmodel uses numerical, categorical, and habitual\\nusage signals to retrieve and rank context items.\\nOur empirical results demonstrate that context\\ntuning significantly enhances semantic search,\\nachieving a 3.5-fold and 1.5-fold improvement\\nin Recall@K for context retrieval and tool re-\\ntrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accu-\\nracy. Additionally, we show that our proposed\\nlightweight model using Reciprocal Rank Fu-\\nsion (RRF) with LambdaMART outperforms\\nGPT-4 based retrieval. Moreover, we observe\\ncontext augmentation at plan generation, even\\nafter tool retrieval, reduces hallucination.\\n1\\nIntroduction\\nLarge language models (LLMs) excel in a variety\\nof tasks ranging from response generation and log-\\nical reasoning to program synthesis. One of the\\nimportant active areas of LLM research is to uti-\\nlize them as planning agents (Huang et al., 2022).\\nPlanning is an essential functionality for processing\\ncomplex natural language instructions. A planner\\nshould possess the ability to select the appropriate\\ntools to complete each sub-task. While LLMs ex-\\nhibit exceptional generation capabilities, they have\\ninherent limitations, such as lacking up-to-date in-\\nformation and exhibiting a tendency to hallucinate\\ntools. By providing LLMs with a relevant set of\\ntools based on the given task (Schick et al., 2023;\\nLu et al., 2023), one can alleviate the issue of out-\\ndated information. The set of methods to augment\\nLLM input with retrieved information, such as rel-\\nevant tools, is referred to as Retrieval Augmented\\nGeneration (RAG) (Guu et al., 2020; Lewis et al.,\\n2020). RAG consists of three primary components:\\nTool Retrieval, Plan Generation, and Execution.1\\nIn this study, we focus on enhancing tool retrieval,\\nwith the goal of achieving subsequent improve-\\nments in plan generation.\\nExisting RAG methodologies rely heavily on se-\\nmantic search for tool retrieval, but this approach\\nhas limitations, especially when queries lack speci-\\nficity or context. To this end, we present Context\\nTuning, a component in RAG that precedes tool\\nretrieval, to provide contextual understanding and\\ncontext seeking abilities to improve tool retrieval\\nand plan generation. Our contribution can be sum-\\nmarized as follows:\\n1. We empirically show that traditional RAG\\nis inadequate for implicit/context-seeking\\nqueries and present context tuning as a viable\\nsolution;\\n2. We provide a systematic comparison of vari-\\nous context retrieval methods applied on both\\nlightweight models and LLMs;\\n3. We share empirically the insight that Chain of\\nThought (CoT) augmentation improves con-\\ntext retrieval when no fine-tuning is applied,\\nwhereas fine-tuning the retrieval model re-\\nmoves the need for CoT augmentation;\\n4. We propose a lightweight model using Re-\\nciprocal Rank Fusion (RRF) (Cormack et al.,\\n1Typically, the query along with retrieved tools undergo\\ndynamic prompt construction before presented to an LLM.\\nThis process is called Query Decoration/Transformation. We\\nomit that in this work for the sake of simplicity.\\narXiv:2312.05708v1  [cs.IR]  9 Dec 2023\\n2009) with LambdaMART (Burges, 2010),\\nwhich outperforms GPT-4 (OpenAI, 2023)\\nsystem, and finally;\\n5. We show that context augmentation at plan\\ngeneration reduces hallucinations.\\n2\\nRelated Work\\nUsing retrieval to incorporate tools into plan gen-\\neration with LLMs has emerged as a burgeoning\\narea of research, with ongoing investigations aimed\\nat enhancing both the retrieval component and the\\nLLMs themselves. Our work falls within the for-\\nmer category, placing a particular emphasis on\\nrefining retrieval methodologies to enhance con-\\ntextual understanding of implicit and ambiguous\\nqueries that demand context-seeking capabilities.\\nThe integration of tools into generation has been\\ndemonstrated to enhance the capabilities of LLM-\\nbased planners in recent studies (Schick et al., 2023;\\nLu et al., 2023). However, these works primarily fo-\\ncus on well-defined or unambiguous queries, where\\nretrieving supplementary information to augment\\nthe query is not strictly required. For question an-\\nswering (QA) tasks, incorporating any off-the-shelf\\ndocument retriever has been shown to improve\\nLLM generation, with the addition of re-ranking\\nfurther boosting performance (Ram et al., 2023).\\nWhile re-ranking is preferred, employing any pre-\\ntrained retriever, particularly a text-based retriever,\\nwould be sub-optimal due to the inadequate in-\\nformation expected from ambiguous queries. Our\\nwork demonstrates the inadequacy of text-based\\nretrievers for context retrieval and the necessity of\\nmore advanced retrieval models.\\nTo address the lack of context inherent in under-\\nspecified queries, some studies have explored the\\nuse of CoT (Wei et al., 2022) mechanisms to gener-\\nate text that closely approximates the semantic sim-\\nilarity of relevant context (Ma et al., 2023). While\\nCoT augmentation improves upon baseline meth-\\nods, such as vanilla semantic search, CoT may\\npotentially increase the input length to the LLM,\\nwhich has a limited context window size. Addi-\\ntionally, studies have demonstrated that the place-\\nment of relevant information impacts LLM gen-\\neration (Liu et al., 2023). Therefore, it is prefer-\\nable to avoid increasing input sequence length if\\nthe same or better results can be achieved with-\\nout query augmentation. Distillation-based query\\naugmentation approaches have been proposed to\\naddress this problem (Srinivasan et al., 2023). Our\\nwork unveils that fine-tuning semantic search ob-\\nviates the necessity for query augmentation while\\nachieving comparable performance.\\nRecent studies have shown LLMs can act as\\nzero-shot rankers through pairwise ranking prompt-\\ning (Qin et al., 2023). While addition of rank-\\ning for retrieval component has shown improve-\\nment in QA tasks, direct use of LLMs for the\\nranking task, in addition to plan generation, incurs\\ntwice the inference cost. We empirically show that\\nour proposed lightweight context tuning method,\\nLambdaMART (Burges, 2010) based RRF (Cor-\\nmack et al., 2009), outperforms both fine-tuning\\napproach and GPT-4 (OpenAI, 2023) based CoT\\nAugmentation.\\n3\\nMethodology\\nOur experiments train and evaluate tool retrieval\\nand planning with and without context tuning. Fig-\\nure 1 illustrates how a context-seeking query uses\\ncontext retrieval to enhance tool retrieval and plan\\ngeneration.\\n3.1\\nData Generation\\nOur study employed a data generation methodology\\nusing synthetic application data, aimed at simulat-\\ning real-world scenarios for a digital assistant. The\\ndata encompasses 7 commonly used applications:\\nmail, calendar, google, music, reminders, notes,\\nand phone call. We generated this data using GPT-\\n4, ensuring diversity in the dataset to reflect a wide\\nrange of user personalities. The synthetic dataset\\ncontained a diverse range of context items spanning\\nvarious applications. A total of 791 distinct per-\\nsonas were synthesized, yielding 4,338 unique im-\\nplicit queries for training and 936 implicit queries\\nfor evaluation.\\nAdditionally, we developed a toolbox containing\\nAPIs for each of the applications we considered.\\nThis toolbox was created using in-context learn-\\ning with GPT-4 and contained a total of 59 APIs\\ndistributed across the applications.\\nTo simulate user interaction with a virtual assis-\\ntant, GPT-4 was also utilized to generate realistic\\nqueries grounded in the application data. Following\\nthis, we employed GPT-4 to retrieve the appropri-\\nate tool from the generated toolbox in response\\nto these queries. Finally, GPT-4 was used to re-\\nsolve the tool’s API with the correct parameters.\\nThis methodology provided a comprehensive and\\nrealistic dataset, essential for the evaluation of our\\nFigure 1: Context-tuned RAG pipeline illustrating end-to-end processing of a complex request with progressive\\nplan generation.\\ncontext tuning approach in RAG-based planning\\nsystems.2\\n3.2\\nContext Tuning\\nTo compare various context retrieval methods, we\\nemploy both text-based and vector-based retrieval\\nbaselines. We simulate different context stores by\\nstructuring context data per persona and train mod-\\nels to perform federated search. We use query and\\npersona meta-signals, such as frequency, usage his-\\ntory, and correlation with geo-temporal features,\\nto perform retrieval. We evaluate context retrieval\\nusing the Recall@K and Normalized Discounted\\nCumulative Gain (NDCG@K) metrics.\\nBM25\\nFor text-based search, we use an improved\\nversion of BM25, called BM25T (Trotman et al.,\\n2014).\\nSemantic Search\\nFor vector-based search, we\\nemploy the widely adopted Semantic Search ap-\\nproach.\\nWe use GTR-T5-XL (Ni et al., 2021)\\nto generate query and context item embeddings,\\nwhich are then ranked using cosine similarity to se-\\nlect the top-K results. We evaluate both pre-trained\\nand fine-tuned variants of this method.\\nCoT Augmentation\\nTo enhance the likelihood\\nof semantic alignment with pertinent contextual\\nelements, we augment the under-specified or im-\\nplicit query with GPT-4 (OpenAI, 2023) generated\\nCoT.3 We evaluate both pre-trained and fine-tuned\\nsemantic search versions utilizing CoT.\\nLambdaMART with RRF\\nReciprocal Rank Fu-\\nsion (RRF) (Cormack et al., 2009) is shown to\\noutperform individual rank learning methods. To\\nleverage this advantage, we propose a lightweight\\n2Refer to Appendix A for more details on data generation.\\n3Please refer Appendix A.6 for the GPT-4 prompt used\\nand Table 5 for CoT examples.\\nmodel that uses LambdaMART (Burges, 2010) for\\ninitial ranking of data across context stores, fol-\\nlowed by re-ranking using RRF.\\n3.3\\nTool Retrieval\\nWhile advanced ranking models can enhance the\\nrecall of tool retrieval, we employ the pre-trained\\nGTR-T5-XL model for semantic search using co-\\nsine similarity to retrieve the top-K tools. Extend-\\ning the tool retrieval process to incorporate ranking\\nshould be a straightforward endeavor. We evaluate\\ntool retrieval performance with and without context\\nretrieval using Recall@K.\\n3.4\\nPlanner\\nThe planner’s objective is to select the most appro-\\npriate tool from the retrieved tool list and gener-\\nate a well-formed plan. A plan comprises an API\\ncall constructed using the chosen tool and parame-\\nters extracted from the query and retrieved context.\\nWe fine-tune OpenLLaMA-v2-7B (Touvron et al.,\\n2023) for plan generation. To assess the planner’s\\nperformance, we employ the Abstract Syntax Tree\\n(AST) matching strategy to compute plan accuracy.\\nA hallucination is defined as a plan generated using\\nan imaginary tool.\\n4\\nResults\\n4.1\\nContext Retrieval\\nConsistent with expectations, vector-based search\\nsurpasses text-based search, as shown in Table 1.\\nNevertheless, both approaches struggle to retrieve\\nrelevant context for under-specified queries. Fine-\\ntuned semantic search and CoT augmentation with\\npre-trained semantic search both significantly en-\\nhance retrieval performance. Notably, when fine-\\ntuning is employed, CoT augmentation yields only\\nmarginal gains, suggesting that comparable im-\\nTable 1: A comparison of various Context Retrieval\\nmethods using Recall@K and NDCG@K metrics. The\\ncontext-seeking query is used as input to perform a\\nfederated search across different context stores, after\\nwhich semantic search or ranking is applied.\\nRetrieval Method\\nRecall@K\\nNDCG@K\\nK=3\\nK=5\\nK=10\\nK=3\\nK=5\\nK=10\\nBM25\\n11.35\\n13.47\\n14.92\\n56.45\\n52.33\\n50.91\\nSemantic Search\\n23.74\\n25.38\\n26.99\\n65.44\\n64.31\\n64.02\\nCoT Augmentation\\n71.77\\n85.61\\n94.41\\n93.67\\n91.78\\n88.40\\nFinetuned Semantic\\nSearch\\n73.48\\n88.52\\n95.13\\n93.81\\n94.07\\n94.23\\nFinetuned w/ CoT\\nAugmentation\\n73.55\\n88.53\\n95.17\\n93.92\\n94.11\\n94.22\\nLambdaMART-\\nRRF\\n81.27\\n92.65\\n98.77\\n96.39\\n97.11\\n98.24\\nFigure 2: Evaluation of tool retrieval using Recall@k,\\nwith and without context tuning.\\nprovements could be achieved without augmenting\\nthe input sequence with CoT.\\nOur proposed approach utilizing LambdaMART\\nwith RRF outperforms both fine-tuned semantic\\nsearch and CoT augmentation. Additionally, we ob-\\nserve that for fine-tuned methods, both Recall@K\\nand NDCG@K increase with K, whereas for pre-\\ntrained methods, NDCG@K decreases with an in-\\ncrease in K and Recall@K.\\n4.2\\nTool Retrieval\\nFigure 2 illustrates the performance of tool retrieval\\nusing semantic search. Incorporating relevant con-\\ntext into tool retrieval consistently yields substan-\\ntial gains across various K-values.\\n4.3\\nPlanner\\nTo establish the planner’s lower bound, we remove\\nthe retrieval step, while the upper bound is set by\\ndirectly utilizing context and/or tool labels, effec-\\nTable 2: End-to-end planner evaluation both with and\\nwithout context tuning. “Lower Bound\" excludes re-\\ntrieval and performs direct plan generation while “Upper\\nBound\" assumes perfect context and tool retrieval.\\nSetting\\nAST-based\\nPlan Acc ↑\\nExact Match ↑\\nHallucination ↓\\nLower Bound\\n43.77\\n39.45\\n2.59\\nRAG-based\\nPlanner\\n76.39\\n58.12\\n1.76\\nContext-tuned\\nRAG Planner\\n85.24\\n67.33\\n0.93\\nUpper Bound\\n91.47\\n72.65\\n0.85\\nContext-tuned\\nUpper Bound\\n91.62\\n72.84\\n0.53\\ntively employing oracle retrievers. Table 2 encap-\\nsulates the end-to-end evaluation of the fine-tuned\\nplanner, demonstrating that the context-tuned plan-\\nner significantly outperforms the planner based on\\ntraditional RAG using semantic search. Notably,\\neven when the correct tool is retrieved, incorpo-\\nrating relevant context in plan generation, as evi-\\ndenced by the upper bound, helps in reducing hal-\\nlucination.\\n5\\nConclusion\\nOur work introduces context tuning, a novel compo-\\nnent that enhances RAG-based planning by equip-\\nping it with essential context-seeking capabilities\\nto address incomplete or under-specified queries.\\nThrough a systematic comparison of various re-\\ntrieval methods applied to both lightweight models\\nand LLMs, we demonstrate the effectiveness of\\ncontext tuning in improving contextual understand-\\ning. Our empirical observations reveal that CoT\\naugmentation enhances context retrieval when fine-\\ntuning is not applied, while fine-tuning the retrieval\\nmodel eliminates the need for CoT augmentation.\\nFurthermore, we observe that context augmenta-\\ntion at the plan generation stage reduces halluci-\\nnations. Finally, we showcase the superiority of\\nour proposed lightweight model using RRF with\\nLambdaMART over the GPT-4-based system.\\nLimitations\\nThe current work does not utilize conversation his-\\ntory, which is crucial for handling explicit multi-\\nturn instructions that contain anaphora or ellipsis.\\nThis limitation also hinders the model’s ability to\\neffectively process and respond to complex tasks\\nthat require multi-hop context retrieval. Addition-\\nally, the absence of conversation history impedes\\nthe model’s ability to adapt to topic shifts that may\\noccur throughout a dialogue.\\nFurthermore, the performance of the planner\\nmodel is constrained by the length of the context\\nwindow. While employing LLMs with longer con-\\ntext windows can enhance performance, it also in-\\ncreases model size and computational complexity.\\nTo address this limitation, incorporating context\\ncompression techniques could potentially improve\\nend-to-end performance without incurring signifi-\\ncant increases in model size.\\nDue to privacy constraints, we simulated real-\\nworld data by generating synthetic user profiles\\nand personas that mirrored real-world use cases for\\na digital assistant.\\nEthics Statement\\nTo safeguard privacy, this study exclusively utilizes\\nsynthetically generated data, eliminating the use of\\nreal user information under ethical considerations.\\nAcknowledgements\\nWe would like to thank Stephen Pulman, Barry\\nTheobald and Joel Moniz for their valuable feed-\\nback.\\nReferences\\nChristopher J.C. Burges. 2010. From ranknet to lamb-\\ndarank to lambdamart: An overview. Microsoft Re-\\nsearch Technical Report MSR-TR-2010-82.\\nGordon V. Cormack, Charles L. A. Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outperforms\\ncondorcet and individual rank learning methods. In\\nProceedings of the 32nd International ACM SIGIR\\nConference on Research and Development in Infor-\\nmation Retrieval., pages 758–759.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\\nIgor Mordatch. 2022. Language models as zero-shot\\nplanners: Extracting actionable knowledge for em-\\nbodied agents.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin\\nParanjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang. 2023. Lost in the middle: How lan-\\nguage models use long contexts.\\narXiv preprint\\narXiv:2307.03172.\\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\\nsitional reasoning with large language models. arXiv\\npreprint arXiv:2304.09842.\\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\\nand Nan Duan. 2023. Query rewriting for retrieval-\\naugmented large language models. arXiv preprint\\narXiv:2305.14283.\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\\ntavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,\\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\\nYang. 2021. Large dual encoders are generalizable\\nretrievers.\\nOpenAI. 2023. Gpt-4 technical report.\\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\\nald Metzler, Xuanhui Wang, and Michael Bender-\\nsky. 2023. Large language models are effective text\\nrankers with pairwise ranking prompting.\\narXiv\\npreprint arXiv:2306.17563v1.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. arXiv preprint arXiv:2302.00083.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:\\nLanguage models can teach themselves to use tools.\\narXiv preprint arXiv:2302.04761.\\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\\nLingrui Liao, Luca Bertelli, and Mike Bendersky.\\n2023. Quill: Query intent with large language mod-\\nels using retrieval augmentation and multi-stage dis-\\ntillation. arXiv preprint arXiv:2210.15718v1.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nAndrew Trotman, Antti Puurula, and Blake Burgess.\\n2014. Improvements to bm25 and language models\\nexamined. In Proceedings of the 32nd International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval., pages 58–65.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\\nDenny Zhou. 2022.\\nChain-of-thought prompting\\nelicits reasoning in large language models. arXiv\\npreprint arXiv:2201.11903.\\nA\\nData Generation Details\\nA.1\\nImplicit Query Dataset\\nFor our experiments, we created a synthetic dataset\\nto simulate realistic interactions across various ap-\\nplications commonly found with digital assistants.\\nThe dataset is structured to encompass a diverse\\nrange of contexts, representing different synthetic\\nuser activities and interactions.\\nData Points:\\nA total of 791 unique personas were\\nsynthesized, covering seven key applications: Mail,\\nCalendar, Google, Music, Reminders, Notes, and\\nPhone Calls. The final dataset contained 4,338 train\\nand 936 test data points.\\nGeneration Method:\\nWe utilized GPT-4 to gen-\\nerate the data. We ensured high diversity in the\\ndataset is met through manual inspection, this is\\nessential to accurately reflect a wide range of syn-\\nthetic user personalities and interaction patterns.\\nData Representation:\\nEach data point in the\\ndataset contains multiple contextual information\\nfields, relevant to the specific application and syn-\\nthetic user’s activity. An example of persona in\\nJSON format is shown in Figure 3.\\nFigure 3: Snippet of a persona\\nTable 3 shows the distribution of context items\\nper application in our dataset.\\nA.2\\nPersona Data Creation Example Prompt\\nI\\'m working on generating synthetic data\\nfor a user (also known as persona)\\nand the persona \\'s\\nApplication\\nAvg. Context Items\\nMail\\n2.93\\nCalendar\\n5.63\\nGoogle\\n9.57\\nNotes\\n2.23\\nMusic\\n4.38\\nReminders\\n4.81\\nPhonecall\\n2.34\\nTable 3: Distribution of context items per application.\\niPhone Data.\\nHere are the characteristics of the\\npersona that we would like to\\ngenerate the data for:\\nage: 22\\nfavorite_music_genre: Pop\\nfavorite_movie_genre: Romance\\nfavorite_cuisine: Italian\\nfavorite_sport: Tennis\\nprofession: Software Developer\\nhobbies: [\\'Cooking \\', \\'Swimming \\', \\'\\nReading \\']\\nI want to generate data for ios App\\ncalled Music with bundle id as com.\\napple.music.\\nCan you generate around 5 recently\\nplayed songs\\nInstructions:\\n1. Today \\'s date is 2023 -12 -07\\n11:18:19.028759 , Please generate any\\ntimes or dates in the past 15 days.\\n2. \\'played_time \\' should be in yyyy -MM -dd\\nHH:mm:ss.SSS format\\nUse the following schema:\\nThe output should be formatted as a JSON\\ninstance that conforms to the JSON\\nschema below.\\nAs an example , for the schema {\"\\nproperties \": {\"foo\": {\" title \": \"Foo\\n\", \"description \": \"a list of strings\\n\", \"type\": \"array\", \"items \": {\"type\\n\": \"string \"}}}, \"required \": [\"foo \"]}\\nthe object {\"foo\": [\"bar\", \"baz \"]} is a\\nwell -formatted instance of the\\nschema. The object {\" properties \": {\"\\nfoo\": [\"bar\", \"baz \"]}} is not well -\\nformatted.\\nHere is the output schema:\\n```\\n{\" $defs \": {\" MusicAppData \": {\" properties\\n\": {\" recent_songs \": {\" items \": {\"$ref\\n\": \"#/ $defs/Song\"}, \"title \": \"Recent\\nSongs\", \"type\": \"array\"}, \"\\ncurrent_playing \": {\"$ref\": \"#/ $defs/\\nSong\"}}, \"required \": [\"\\ncurrent_playing \"], \"title \": \"\\nMusicAppData\", \"type\": \"object\"}, \"\\nSong\": {\" properties \": {\" played_time\\n\": {\" default \": \"\", \"title\": \"Played\\nTime\", \"type\": \"string\"}, \"\\nalbum_title \": {\" default \": \"\", \"title\\n\": \"Album Title\", \"type\": \"string\"},\\n\"artist \": {\" default \": \"\", \"title \":\\n\"Artist\", \"type\": \"string\"}, \"\\nsong_name \": {\" default \": \"\", \"title \":\\n\"Song Name\", \"type\": \"string\"}, \"id\\n\": {\" default \": \"\", \"title\": \"Id\", \"\\ntype\": \"string \"}}, \"title\": \"Song\",\\n\"type\": \"object \"}}, \"properties \": {\"\\napp_name \": {\" default \": \"\", \"title \":\\n\"App Name\", \"type\": \"string\"}, \"\\napp_bundle_id \": {\" default \": \"\", \"\\ntitle \": \"App Bundle Id\", \"type\": \"\\nstring\"}, \"app_data \": {\"$ref\": \"#/\\n$defs/MusicAppData \"}}, \"required \":\\n[\" app_data \"]}\\n```\\nDo not include any explanations , only\\nprovide a RFC8259 compliant JSON\\nresponse following this format\\nwithout deviation.\\nA.3\\nSynthetic Toolbox Generation\\nYou are an intelligent AI assistant\\ntasked with generating APIs for iOS\\nthat can be used to interact with\\nApplications. For example , if I ask\\nyou to generate APIs for Messages\\niOS Application , you would generate\\na comprehensive set of APIs that can\\nperform any action on the app. Some\\nexamples below are:\\napi: read_message\\ndescription: Messages App \\'s read_message\\nAPI is used to read messages from a\\nparticular contact\\narguments:\\n- contact: contact from which the\\nmessage was received\\napi: read_unread_messages\\ndescription: Messages App \\'s\\nread_unread_messages API is used to\\nread all unread messages on your\\niPhone\\narguments:\\n-\\napi: send_message\\ndescription: Messages App \\'s send_message\\nAPI is used to send message to a\\nparticular contact\\narguments:\\n- text: text to be sent to the\\ncontact\\n- contact: contact information\\napi: send_group_message\\ndescription: Messages App \\'s\\nsend_group_message API is used to\\nsend a message to a list of contacts\\n.\\narguments:\\n- text: text to be sent to the group\\n- contacts: list of contacts in the\\ngroup\\napi: search_messages\\ndescription: Messages App \\'s\\nsearch_messages API is used to\\nsearch messages by text , recipient ,\\nsender.\\narguments:\\n- text: text to be searched.\\n- recipient: search messages by\\nrecipient name\\n- sender: Search messages by sender\\nname\\nSimilarly , can you generate the APIs for\\nthe following Application: {\\napplication }?\\nDo not include any explanations. Only\\nprovide the APIs in YAML format as\\nabove.\\nThe following table represents the distribution\\nof APIs:\\nApplication\\nAPIs Count\\nMusic\\n11\\nGoogle\\n10\\nNotes\\n9\\nMail\\n8\\nPhoneCall\\n8\\nCalendar\\n7\\nReminders\\n6\\nTable 4: Distribution of APIs generated by Synthetic\\nToolbox Generation\\nA.4\\nTool Retrieval\\nI have the following toolbox defined\\nwith the available APIs:\\n{tools}\\nFor the following query:\\n{query}\\nSuggest the most appropriate api? If\\nthere is no API available in the\\ntoolbox , then output default.\\nOnly output the API name without any\\nexplanations\\nA.5\\nPlan Resolution\\nYou are an intelligent AI Planner\\nhelping me come up with a plan and\\nresolve the variables.\\nI have the following query:\\n{query}\\nI have selected the following tool to\\nperform the task:\\n{tool}\\nCan you come up with fully resolved plan\\nusing the following schema?\\n{format_instructions}\\nA.6\\nPrompt to generate CoT\\nYou are an expert in processing context -\\nseeking or under -specified queries\\nby finding missing context in the\\nquery. As an expert , your task is to\\ngenerate concise chain of thought\\nwhich when used to augment the\\ncontext -seeking query , increases the\\nsemantic similarity of the updated\\nquery with relevant context items.\\nPlease only use the following\\ncontext types: \\'Mail \\', \\'Calendar \\', \\'\\nReminders \\', \\'Notes \\', \\'Photos \\', \\'\\nPhoneCall \\', \\'Message \\', \\'Messenger \\',\\n\\'Maps \\', \\'Google Maps \\', \\'Music \\', \\'\\nSpotify \\', \\'Find My \\', \\'Workout \\'; and\\ndo not create new context types.\\nContext -seeking Query: {query}\\nYour expert Chain of Thought:\\nExamples showing generated implicit queries\\nalong with CoT, context and plan labels are shown\\nin Table 5.\\nTable 5: A sample of context-seeking or under-specified queries along with CoT produced by GPT-4. The columns\\nfor context and tools show labels for those retrieval tasks.\\nImplicit Query\\nCoT\\nRelevant Context\\nTop-3 Relevant Tools\\nWhen is my next\\nguitar lesson?\\nCheck the ’Calendar’ for any\\nupcoming guitar lessons.\\nIf not there, check ’Reminders’\\nfor any alerts set about the lesson.\\nThe user has a reminder\\ntitled “Guitar Class\"\\n[’Reminders’, ’Calendar’,\\n’Notes’]\\nI need to check my\\ndiet plan again.\\nI may have noted down the\\ndiet plan in ’Notes’. If not\\nthere, perhaps I saved a photo\\nof it in ’Photos’.\\nThe user has a note titled\\n“Intermittent Fasting Plan.\"\\nThe user also has an\\nimage titled “Keto Diet.\"\\n[’Photos’, ’Notes’,\\n’Mail’]\\nI’m running late.\\nCheck ’Calendar’ for any\\nscheduled meetings. If so, verify\\n’Maps’ or ’Google Maps’ to\\ngauge current traffic situation\\nand estimated time of arrival.\\nUse ’Messages’ or ’Messenger’\\nor ’Mail’ to inform the meeting\\nattendees that you are\\n“running late\".\\nThe user has an upcoming\\nmeeting titled “LLM\\nDiscussion\" organized by\\n“John Doe.\"\\n[’Calendar’, ’Mail’,\\n’Messages’]\\n', metadata={'Published': '2023-12-09', 'Title': 'Context Tuning for Retrieval Augmented Generation', 'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\"}),\n",
       " Document(page_content='Hybrid Retrieval-Augmented Generation for\\nReal-time Composition Assistance\\nXuchao Zhang∗\\nMenglin Xia∗\\nCamille Couturier\\nGuoqing Zheng\\nSaravan Rajmohan\\nVictor Rühle\\nMicrosoft\\n{xuchaozhang, mollyxia, cacoutur, zheng, saravar, viruh}@microsoft.com\\nAbstract\\nRetrieval augmented models show promise in\\nenhancing traditional language models by im-\\nproving their contextual understanding, inte-\\ngrating private data, and reducing hallucination.\\nHowever, the processing time required for re-\\ntrieval augmented large language models poses\\na challenge when applying them to tasks that\\nrequire real-time responses, such as composi-\\ntion assistance. To overcome this limitation,\\nwe propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that lever-\\nages a hybrid setting that combines both client\\nand cloud models. HybridRAG incorporates\\nretrieval-augmented memory generated asyn-\\nchronously by a Large Language Model (LLM)\\nin the cloud. By integrating this retrieval aug-\\nmented memory, the client model acquires\\nthe capability to generate highly effective re-\\nsponses, benefiting from the LLM’s capabili-\\nties. Furthermore, through asynchronous mem-\\nory integration, the client model is capable\\nof delivering real-time responses to user re-\\nquests without the need to wait for memory syn-\\nchronization from the cloud. Our experiments\\non Wikitext and Pile subsets show that Hy-\\nbridRAG achieves lower latency than a cloud-\\nbased retrieval-augmented LLM, while outper-\\nforming client-only models in utility.\\n1\\nIntroduction\\nRetrieval-augmented approaches (Lewis et al.,\\n2020; Liu et al., 2022) have emerged as a pow-\\nerful tool to boost Large Language Model (LLM)\\nperformance by incorporating external documents\\n(Lewis et al., 2020; Liu et al., 2022). This integra-\\ntion enables models like GPT3 (Brown et al., 2020)\\nto leverage external contextual information, result-\\ning in improved contextual understanding, stream-\\nlined integration of private data, and reduced oc-\\ncurrence of hallucinations. However, the retrieval-\\naugmented large language models can be slow to\\n*These authors contributed equally to this work.\\nrun due to the size of the model and the extra re-\\ntrieval step they require, which can cause latency\\nand limit its application in tasks requiring real-time\\nresponses, such as composition assistance.\\nReal-time composition tools are designed to\\npromptly suggest next words or sentences, and\\ntherefore operate within tight latency budgets (typ-\\nically in the order of 100ms or less). To avoid\\nlatency overheads for sending inference requests\\nto the cloud, these models are usually deployed\\non users’ edge devices. This imposes strict con-\\nstraints on the model’s size and capabilities, lim-\\niting the effectiveness of composition assistance.\\nWhile recent advancements have enabled LLMs\\nlike LLAMA (Touvron et al., 2023) to generate 5\\ntokens per second1, they still fall short in terms of\\nachieving real-time response time for completing a\\nsentence within a few hundred milliseconds. In ad-\\ndition, embedding a retrieval-augmentation module\\ninto the edge model may not be practical because\\nrelevant documents are often stored in the cloud,\\nsuch as a company’s centralized document stor-\\nage, and the retrieval step can introduce additional\\nlatency overhead.\\nTo address the challenges previously outlined,\\nwe propose the Hybrid Retrieval-Augmented Gen-\\neration (HybridRAG) framework. This framework\\nleverages cloud-generated memory augmentation\\nto boost the performance of small language mod-\\nels on edge devices, while operating in an asyn-\\nchronous manner. The HybridRAG framework\\nconsists of a retriever model and memory generator\\nresiding on the cloud server, as well as an augmen-\\ntation coordinator and memory-augmented client\\nmodel deployed on client devices. The cloud model\\ncreates the retrieval-augmented memory and sends\\nit asynchronously to the client model. This allows\\nthe client model to respond to user requests for sug-\\ngestions in real-time without waiting for the cloud\\nmemory. This asynchronous communication also\\n1https://news.ycombinator.com/item?id=35171116\\narXiv:2308.04215v1  [cs.CL]  8 Aug 2023\\nreduces the computational cost of the cloud model,\\nas it does not need to process every user request.\\nIn summary, the contributions in this work can\\nbe summarized as follows:\\n• Hybrid retrieval-augmentation enables real-\\ntime generation: A novel hybrid framework\\nis proposed to enable real-time text genera-\\ntion on client devices, utilizing retrieval aug-\\nmentation in the cloud server. Our approach\\nleverages asynchronous client-cloud commu-\\nnication to achieve prompt responses while\\nmitigating the effects of network latency and\\navoiding slow inference inherent to cloud-\\nbased retrieval-augmented LLMs.\\n• Competitive utility compared to cloud-based\\nlanguage models: We introduce an LLM-\\naugmented memory approach to enhance the\\nutility of the client language model, using\\nGPT3-generated labels for instruction-tuning\\nthe client model. Our model effectively uti-\\nlizes the LLM-augmented memory, resulting\\nin substantial improvement in client model\\nperformance.\\n• Reduced client-to-cloud communication: Our\\naugmentation coordinator module enables\\nasynchronous memory augmentation, mini-\\nmizing client-to-cloud communication by re-\\nquesting augmented memory only when exist-\\ning memory becomes stale. Additionally, uti-\\nlizing GPT3-generated bullet points as mem-\\nory further minimizes data transfer volume.\\nTo assess the effectiveness of our proposed ap-\\nproach, we conducted experiments using 5 bench-\\nmark datasets spanning various domains.\\nOur\\nmodel demonstrated superior performance com-\\npared to the best hybrid baseline, achieving a no-\\ntable average improvement of 48.6% in GLEU\\nscore through LLM-generated memory augmenta-\\ntion, and an additional 9.5% improvement through\\ninstruction tuning on client model. In addition, our\\nasynchronous framework demonstrated an impres-\\nsive 138-fold speed improvement compared to a\\nsynchronous approach within the same experimen-\\ntal setup. We plan to make our code and datasets\\npublic at a later time.\\n2\\nRelated Work\\nThe concept of hybrid computing between edge and\\ncloud devices originated outside the realm of ma-\\nchine learning literature. Hybrid edge-cloud com-\\nputing involves the collaboration between edge and\\ncloud devices for computation. This approach typ-\\nically divides processing tasks between the edge\\nand the cloud, effectively addressing the limited\\ncomputation capabilities of low-power edge de-\\nvices and enabling real-time responses of critical\\nservices such as autonomous driving (Loghin et al.,\\n2019; Wang et al., 2020). However, literature on\\nhybrid computing for machine learning models is\\nrelatively scarce.\\nTo our knowledge, the most relevant topic in the\\nliterature regarding hybrid inference for machine\\nlearning is split computing, which involves parti-\\ntioning modules of machine learning pipelines or\\nlayers of neural network models between edge and\\ncloud devices (Matsubara et al., 2022). For exam-\\nple, in the work of Osia et al. (2020), a deep neural\\nnetwork model for image classification is split into\\nhead layers (acting as a feature extractor) and tail\\nlayers (serving as a classifier). The feature extrac-\\ntor runs on an edge device, while the classifier runs\\non the cloud. Apart from balancing overall com-\\nputation cost and efficiency, split computing also\\noffers privacy benefits. Communication between\\nthe edge and the cloud in split computing is inher-\\nently synchronized, as both devices contribute to\\ncompleting one inference run.\\nAnother notable paradigm for hybrid computing\\nin machine learning is federated learning, which\\nleverages multiple computing devices for training\\nmachine learning models for safety or efficiency\\npurposes (Bonawitz et al., 2019). However, this\\ntechnique is less commonly used for inference.\\nCloud service providers such as AWS also have\\ndeveloped patterns for hosting machine learning\\npipelines across local and cloud devices (AWS-\\nWhitepaper, 2021). However, the design usually\\ninvolves splitting the components of an entire ma-\\nchine learning pipeline, with the core machine\\nlearning models still hosted in the cloud.\\nIn addition to hybrid computing, there is also\\nliterature on improving the efficiency of models\\ndeployed on edge devices (Tambe et al., 2021) as\\nwell as methods focused on reducing the size of\\nlarge models for deployment on smaller devices\\n(Hoefler et al., 2021). However these methods are\\nnot directly comparable to our work.\\nMemory-augmented\\nClient Model\\nRetriever\\nComposition Assistant\\n… …\\nRetrieval augmentation (RA) as a proposed solution\\nAs a first step, we are proposing to augment \\ncommonly used transformer-based language \\nmodels with the ability to retrieve from an external \\nmemory\\nquery\\nRetrieved\\ninformation\\nClient\\nCloud\\nreturn results\\nRequest augmented \\nmemory (async)\\nUpdate augmented \\nmemory\\nRetrieval \\nCorpus\\nAugmentation\\nCoordinator\\nMemory\\nGenerator\\nExternal Memory\\nGenerate memory\\nFigure 1: Overview of our HybridRAG Framework\\n3\\nModel\\nWe present our HybridRAG approach that utilizes\\ncloud-generated memory to enhance the utility of\\nclient-based language model. First, we provide an\\noverview of our approach in Section 3.1. Then we\\ndelve into the details of cloud-based memory gener-\\nation in Section 3.2 and Section 3.3. Furthermore,\\nwe describe the client model in Section 3.4.\\n3.1\\nOverall Architecture\\nThe HybridRAG framework consists of four main\\ncomponents: an augmentation coordinator (client),\\na memory-augmented client model (client), a re-\\ntriever model (cloud), a memory generator (cloud).\\nFigure 1 illustrates the model architecture. The\\naugmentation coordinator monitors the writing con-\\ntext and determines when to request an augmented\\nmemory from the cloud. The retriever model on\\nthe cloud server then searches the retrieval corpus\\nto find relevant data. Subsequently, the memory\\ngenerator employs the GPT3 model to construct\\nan augmented memory that includes all essential\\ninformation from the retrieved data, optimizing\\nits usefulness. Finally, the augmented memory is\\ntransmitted to the client and seamlessly integrated\\ninto the client model, thereby enhancing its overall\\nperformance.\\n3.2\\nAugmentation Coordinator\\nThe augmentation coordinator component is re-\\nsponsible for managing the augmented memory\\nM by monitoring changes to the writing context.\\nThe entire process of the augmentation coordinator\\nis depicted in Figure 2. To determine whether a\\nmemory update is necessary, the coordinator takes\\ninto account both the current context ct and the\\ncontext ct−1 from the previous step and calculates\\nthe context edit distance ED(ct, ct−1). Once the\\ndistance exceeds a pre-determined threshold τ, the\\ncoordinator initiates a request to the cloud server\\nAugmentation\\nCoordinator\\nCloud\\nDeprecated\\nNew memory\\nPrevious \\ncontext 𝑐𝑐𝑡𝑡−1\\nED(𝑐𝑐𝑡𝑡−1, 𝑐𝑐𝑡𝑡)\\nCurrent \\ncontext 𝑐𝑐𝑡𝑡\\nrequest memory\\nreturn memory\\nEdit Distance\\n𝑚𝑚𝑡𝑡\\n𝑚𝑚0\\ndelete memory\\nFigure 2: Process of the Augmentation Coordinator\\nfor augmented memory. We employ the Leven-\\nshtein distance (Yujian and Bo, 2007) to measure\\nthe token-level distance. To avoid redundant mem-\\nory requests, we adopt an incremental memory\\nupdate approach, where only the newly updated\\ncontext is used as the query input to generate the\\nnew memory mt. When the augmented memory\\nreaches its maximum capacity of M, the earliest\\nstored memory is replaced with the new one. This\\nprocess is depicted in Figure 2, where we observe\\nthat upon reaching the maximum memory capacity,\\nthe oldest memory m0 is swapped out and replaced\\nby the new memory mt.\\n3.3\\nRetrieval-Augmented Memory Generator\\nUpon receiving a request from the augmentation\\ncoordinator, the memory generator on the cloud\\ninitiates the preparation of the augmented memory,\\nwhich is then returned to the client. The memory\\npreparation process consists of two steps: docu-\\nment retrieval and memory generation.\\n3.3.1\\nDocument Retrieval\\nGiven an input context x, the goal of the retriever\\nis to select the top-k most relevant documents\\nPrompt\\nGenerate a list of key takeaways in complete sentences for each paragraph one by one, containing all the details such as entities,\\npersons, organizations, numbers, years, and locations.\\nP1: Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model released in 2020 that uses deep\\nlearning to produce human-like text. Given an initial text as prompt, it will produce text that continues the prompt.\\nP2: The architecture is a decoder-only transformer network with a 2048-token-long context and then-unprecedented size of\\n175 billion parameters, requiring 800GB to store. ... This eliminated the need for human supervision and for time-intensive\\nhand-labeling.\\nKey Takeaways: ### P1:\\nOutput\\n- Generative Pre-trained Transformer 3 (GPT-3) is a deep learning-based autoregressive language model released in 2020 that\\nproduces human-like text.\\n- It takes an initial text as prompt and produces text that continues the prompt.\\n### P2:\\n- GPT-3 has a decoder-only transformer network with a 2048-token-long context and 175 billion parameters, requiring 800GB\\nto store.\\n- It is trained to predict what the next token is based on previous tokens using generative pre-training. ...\\nTable 1: Example of Augmented Memory Generated by GPT3 Model.\\nDr = {d1, . . . , dk} from a large retrieval corpus\\nD, where Dr ⊆ D. Following prior work (Lewis\\net al., 2020; Ni et al., 2021), we use the Dense\\nPassage Retrieval (DPR) (Karpukhin et al., 2020)\\nmethod, which is based on a dual encoder archi-\\ntecture pre-trained for question and answering task.\\nDPR encodes the document and the query with a\\ncontext encoder and a query encoder respectively,\\nand calculates the cosine similarity of the encoded\\nembeddings to retrieve the top-k most relevant doc-\\numents. We use the FAISS library (Johnson et al.,\\n2019) for efficient retrieval.\\n3.3.2\\nMemory Generation\\nAfter retrieving the relevant documents Dr, instead\\nof directly using them as in Lewis et al. (2020),\\nwe employ a LLM such as GPT3/3.5 to generate\\nconcise bullet points that capture the essential in-\\nformation from the retrieved documents. These\\nbullet points serve as the augmented memory, sig-\\nnificantly reducing its size by including only the\\nkey takeaways instead of the full documents. This\\nreduces both the communication cost to the client\\nand the inference cost of the client model.\\nTo generate summary bullet points from a re-\\ntrieved document d ∈ Dr, we first split the whole\\ndocument into paragraphs d = {p1, . . . , pl}, where\\nl is the number of paragraphs. We choose an ap-\\npropriate paragraph size that maintains sentence\\nintegrity, avoiding breaking sentences in the mid-\\ndle. Once the paragraphs are created, we utilize the\\nGPT3 Davinci model to extract the key takeaways\\nfrom each paragraph pi, as outlined in Table 1. To\\nminimize the frequency of GPT3 call requests, we\\nconsolidate multiple paragraphs within a document.\\nTable 1 illustrates an example where bullet points\\nare generated for two paragraphs. Subsequently, all\\nthe generated bullet points from the retrieval docu-\\nments are merged to form the memory mt for the\\ncurrent t-th memory request. This memory is then\\ncombined with the existing memory to construct\\nthe new M by the augmentation coordinator, as\\nelucidated in Section 3.2.\\n3.4\\nMemory-Augmented Client Model\\nWhile most previous work on memory-augmented\\nnetworks (Weston et al., 2014; Sukhbaatar et al.,\\n2015; Yogatama et al., 2021; Wu et al., 2022) focus\\non language modeling task performance, our client\\nmodel aims to effectively leverage the augmented\\nmemory generated by the LLM. We hypothesize\\nthat although a small language model may lack\\nthe capacity to handle complex tasks like its larger\\ncounterpart, it can still be trained effectively to\\nleverage augmented memory to accomplish sim-\\npler tasks, such as paraphrasing content. To this\\nend, we propose an instruction-finetuning approach\\nthat aims to bolster the client model’s ability to ef-\\nfectively leverage augmented memory.\\nTraining such a client model is a challenging task\\ndue to the scarcity of training data. Specifically, it\\nrequires data that includes an input prompt I, an\\naugmented memory M, and a ground truth text\\nfor completion C. However, obtaining the latter\\ntwo can be difficult. To address this, we propose a\\nnovel method that leverages a LLM to generate the\\nnecessary training data.\\nGiven a document d, our initial step involves\\nrandomly selecting a percentage of the document\\n(ranging from 12.5% to 50%) to serve as the in-\\nput prompt I(d). Subsequently, we generate the\\naugmented memory M(d) with the steps outlined\\nin Section 3.3. As for the text completion labels,\\nan alternative approach is to directly utilize the\\nremaining portion of the document d, excluding\\nthe prompt. However, this method is suboptimal\\nPrompt\\nReference: In 2020, Generative Pre-trained Transformer 3 (GPT-3) was unveiled, a deep learning-based autoregressive language\\nmodel that can produce human-like text. When provided an initial text as a prompt, it can then generate text that follows on\\nfrom it. ... This process has eliminated the need for laborious manual labeling and human supervision.\\nComplete the following paragraph based on the above reference.\\nGenerative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model released in 2020 that\\nOutput\\nis capable of producing human-like text when prompted with an initial text. GPT-3 has a 2048-token-long context, a record-\\nbreaking 175 billion parameters and a storage capacity of 800GB.\\nTable 2: Example of Constructing an Instruction-enhanced Prompt for Pseudo Label Generation\\nsince the original text may not encompass the infor-\\nmation contained in the augmented memory. The\\ndiscrepancy between the completion and the aug-\\nmented memory used as a reference can negatively\\nimpact the performance of the client model.\\nTo address this issue, we employ the GPT3\\nDavinci model to generate the text completion\\nlabels. We structure the input prompt and aug-\\nmented memory into an instruction-based prompt\\nfollowing the format specified in Table 2. This\\nenables us to instruct the GPT3 model to com-\\nplete the input text based on the provided reference\\nmemory. The completion label can be expressed\\nas C(d) = Gc(I(d); M(d)), with Gc(·) referring\\nto the GPT3-generated completion model. Addi-\\ntional details can be found in Appendix D, where\\nwe present empirical evidence demonstrating that\\nGPT3-completed labels outperform ground truth\\nlabels from the original text.\\nAfter preparing the training data, we proceed\\nto finetune our client model using the instruction-\\nenhanced prompt along with the GPT3-completed\\nlabels, as demonstrated in Table 2 (Output). By\\nincorporating the instruction-tuning method, the\\nclient model learns to effectively utilize the aug-\\nmented memory generated by the GPT3 model. To\\nminimize the discrepancy between our model’s pre-\\ndictions and the GPT3-completed pseudo labels,\\nwe employ the cross-entropy loss, as defined in\\nEquation 1.\\nLd = −\\nl\\nX\\ni=1\\nGc(I(d); M)i log\\n\\x12\\nH(I(d); M)i\\n\\x13\\n,\\n(1)\\nwhere l is the length of completion label and H(·)\\nrefers to the probability of tokens generated by our\\nclient model.\\n4\\nExperiments\\nIn this section, we present the evaluation results\\nof our proposed HybridRAG approach on multiple\\nbenchmark datasets. We introduce the experiment\\nsetup in Section 4.1 and detail the performance\\nof the proposed method compared against several\\nbaseline models in terms of both composition as-\\nsistance utility and inference latency in Section 4.2.\\nFurthermore, we present a case study in Section\\n4.3. Additionally, we provide results from further\\nexperiments examining various components of our\\nmodel in the appendix.\\n4.1\\nExperimental Setup\\n4.1.1\\nDatasets and Labels\\nWe use the WikiText-103 dataset2 and four datasets\\nfrom the Pile benchmark (Gao et al., 2020) for\\nour model training and evaluation. For instruction-\\ntuning the client model, we utilize the training set\\nof WikiText-103, which consists of 15,220 wiki\\npages. Specifically, we employ the abstract section\\nof each page for text completion and use the re-\\nmaining sections of the wiki pages as retrieval texts\\nfor memory generation, as outlined in Section 3.4.\\nFor evaluation, we use the WikiText-103 test set\\nand four subsets from the Pile benchmark: Enron\\nEmails, HackerNews, NIH ExPorter, and Youtube\\nSubtitles.\\nThese datasets encompass a diverse\\nrange of domains, including news, emails, med-\\nical documents and video subtitles. We compare\\nthe predictions of our models against the GPT3\\ncompletion label.\\n4.1.2\\nImplementation Details\\nFor our client model, we select two small OPT lan-\\nguage models (Zhang et al., 2022) with 125 million\\n(OPT-125M) and 350 million model parameters\\n(OPT-350M) for text prediction. These language\\nmodels are pre-trained to predict text and have a\\nsmall enough size to be well-suited for real-time\\ncomposition assistance. We use greedy decoding\\nfor generation.\\nDuring the training phase, these models are\\ntrained on machines equipped with one Tesla V100\\nGPU with 16GB memory. For latency evaluation,\\nwe deploy our client model on two different ma-\\nchines: a GPU machine equipped with an 11GB\\n2https://huggingface.co/datasets/wikitext\\nNvidia Tesla K80 GPU, and a laptop without a\\nGPU, specifically a Surface 4 laptop featuring an\\nIntel i7 CPU @3.00GHz with 4 cores and 16GB\\nof physical memory. For bullet point generation,\\nwe utilize the GPT3 Davinci model in the OpenAI\\nAPI with temperature = 0, top_p = 1.\\n4.1.3\\nEvaluation Metrics\\nWe employ various automated metrics for evaluat-\\ning the model performance. First, we use Perplex-\\nity3, a measure of the language model’s level of\\nsurprise or uncertainty when processing given text.\\nLower Perplexity values indicate higher model\\nconfidence and better comprehension of the in-\\nput text. In addition, we present the results of\\nlexical and semantic similarity metrics, including\\nGLEU (Wu et al., 2016), BLEU (Papineni et al.,\\n2002), ROUGE (Lin, 2004), METEOR (Banerjee\\nand Lavie, 2005), and BERTScore (Zhang* et al.,\\n2020), to evaluate the degree of similarity between\\nthe model’s predictions and the provided labels.\\nTo evaluate the inference latency of our system,\\nwe measure the running time required for three\\nsteps in our task: document retrieval, memory gen-\\neration, and text prediction. This allows us to quan-\\ntify the time cost associated with each of these steps\\nand analyze the overall efficiency of our system.\\n4.1.4\\nBaseline Methods\\nWe compare our approach against the following\\nbaselines: (i) Vanilla OPT: We employ a vanilla\\nclient OPT model for text completion, which does\\nnot use any additional memory. (ii) RAG: The Re-\\ntrieval-Augmented Generation (RAG) (Lewis et al.,\\n2020) approach can be easily turned into a hybrid\\nmodel with our framework. In this setting, we use\\nthe DPR model to retrieve relevant data from the\\ncloud and feed the full retrieved text to the client\\nmodel for generation. (iii) HybridRAG without fine-\\ntuning (HybridRAG w/o FT): To assess the efficacy\\nof our instruction-tuned client model, we examine\\na HybridRAG model without applying finetuning\\nto the cilent model for text prediction. (iv) GPT3\\nzero-shot: We use the GPT3 Davinci model in a\\nzero-shot manner for text completion. However,\\nit’s important to note that the GPT3 model cannot\\nbe deployed on client devices for real-time compo-\\nsition assistance.\\nWhen evaluating the baseline models, we ensure\\na fair comparison by regenerating reference labels\\nusing the GPT3 model, based on the memory used\\n3https://en.wikipedia.org/wiki/Perplexity\\nby each baseline. Specifically, for the Vanilla OPT\\nbaseline, reference labels are generated with GPT3\\nwithout additional memory. For RAG, reference\\nlabels are generated by GPT3 with full text. In the\\ncase of GPT3-zeroshot baseline, since there is no\\nideal reference label for comparison, we used the\\nsame label as our HybridRAG approach.\\n4.2\\nExperimental Results\\n4.2.1\\nUtility\\nTable 3 presents the performance comparison be-\\ntween our models and the baselines on the Wikitext-\\n103 dataset, using the OPT-125M and OPT-350M\\nmodels. The results demonstrate that our approach\\noutperforms all client/hybrid baselines across all\\nevaluated metrics. Compared to the vanilla client\\nmodels, our approach exhibited remarkable perfor-\\nmance improvements. On average, the HybridRAG\\napproach achieves an improvement of 67.6% in\\nPerplexity and 171.7% in GLEU when comparing\\nOPT-125M and OPT-350M. In comparison to the\\nRAG approach (where we use the top 3 documents\\nfor memory augmentation due to prompt size limi-\\ntations), our model still achieved substantial perfor-\\nmance improvement of 50.4% on Perplexity and\\n113.1% on GLEU. Furthermore, when comparing\\nour full approach to the variant without finetuning,\\nour model outperformed it by 4.7% and 9.5% in\\nthe respective metrics. Comparing RAG and Hy-\\nbridRAG w/o FT, we can see that the utilization\\nof GPT3-compressed memory results in a signifi-\\ncant average performance gain of 48.6% in GLEU.\\nFinally, the zero-shot performance from the GPT\\nDavinci model showcased impressive generation\\ncapability without any finetuning or additional con-\\ntext, surpassing the OPT-125M model albeit still\\nfalling short of the OPT-350M model.\\nThe results obtained from the OPT-350M model\\non the four Pile datasets are presented in Table 4.\\nConsistent with the findings on the Wikitext-103\\ndataset, our model demonstrates superior perfor-\\nmance compared to the baseline models across all\\nfour datasets. It is important to note that we did\\nnot finetune the client model specifically on the\\nPile datasets, further highlighting the model’s gen-\\neralization capabilities. Lastly, we have also ob-\\nserved a significantly high perplexity when using\\nthe GPT3 Davinci model. This is due to substan-\\ntial variance in probability between the model’s\\npredictions (generated by GPT3 zero-shot) and the\\nground truth labels (generated by GPT3 with aug-\\nmented memory).\\n4.2.2\\nInference Latency\\nWe performed a latency evaluation for both the\\nOPT-125M and OPT-350M models on the two hard-\\nware setups, as described in Section 4.1.2. Figure\\n5a illustrates that the OPT-125M model exhibits a\\n49.3% faster inference time compared to the OPT-\\n350M model. This finding emphasizes that the size\\nof the client model plays a crucial role in the in-\\nference time. Figure 5b presents the running time\\nfor the retrieval and memory generation steps. The\\nresults indicate that memory generation utilizing\\na large language model consumes the majority of\\nthe memory preparation time. Figure 3c compares\\nsynchronous inference with retrieval augmentation\\nusing the GPT3 model and our asynchronous Hy-\\nbridRAG approach. Notably, our approach show-\\ncases an impressive speed enhancement, achieving\\na remarkable 138.3 times faster performance com-\\npared to the synchronous approach. Lastly, we con-\\nducted a comparison of the running time between\\nthe GPU machine and the laptop in Figure 3d. The\\nresults indicate that our approach can be deployed\\non user edge devices without GPUs, although the\\ninference time is approximately 1.45 times slower\\ncompared to a GPU machine. It should be noted\\nthat we didn’t optimize the client model for decod-\\ning speed with established methods such as caching\\nand quantization. These methods are orthogonal to\\nour work and could be used in conjunction with our\\napproach to further reduce the inference latency.\\n4.2.3\\nAsynchronous Memory Update\\nFigure 4 illustrates the impact of asynchronous\\nmemory update on model performance. The re-\\nsults indicate that as the edit distance increases,\\nthe memory becomes less up-to-date, resulting in a\\ndecline in utility. Notably, when the edit distance\\nthreshold is raised from 5 to 10, there is only a\\nslight decrease of 2.9% in the GLEU score . How-\\never, as the threshold reaches 20, we observe a\\nnotable drop of 13.4% in the GLEU score . More\\ndetailed information can be found in Appendix A.\\nThese results imply that that setting the edit dis-\\ntance threshold below 10 yields the optimal utility\\nfor asynchronous memory update.\\n4.3\\nCase Study\\nTo better understand the strengths and limitations\\nof HybridRAG, we manually examined the comple-\\ntions of different models: GPT3 zero-shot, GPT3-\\n(a) 125M vs 350M\\n(b) retrieval vs memory\\n(c) async vs sync\\n(d) laptop vs gpu\\nFigure 3: Inference Latency for client inference time,\\nretrieval time and memory generation time on multiple\\ndevices\\nFigure 4: Utility on Asynchronous Memory Update\\ngenerated ground truth, vanilla OPT-350M and Hy-\\nbridRAG models with OPT-125M and OPT-350M.\\nWe evaluated the completions of the models\\nbased on several criteria: factualness, repetitions\\nand fluency. We manually annotated the comple-\\ntions of the models with these criteria, using a bi-\\nnary score for each criterion. Tables 5 shows an\\nexample of completions of the prompt by differ-\\nent models that illustrates the main observations\\nand learnings. If we focus on the restricted task\\nof completing the sentence, both OPT-125M and\\nOPT-350M HybridRAG models are able to gener-\\nate fluent, factual texts. When looking at longer\\ncompletions, the OPT-125M HybridRAG model\\nseems to perform better than the OPT-350M coun-\\nterpart, despite having a smaller size. This might\\nPPL\\nGLEU\\nBLEU-4\\nROUGE-1\\nROUGE-L\\nMETEOR\\nBERTScore\\nGPT3 zero-shot\\n4.9\\n32.4\\n31.3\\n50.0\\n44.3\\n43.6\\n89.4\\nOPT-\\n125M\\nVanilla OPT\\n10.3\\n12.5\\n8.6\\n28.9\\n23.7\\n22.0\\n84.2\\nRAG\\n6.7\\n15.5\\n12.6\\n32.4\\n27.0\\n25.9\\n84.9\\nHybridRAG w/o FT\\n3.3\\n31.1\\n31.1\\n45.8\\n40.7\\n40.5\\n87.7\\nHybridRAG\\n3.3\\n34.1\\n33.9\\n48.4\\n42.9\\n43.4\\n88.4\\nOPT-\\n350M\\nVanilla OPT\\n8.1\\n14.0\\n10.2\\n31.3\\n25.5\\n24.3\\n84.9\\nRAG\\n5.3\\n18.3\\n15.8\\n35.5\\n29.4\\n29.1\\n85.8\\nHybridRAG w/o FT\\n2.9\\n34.6\\n35.0\\n49.0\\n43.8\\n44.0\\n88.5\\nHybridRAG\\n2.7\\n37.8\\n37.8\\n52.4\\n47.2\\n47.5\\n89.3\\nTable 3: Utility Performance on the Wikitext-103 dataset\\nEnron Emails\\nNIH ExPorter\\nHacker News\\nYoutube Subtitles\\nPerplexity\\nGLEU\\nPerplexity\\nGLEU\\nPerplexity\\nGLEU\\nPerplexity\\nGLEU\\nGPT3 zero-shot\\n106.9\\n12.3\\n12.2\\n18.5\\n65.1\\n15.3\\n36.6\\n13.7\\nVanilla OPT\\n5.4\\n8.7\\n6.3\\n13.1\\n4.8\\n12.7\\n5.5\\n9.7\\nRAG\\n3.4\\n13.6\\n3.9\\n17.9\\n3.3\\n13.0\\n3.8\\n14.8\\nHybridRAG w/o FT\\n2.9\\n19.9\\n3.3\\n22.1\\n2.5\\n23.6\\n2.8\\n20.1\\nHybridRAG\\n2.9\\n23.6\\n3.0\\n24.0\\n2.7\\n25.7\\n2.8\\n21.7\\nTable 4: Utility Performance on Pile dataset\\nprompt\\nGraham Arthur Chapman (8 January 1941 - 4 October 1989\\nGPT3 zero-shot\\n) was an English cricketer. Chapman was a right-handed batsman who bowled right-arm\\nmedium pace. He was born at Leicester, Leicestershire.\\nChapman made his first\\nGPT3-generated label\\n) was an English comedian, writer, actor, author, and one of the six members of the British\\nsurreal comedy group Monty Python. Chapman and Cleese went on to collaborate on\\ndifferent projects after the success of Mont\\nHybridRAG OPT-125M\\n) was a British actor and comedian. He was originally cast in the Red Dwarf episode\\n\" Timeslides \". He met his long term partner, David Sherlock in 1966. Chapman and\\nSherlock moved to Belsize Park in\\nVanilla OPT-350M\\n) was an English cricketer. He played first-class cricket for Surrey in the 1960s and 1970s.\\nReferences\\nExternal links\\nHybridRAG OPT-350M\\n) was a British comedian and actor. He was born in London, the son of a British diplomat.\\nHe was educated at Emmanuel College, Cambridge, where he studied medicine. He also\\njoined the Cambridge Footlights, where\\nTable 5: Completions of the prompt by different models, showcasing a working case for HybridRAG OPT-125M;\\nthis is also a working case for HybridRAG OPT-350M if we only consider the first sentence.\\nindicate that the HybridRAG OPT-125M has a bet-\\nter balance between the retrieval and generation\\nmechanisms.\\nHowever, we also observe cases where the Hy-\\nbridRAG model fails (refer to Appendix F). Im-\\nproving the memory generator by reducing dupli-\\ncate information, and enhancing the reasoning abil-\\nities of the client model or encouraging it to stick to\\nthe memories content would be some of the ways\\nto address these failing cases and limitations.\\n5\\nConclusion\\nIn this paper, we propose HybridRAG, a novel hy-\\nbrid retrieval-augmented generation approach for\\nreal-time composition assistance. By integrating\\nLLM-enhanced memory into our instruction-tuned\\nclient model with asynchronous update, we show\\nwith experiment results on multiple datasets that\\nour hybrid retrieval approach enables substantial\\nutility improvements over smaller language models\\nwhile maintaining inference efficiency, making it a\\nvaluable solution for real-time tasks.\\nIn our work, we employ retrieval-based memory\\naugmentation as the solution to combine the pow-\\nerful LLM on the cloud and the more agile client\\nmodel. Naturally, the performance of the system\\nrelies on the quality of the memory that the cloud\\nprovides to the client and how the memory is in-\\ntegrated into the client model. The quality of the\\nmemory is influenced by multiple factors: the rep-\\nresentation of the memory (e.g. original text chunk\\nvs condensed information snippets), the relevance\\nof the retrieved data, and the freshness of infor-\\nmation compared to the current input context. In\\nfuture work, we will continue to investigate more\\neffective ways of representing memory according\\nto the tasks, and explore alternative memory aug-\\nmentation approaches, such as RETRO (Borgeaud\\net al., 2021) and Knowledge Infused Decoding (Liu\\net al., 2022), to further enhance our model perfor-\\nmance.\\nReferences\\nAWS-Whitepaper. 2021. Hybrid machine learning.\\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\\nAn automatic metric for MT evaluation with im-\\nproved correlation with human judgments. In Pro-\\nceedings of the ACL Workshop on Intrinsic and Ex-\\ntrinsic Evaluation Measures for Machine Transla-\\ntion and/or Summarization, pages 65–72, Ann Arbor,\\nMichigan. Association for Computational Linguis-\\ntics.\\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,\\nDzmitry Huba, Alex Ingerman, Vladimir Ivanov,\\nChloe Kiddon, Jakub Koneˇcn`y, Stefano Mazzocchi,\\nBrendan McMahan, et al. 2019. Towards federated\\nlearning at scale: System design. Proceedings of\\nmachine learning and systems, 1:374–388.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2021. Improving language models by retrieving from\\ntrillions of tokens. CoRR, abs/2112.04426.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\\ning, Travis Hoppe, Charles Foster, Jason Phang,\\nHorace He, Anish Thite, Noa Nabeshima, Shawn\\nPresser, and Connor Leahy. 2020.\\nThe Pile: An\\n800gb dataset of diverse text for language modeling.\\narXiv preprint arXiv:2101.00027.\\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli\\nDryden, and Alexandra Peste. 2021. Sparsity in deep\\nlearning: Pruning and growth for efficient inference\\nand training in neural networks. J. Mach. Learn. Res.,\\n22(1).\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\\nBillion-scale similarity search with gpus.\\nIEEE\\nTransactions on Big Data, 7(3):535–547.\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020.\\nDense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems, 33:9459–9474.\\nChin-Yew Lin. 2004. ROUGE: A package for auto-\\nmatic evaluation of summaries. In Text Summariza-\\ntion Branches Out, pages 74–81, Barcelona, Spain.\\nAssociation for Computational Linguistics.\\nRuibo Liu, Guoqing Zheng, Shashank Gupta, Rad-\\nhika Gaonkar, Chongyang Gao, Soroush Vosoughi,\\nMilad Shokouhi, and Ahmed Hassan Awadallah.\\n2022. Knowledge infused decoding. arXiv preprint\\narXiv:2204.03084.\\nDumitrel\\nLoghin,\\nLavanya\\nRamapantulu,\\nand\\nYong Meng Teo. 2019.\\nTowards analyzing the\\nperformance of hybrid edge-cloud processing. In\\n2019 IEEE International Conference on Edge\\nComputing (EDGE), pages 87–94.\\nYoshitomo Matsubara, Marco Levorato, and Francesco\\nRestuccia. 2022. Split computing and early exiting\\nfor deep learning applications: Survey and research\\nchallenges. ACM Comput. Surv., 55(5).\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\\ntavo Hernández Ábrego, Ji Ma, Vincent Y Zhao,\\nYi Luan, Keith B Hall, Ming-Wei Chang, et al.\\n2021. Large dual encoders are generalizable retriev-\\ners. arXiv preprint arXiv:2112.07899.\\nSeyed Ali Osia, Ali Shahin Shamsabadi, Sina Sajad-\\nmanesh, Ali Taheri, Kleomenis Katevas, Hamid R.\\nRabiee, Nicholas D. Lane, and Hamed Haddadi. 2020.\\nA hybrid deep learning architecture for privacy-\\npreserving mobile analytics. IEEE Internet of Things\\nJournal, 7(5):4505–4518.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei\\njing Zhu. 2002. Bleu: a method for automatic evalu-\\nation of machine translation. pages 311–318.\\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\\n2015. End-to-end memory networks. Advances in\\nneural information processing systems, 28.\\nThierry Tambe, Coleman Hooper, Lillian Pentecost,\\nTianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh,\\nPaul Whatmough, Alexander M. Rush, David Brooks,\\nand Gu-Yeon Wei. 2021. Edgebert: Sentence-level\\nenergy optimizations for latency-aware multi-task nlp\\ninference. In MICRO-54: 54th Annual IEEE/ACM\\nInternational Symposium on Microarchitecture, MI-\\nCRO ’21, page 830–844, New York, NY, USA. Asso-\\nciation for Computing Machinery.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nBo Wang, Changhai Wang, Wanwei Huang, Ying Song,\\nand Xiaoyun Qin. 2020. A survey and taxonomy\\non task offloading for edge-cloud computing. IEEE\\nAccess, 8:186080–186101.\\nJason Weston, Sumit Chopra, and Antoine Bordes. 2014.\\nMemory networks. arXiv preprint arXiv:1410.3916.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le,\\nMohammad Norouzi, Wolfgang Macherey, Maxim\\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\\nLiu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\\n2016. Google’s neural machine translation system:\\nBridging the gap between human and machine trans-\\nlation.\\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and\\nChristian Szegedy. 2022. Memorizing transformers.\\narXiv preprint arXiv:2203.08913.\\nDani Yogatama, Cyprien de Masson d’Autume, and\\nLingpeng Kong. 2021. Adaptive semiparametric lan-\\nguage models. Transactions of the Association for\\nComputational Linguistics, 9:362–373.\\nLi Yujian and Liu Bo. 2007. A normalized levenshtein\\ndistance metric. IEEE transactions on pattern analy-\\nsis and machine intelligence, 29(6):1091–1095.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\\nOpt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068.\\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\\nuating text generation with bert. In International\\nConference on Learning Representations.\\nA\\nAsynchronous Memory Update\\nFigure 5 shows the changes in GLEU score and perplexity as the edit distance increases in asynchronous\\nmemory update.\\n(a) GLEU\\n(b) Perplexity\\nFigure 5: Changes in GLEU and Perplexity with frequency in Asynchronous Memory Update\\nB\\nLocal vs. Global Retrieval Corpus\\nFigure 6 displays a performance comparison between local and global retrieval corpus on the Wikitext-103\\ndataset. In the local setting, the retrieval corpus consists of the texts contained within the same document,\\nwhile in the global setting, the entire dataset is utilized as the retrieval corpus. The results indicate that the\\nlocal retrieval corpus yields superior GLEU and Perplexity scores. This can be attributed to the smaller\\npool of documents in the local corpus, making it easier to obtain the relevant documents for retrieval.\\n(a) GLEU\\n(b) Perplexity\\nFigure 6: Comparison between local and global retrieval corpus\\nC\\nNumber of Retrieval Documents\\nFigure 7 depicts the model performance for various numbers of retrieval documents on Wikitext-103\\ndataset. Based on the results, we can deduce that both the OPT-125M and OPT-350M models exhibit\\noptimal performance when four retrieval documents are used. As more documents are included beyond\\nfour, the performance remains consistent as the top four documents already encompass the majority of\\nrelevant information for our task.\\nFigure 7: Performance analysis on number of retrieval documents\\nD\\nGPT Completion Labels\\nFigure 8: Comparison between ground truth labels derived from GPT3 completion and the original text\\nWe investigate the discrepancy between using ground-truth labels from GPT3 completion and the\\noriginal text for both model training and evaluation. As shown in Figure 8, fine-tuning our client model\\non GPT3 completion labels consistently yields consistent better results. Notably, even when evaluating\\nwith the original text as ground truth, the model fine-tuned with GPT3-generated labels outperforms the\\nalternative.\\nE\\nMore experiments on Pile datasets\\nThe results of the 5 Pile datasets on all seven metrics are presented in Tables 6 and 7. We can observe\\nthat our model consistently outperforms all the other baselines, demonstrating the superior performance.\\nHowever, it is worth noting that the perplexity of the OPT-125M model does not consistently surpass that\\nof the HybridRAG w/o FT model.\\nF\\nMore examples from human evaluation\\nTable 8 shows another working example for HybridRAG models and Table 9 shows an example of a\\nfailing case for both OPT-125M and OPT-350M HybridRAG models. The OPT-125M model generates\\na repetitive and factually incorrect text, while the OPT-350M model generates a text that is factually\\nincorrect. GPT3 Davinci, however, can still use the same retrieved memories to provide a factual and useful\\ncompletion for this prompt. The memories are bullet points generated from several document chunks;\\nPPL\\nGLEU\\nBLEU-4\\nROUGE-1\\nROUGE-L\\nMETEOR\\nBERTScore\\nEnron\\nEmails\\nGPT3 zero-shot\\n106.9\\n12.3\\n10.4\\n26.1\\n23.3\\n21.6\\n83.6\\nVanilla OPT\\n5.4\\n8.7\\n6.5\\n21.1\\n18.6\\n17.1\\n80.2\\nRAG\\n3.4\\n13.6\\n12.2\\n26.8\\n24.0\\n22.9\\n81.2\\nHybridRAG w/o FT\\n2.9\\n19.9\\n19.4\\n32.7\\n30.2\\n28.8\\n83.0\\nHybridRAG\\n2.9\\n23.6\\n23.7\\n35.0\\n32.3\\n31.9\\n84.7\\nNIH\\nExPorter\\nGPT3 zero-shot\\n12.2\\n18.5\\n16.2\\n36.6\\n31.7\\n29.2\\n86.7\\nVanilla OPT\\n6.3\\n13.1\\n8.9\\n31.9\\n26.7\\n23.9\\n85.3\\nRAG\\n3.9\\n17.9\\n15.2\\n36.7\\n31.1\\n29.7\\n86.5\\nHybridRAG w/o FT\\n3.3\\n22.1\\n20.2\\n40.0\\n35.1\\n33.5\\n87.1\\nHybridRAG\\n3.0\\n24.0\\n22.9\\n41.5\\n37.1\\n35.4\\n87.1\\nHacker\\nNews\\nGPT3 zero-shot\\n65.1\\n15.3\\n14.3\\n30.2\\n27.7\\n20.4\\n85.8\\nVanilla OPT\\n4.8\\n12.7\\n11.3\\n29.3\\n26.4\\n22.4\\n84.9\\nRAG\\n3.3\\n13.0\\n12.2\\n26.8\\n24.5\\n20.5\\n84.3\\nHybridRAG w/o FT\\n2.5\\n23.6\\n24.4\\n37.9\\n35.7\\n31.5\\n86.6\\nHybridRAG\\n2.7\\n25.7\\n25.9\\n39.5\\n36.8\\n34.4\\n86.5\\nYoutube\\nSubtitles\\nGPT3 zero-shot\\n36.6\\n13.7\\n11.8\\n27.1\\n24.5\\n23.3\\n84.4\\nVanilla OPT\\n5.5\\n9.7\\n7.2\\n22.2\\n20.0\\n19.1\\n82.4\\nRAG\\n3.8\\n14.8\\n12.9\\n28.4\\n25.3\\n25.1\\n84.7\\nHybridRAG w/o FT\\n2.8\\n20.1\\n19.9\\n32.9\\n30.4\\n29.7\\n85.1\\nHybridRAG\\n2.8\\n21.7\\n21.1\\n34.6\\n31.8\\n31.9\\n85.9\\nTable 6: Utility Performance of OPT-350M Model on Pile datasets\\nPPL\\nGLEU\\nBLEU-4\\nROUGE-1\\nROUGE-L\\nMETEOR\\nBERTScore\\nEnron\\nEmails\\nGPT3 zero-shot\\n106.9\\n12.3\\n10.4\\n26.1\\n23.3\\n21.6\\n83.6\\nVanilla OPT\\n6.0\\n10.5\\n9.1\\n21.5\\n19.3\\n18.0\\n80.3\\nRAG\\n3.7\\n12.7\\n11.9\\n25.2\\n22.9\\n21.6\\n80.4\\nHybridRAG w/o FT\\n3.2\\n20.3\\n19.9\\n31.0\\n28.6\\n27.4\\n82.7\\nHybridRAG\\n3.7\\n18.9\\n18.9\\n31.6\\n28.3\\n28.2\\n83.8\\nNIH\\nExPorter\\nGPT3 zero-shot\\n12.2\\n18.5\\n16.2\\n36.6\\n31.7\\n29.2\\n86.7\\nVanilla OPT\\n5.4\\n12.0\\n10.8\\n27.9\\n25.3\\n21.4\\n84.3\\nRAG\\n3.8\\n11.5\\n10.5\\n25.4\\n23.3\\n18.6\\n83.8\\nHybridRAG w/o FT\\n2.9\\n19.5\\n19.8\\n33.5\\n31.4\\n27.2\\n85.6\\nHybridRAG\\n3.3\\n23.0\\n23.5\\n36.2\\n33.2\\n30.8\\n85.9\\nHacker\\nNews\\nGPT3 zero-shot\\n65.1\\n15.3\\n14.3\\n30.2\\n27.7\\n20.4\\n85.8\\nVanilla OPT\\n7.8\\n11.8\\n7.5\\n29.7\\n25.2\\n22.2\\n84.7\\nRAG\\n4.7\\n15.8\\n12.8\\n34.1\\n29.1\\n27.2\\n85.8\\nHybridRAG w/o FT\\n3.8\\n19.5\\n17.5\\n37.3\\n32.6\\n30.8\\n86.4\\nHybridRAG\\n4.0\\n21.7\\n20.5\\n38.0\\n32.8\\n32.2\\n86.5\\nYoutube\\nSubtitles\\nGPT3 zero-shot\\n36.6\\n13.7\\n11.8\\n27.1\\n24.5\\n23.3\\n84.4\\nVanilla OPT\\n6.4\\n9.4\\n6.9\\n23.2\\n20.7\\n19.6\\n82.9\\nRAG\\n4.5\\n12.2\\n9.6\\n26.1\\n23.2\\n23.4\\n84.1\\nHybridRAG w/o FT\\n3.1\\n15.9\\n15.0\\n28.5\\n26.1\\n25.1\\n84.2\\nHybridRAG\\n3.5\\n17.1\\n16.2\\n29.4\\n26.5\\n25.7\\n84.7\\nTable 7: Utility Performance of OPT-125M Model on Pile datasets\\nOPT-125M and OPT-350M limited reasoning abilities does not allow them to process the memories\\nextensively and reorganize them. The working cases correspond to situations where the client model only\\nhas to paste an existing chunk of text as is; we’ve noticed that when the small OPT client models combine\\ninformation from different bits of the memories, they usually end up fabricating wrong facts. On top of\\nthis, the completions from the current finetuned OPT-350M model seem to put more emphasis on the\\n(limited) parametric memory than on the retrieved memories, leading to more \"creative\" – but factually\\nwrong – generated text.\\nImproving the memory generator by reducing duplicate information, and enhancing the reasoning\\nabilities of the client model or encouraging it to stick to the memories content would be some of the ways\\nto address these failing cases and limitations.\\nprompt\\nGears of War 2 is a 2008 military science fiction third-person shooter video\\nGPT3 zero-shot\\ngame developed by Epic Games and published by Microsoft Game Studios for the Xbox\\n360. It is the second installment of the Gears of War series and the sequel to 2006’s Gears\\nof War. The game was released worldwide in\\nGPT3-generated label\\ngame. The title and spoken words of the trailer are taken from the World War I poem Ï\\nHave a Rendezvous with Death¨by Alan Seeger. In Gears of War 2, players are able to\\ncarry a\\nHybridRAG OPT-125M\\ngame. The title and spoken words of the trailer were taken from Alan Seeger’s World War\\nI poem \"I Have a Rendezvous with Death\". On October 10, 2008, Microsoft and Epic\\nGames announced the game had\\nVanilla OPT-350M\\ngame developed by Electronic Arts and published by Electronic Arts for the PlayStation 2\\nand Xbox 360. The game was released on November 26, 2008 for the PlayStation 2 and\\nXbox 360.\\nGameplay\\nGears of War\\nHybridRAG OPT-350M\\ngame developed and published by Epic Games and Microsoft. The game is set 18 months\\nafter the events of Gears of War 2. The game is set in the fictional world of the United\\nStates of America. The player controls a\\nTable 8: Completions of the prompt by different models, showcasing a working case for OPT-125M HybridRAG;\\nthis is also a working case for OPT-350M HybridRAG if we only consider the first 2 sentences.\\nprompt\\nThe Bengali language movement (Bengali: Bhasha Andolon), was a political movement in\\nformer East Bengal (today Bangladesh) advocating the recognition of the Bengali language\\nas an\\nGPT3 zero-shot\\nofficial language of Pakistan. The movement was a major part of the Bengali Language\\nMovement and led to the establishment of Bengali as one of the state languages of then\\nEast Pakistan in 1956.\\nThe movement was\\nGPT3-generated label\\nofficial language of the then-Dominion of Pakistan. That sparked many similar movements\\nin other parts of the country, especially in East Pakistan, where many people felt subjugated\\nby the central government dominated by West Pakistani elite.\\nHybridRAG OPT-125M\\nofficial language. The movement was founded by the Bengali community in the early 20th\\ncentury. The movement was founded by the Bengali community in the early 20th century.\\nThe movement was founded by the Bengali community\\nVanilla OPT-350M\\nofficial language of the state.\\nThe movement was founded by the Bengali-language activist and writer, Bhasha Andolôn.\\nThe movement was founded in the year 2000. The movement was led by the\\nHybridRAG OPT-350M\\nofficial language of Bangladesh. The movement was founded in the early 1990s by Fazlur\\nRahman, a Bengali-language journalist and activist. The movement was founded in the\\nearly 1990s by Fazlur\\nTable 9: Completions of the prompt by different models corresponding to failing cases for OPT-125M HybridRAG\\nand OPT-350M HybridRAG.\\n', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}),\n",
       " Document(page_content='Augmentation-Adapted Retriever Improves Generalization of Language\\nModels as Generic Plug-In\\nZichun Yu1\\nChenyan Xiong2\\nShi Yu1\\nZhiyuan Liu13\\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\\n2Microsoft Research, Redmond, USA\\n3Beijing National Research Center for Information Science and Technology, Beijing, China\\n{yuzc19, yus21}@mails.tsinghua.edu.cn; chenyan.xiong@microsoft.com\\nliuzy@tsinghua.edu.cn\\nAbstract\\nRetrieval augmentation can aid language\\nmodels (LMs) in knowledge-intensive tasks\\nby supplying them with external information.\\nPrior works on retrieval augmentation usually\\njointly fine-tune the retriever and the LM,\\nmaking them closely coupled. In this paper, we\\nexplore the scheme of generic retrieval plug-in:\\nthe retriever is to assist target LMs that may\\nnot be known beforehand or are unable to\\nbe fine-tuned together.\\nTo retrieve useful\\ndocuments for unseen target LMs, we propose\\naugmentation-adapted retriever (AAR), which\\nlearns LM’s preferences obtained from a\\nknown source LM. Experiments on the MMLU\\nand PopQA datasets demonstrate that our AAR\\ntrained with a small source LM is able to signif-\\nicantly improve the zero-shot generalization of\\nlarger target LMs ranging from 250M Flan-T5\\nto 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap,\\nenabling AAR trained with a single source\\nLM to serve as a generic plug-in for various\\ntarget LMs.\\nOur code is open-sourced at\\nhttps://github.com/OpenMatch/Augmentation-\\nAdapted-Retriever.\\n1\\nIntroduction\\nLarge language models (LMs) that possess bil-\\nlions of parameters are able to capture a signif-\\nicant amount of human knowledge, leading to\\nconsistent improvements on various downstream\\ntasks (Brown et al., 2020; Kaplan et al., 2020;\\nRoberts et al., 2020). However, the undeniable\\ndrawback of large LMs lies in their high compu-\\ntational cost, which negatively impacts their effi-\\nciency (Strubell et al., 2019; Bender et al., 2021).\\nFurthermore, the knowledge memorized from pre-\\ntraining and the implicit reasoning process of LMs\\ncan be inaccurate and intractable sometimes, hin-\\ndering their applications on knowledge-intensive\\ntasks (Guu et al., 2020; Lewis et al., 2020; Mallen\\net al., 2022; Wei et al., 2022).\\nFlan-T5Base\\n(250M)\\nFlan-T5Large\\n(780M)\\nFlan-T5XL\\n(3B)\\nInstructGPT\\n(175B)\\n# Parameters\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\nMMLU Accuracy\\nStandalone LM\\nLM w/ Few-Shot Prompting\\nLM w/ Adaptive Retrieval\\nLM w/ AAR (Ours)\\nFigure 1: Performance of LM w/ AAR (Ours).\\nInstead of leveraging the knowledge and rea-\\nsoning abilities embedded within the parameters\\nof the LMs, retrieval augmentation (Guu et al.,\\n2020; Lewis et al., 2020; Borgeaud et al., 2022)\\nenhances the LM with a retriever that can retrieve\\nknowledge from an external corpus. On the other\\nhand, prior retrieval augmentation methods (Izac-\\nard and Grave, 2021a; Izacard et al., 2022) necessi-\\ntate fine-tuning the backbone LM to adjust to the\\nretriever and tackle specific downstream tasks. This\\nkind of fine-tuning can be expensive when more\\nand more unique demands emerge (Maronikolakis\\nand Schütze, 2021). More importantly, many top-\\ntier LMs can only be accessed through black-box\\nAPIs (Ouyang et al., 2022; OpenAI, 2023). These\\nAPIs allow users to submit queries and receive re-\\nsponses but typically do not support fine-tuning.\\nIn this paper, we introduce Augmentation-\\nAdapted Retriever (AAR) to assist black-box LMs\\nwith downstream tasks as generic plug-in. To re-\\ntrieve valuable documents for many unseen LMs,\\nwe propose to leverage a small source LM to pro-\\nvide LM-preferred signals for retriever’s training.\\nThe retriever after training (i.e., AAR) can be di-\\nrectly utilized to assist a large target LM by plug-\\nging in the retrieved documents.\\nSpecifically, we choose a small encoder-decoder\\nLM as the source LM and utilize its fusion-\\narXiv:2305.17331v1  [cs.CL]  27 May 2023\\nin-decoder attention scores (Izacard and Grave,\\n2021a) to annotate LM-preferred documents. The\\nLM-preferred documents are then combined with\\nhuman-preferred documents to form the positive\\ndocument set. Negative documents are mined by\\nthe retriever itself using the ANCE (Xiong et al.,\\n2021) technique. After fine-tuning the retriever\\nwith LM’s preferences, it can directly assist unseen\\ntarget LMs in the zero-shot task generalization.\\nWe evaluate AAR on a multi-task language\\nunderstanding dataset MMLU (Hendrycks et al.,\\n2021) and an entity-centric question answering\\ndataset PopQA (Mallen et al., 2022). For the tar-\\nget LMs, we choose Flan-T5 (Chung et al., 2022)\\nseries as our backbone for encoder-decoder LMs\\nand InstructGPT (Ouyang et al., 2022) as our back-\\nbone for decoder-only LMs. Figure 1 shows that\\nassisted with a generic AAR, LMs of different sizes\\nand architectures can consistently outperform the\\nstandalone LMs; the performance of smaller LMs\\ncan sometimes surpass the standalone counterparts\\nof significantly larger sizes (e.g., Flan-T5Large w/\\nAAR outperforms standalone Flan-T5XL by 0.6%).\\nAAR also demonstrates advantages over other aug-\\nmentation approaches such as few-shot prompting\\nand adaptive retrieval (Mallen et al., 2022).\\nFurther analysis reveals that the preferences ob-\\ntained from different-sized source LMs are similar,\\nand LMs with near capacities tend to yield closer\\npreferred document sets. As a result, our AAR\\nmodel trained from a small source LM can be con-\\nsidered as a generic plug-in to enhance the zero-\\nshot generalization of a significantly larger target\\nLM. We also discover that the documents preferred\\nby LMs can provide assistance to the model from\\nalternative perspectives, rather than relying solely\\non the full information favored by search users.\\n2\\nRelated Work\\nRetrieval Augmentation. Augmenting LMs with\\nretrieved information from external memories has\\nshown effective on diverse knowledge-intensive\\ntasks (Guu et al., 2020).\\nPrior works explore\\nnovel ways to train the whole retriever-LM sys-\\ntem in an end-to-end fashion, using retrieval-\\naugmented sequence log-likelihood (Lewis et al.,\\n2020; Borgeaud et al., 2022), fusion-in-decoder\\nattention distillation (Izacard and Grave, 2021a;\\nIzacard et al., 2022), or knowledge graph (Ju et al.,\\n2022). To decouple the retriever from LM, Rubin\\net al. (2022) train an independent prompt retriever\\nfor in-context learning, and Lin et al. (2022) only\\nfine-tune the LM via the retrieved data that is simi-\\nlar to few-shot unsupervised samples.\\nRecent researches adopt zero-shot retrieval aug-\\nmentation that does not fine-tune the LM on In-\\nstructGPT (Ouyang et al., 2022). It can benefit\\nentity-centric question answering (Mallen et al.,\\n2022), chain-of-thought reasoning (He et al., 2022),\\nand multi-hop question answering (Khattab et al.,\\n2022). Parallel work (Shi et al., 2023) uses LM\\nlikelihood to train the retriever for satisfying black-\\nbox LM’s preferences, and they adopt GPT-3\\nCurie (Brown et al., 2020) to provide the super-\\nvision signals. In this work, we devise the retriever\\nthat can be used as a generic plug-in to assist a\\nvariety of unseen LMs.\\nZero-shot Learning and Reasoning.\\nLarge-\\nscale unsupervised pre-trained LMs like GPT-\\n3 (Brown et al., 2020), GPT-4 (OpenAI, 2023),\\nand PaLM (Chowdhery et al., 2022) are able to\\nperform zero-shot learning on many downstream\\ntasks with a task description provided at inference\\ntime. Instruction-finetuned LMs (Sanh et al., 2022;\\nChung et al., 2022; Ouyang et al., 2022), which\\nare pre-trained on multiple supervised tasks using\\nhuman instructions, also also exhibit robust zero-\\nshot learning capabilities. Yu et al. (2023) pro-\\npose a new scheme of zero-shot reasoning, which\\nfirst prompts large LMs to generate relevant docu-\\nments and then perform reading comprehension\\non the generated contents. Recently, there has\\nbeen a growing trend of utilizing plug-and-play\\nknowledge injection to enhance the zero-shot per-\\nformance of LMs, which is achieved through map-\\nping network (Zhang et al., 2023) or document\\nencoding (Xiao et al., 2023). Our work improves\\nthe zero-shot generalization of LMs by utilizing the\\nretrieved information. We demonstrate that identi-\\nfying LMs’ preferences to train the retriever can in\\nturn bring additional evidence texts for LMs.\\n3\\nMethod\\nIn this section, we first introduce the preliminaries\\nof the dense retrieval and the retrieval-augmented\\nLM (§ 3.1), then propose our augmentation-\\nadapted retriever (§ 3.2).\\n3.1\\nPreliminaries\\nRetrieval-augmented LM (Guu et al., 2020; Lewis\\net al., 2020) is a type of LM that leverages external\\ninformation to improve its performance. It retrieves\\nrelevant documents from a corpus using a retriever,\\nand then utilizes the documents to enhance its lan-\\nguage generation capabilities.\\nThe objective of the retriever is to find an aug-\\nmentation document set Da from a corpus C that\\nhelps the LM handle a given query q. Previous\\nresearches (Karpukhin et al., 2020; Xiong et al.,\\n2021) concentrate primarily on the dense retrieval\\nsystem that searches in the dense vector space since\\ndense retrieval usually performs more accurately\\nand efficiently than sparse one.\\nA dense retrieval model first represents q and\\nthe document d into an embedding space using a\\npre-trained encoder g,\\nq = g(q); d = g(d), d ∈ C,\\n(1)\\nand match their embeddings by dot product func-\\ntion f, which supports fast approximate nearest\\nneighbor search (ANN) (André et al., 2016; John-\\nson et al., 2021). We then define Da that contains\\ntop-N retrieved documents as:\\nDa = {da\\n1 . . . da\\nN} = ANNN\\nf(q,◦).\\n(2)\\nFor the LM backbones, the decoder-only and\\nthe encoder-decoder models are the two primary\\nchoices of the retrieval-augmented LMs (Izacard\\nand Grave, 2021b; Yu et al., 2023).\\nGiven a decoder-only LM like GPT-3 (Brown\\net al., 2020), the LM input can be a simple concate-\\nnation of the query and all the augmentation docu-\\nments {da\\n1 . . . da\\nN}. Then, the LM will generate the\\nanswer based on the inputs auto-regressively.\\nFor an encoder-decoder LM like T5 (Raffel et al.,\\n2020), taking simple concatenation as the encoder\\ninput may still be effective. However, this method\\nmay not scale to a large volume of documents due\\nto the quadratic self-attention computation associ-\\nated with the number of documents. To aggregate\\nmultiple documents more efficiently, Izacard and\\nGrave (2021b) propose the fusion-in-decoder (FiD)\\nmechanism, which soon becomes the mainstream\\nin the development of encoder-decoder retrieval-\\naugmented LMs. It first encodes each concatena-\\ntion of the (da\\ni , q) pair separately and then lets the\\ndecoder attend to all parts:\\nFiD(q) = Dec(Enc(da\\n1⊕q) . . . Enc(da\\nN ⊕q)). (3)\\nIn this way, the encoder computes self-attention\\nover one document at a time so that the compu-\\ntational cost can grow linearly with the number\\nof documents. Furthermore, FiD cross-attention\\nis found effective in estimating the relative im-\\nportance of the augmentation documents from\\nNegatives \\nANCE Sampling \\nPositives \\nGround Truth  \\nTop-K FiDAtt \\n  \\n  \\n \\nPre-Trained Retriever\\nQ + D1\\n...\\nEnc\\nEnc\\nDec\\nEnc\\n...\\nSource LM\\nFusion-in-Decoder\\nRetrieve \\nN Docs \\nSource Task\\nQ + D2\\nQ + DN\\nAugmentation-Adapted Retriever\\nTarget LMs Target Tasks\\nGeneric  \\nPlug-In \\nFigure 2: Illustration of augmentation-adapted retriever.\\nthe LM’s perspective (Izacard and Grave, 2021a).\\nTherefore, soft FiD distillation (Izacard and Grave,\\n2021a; Izacard et al., 2022; Shi et al., 2023), which\\nminimizes the KL-divergence between retrieval\\nlikelihood and LM likelihood, is often used to train\\nthe retriever and the LM end-to-end.\\n3.2\\nAugmentation-adapted Retriever\\nDue to the emerging real-world demands and\\nthe limitations of black-box APIs, fine-tuning\\nretrieval-augmented LM for each possible down-\\nstream task can be infeasible. Hence, we intro-\\nduce Augmentation-Adapted Retriever (AAR) as a\\ngeneric plug-in for black-box LMs. As illustrated\\nin Figure 2, AAR can learn the preferences of LMs\\nwithout the need for fine-tuning them.\\nSpecifically, we utilize an encoder-decoder LM\\nas source LM (Ls) to provide LM-preferred signals\\non a source task (Ts) for fine-tuning a pre-trained\\nretriever. Then, we plug the fine-tuned retriever\\ninto unseen target LM (Lt) on a set of target tasks\\n(Tt) non-intersecting with Ts.\\nOur training method starts from a source task Ts,\\nwhere we aggregate the source LM Ls’s average\\nFiD cross-attention (FiDAtt) scores Sa\\ni correspond-\\ning to document da\\ni from the first decoder token\\nover all the layers, all the heads and all the input\\ntokens t of da\\ni ⊕ q:\\nSa\\ni =\\n1\\nln ∗ hn ∗ tn\\nX\\nlayers\\nX\\nheads\\nX\\nt∈da\\ni ⊕q\\nFiDAtt(FiD(q)). (4)\\nwhere ln, hn, tn are the numbers of the layers, the\\nheads and the input tokens.\\nTo make the training process more robust, we uti-\\nlize the FiDAtt scores to annotate the LM-preferred\\npositive documents in a discrete way:\\nDa+ = Dh+ ∪ Top-KSa\\ni ,Da,\\n(5)\\nwhere Dh+ is the human-preferred positive doc-\\nument set (i.e., ground truth) on Ts. Top-KSa\\ni ,Da\\nmeans the documents with the top-k average Fi-\\nDAtt scores Sa\\ni in the retrieved document set Da.\\nThen, we sample hard negatives following\\nANCE (Xiong et al., 2021) and formulate the train-\\ning loss L of the retriever as:\\nD− = ANNM\\nf(q,◦)\\\\Da+,\\n(6)\\nL =\\nX\\nq\\nX\\nd+∈Da+\\nX\\nd−∈D−\\nl(f(q, d+), f(q, d−)),\\n(7)\\nwhere M is the hyperparameter of the negative\\nsampling depth and l is the standard cross entropy\\nloss. After fine-tuning the retriever, we directly use\\nit to augment unseen target LM Lt on each task\\nfrom target task set Tt.\\n4\\nExperimental Methodologies\\nIn this section, we discuss our main experimental\\nsetup. More details can be found in Appendix A.\\n4.1\\nTarget Tasks\\nFollowing prior works (Chung et al., 2022; Mallen\\net al., 2022), we choose MMLU (Hendrycks et al.,\\n2021) and PopQA (Mallen et al., 2022) as target\\ntasks Tt.\\nMMLU is a multitask language understanding\\ndataset, which includes 57 multi-choice question\\nanswering subtasks. These subtasks can be gen-\\nerally classified into four categories: humanities,\\nsocial sciences, STEM, and other. We average the\\naccuracy of the subtasks in each category to ob-\\ntain the final score. We report the accuracy of the\\nevaluation set in our main experiments.\\nPopQA is an entity-centric question answering\\ndataset that mainly concentrates on long-tail ques-\\ntions. We report the accuracy of the test set in our\\nmain experiments.\\n4.2\\nOur Method\\nRetrievers. We adopt two widely used retriev-\\ners to initialize AAR: ANCE initialized from\\nT5Base (Raffel et al., 2020; Ge et al., 2023) and\\nContriever (Izacard et al., 2021) initialized from\\nBERTBase (Devlin et al., 2019). Both of them have\\nbeen fine-tuned on MS MARCO (Bajaj et al., 2016)\\npreviously. For the retrieval corpus, we choose the\\nMS MARCO (Bajaj et al., 2016) for MMLU and\\nthe KILT-Wikipedia (Petroni et al.) for PopQA.\\nLanguage Models. We adopt Flan-T5 (Chung\\net al., 2022) series as our backbone for encoder-\\ndecoder LMs and InstructGPT1 (Ouyang et al.,\\n2022) as our backbone for decoder-only LMs.\\nThese models have been multi-task instruction-\\nfinetuned and are widely utilized for assessing zero-\\nshot generalization (Zhou et al., 2023).\\nImplementation Details.\\nWe utilize the MS\\nMARCO (Bajaj et al., 2016) as our source task\\nTs since it is the common choice to train the re-\\ntriever (Xin et al., 2022). This dataset consists\\nof high-quality questions that require real-world\\nknowledge to answer, which aligns strongly with\\nour target tasks Tt and possesses no overlap with\\nthem. Considering the implementation efficiency,\\nwe take the Flan-T5Base as the source LM Ls and\\ntreat the larger model as the target LM Lt. We di-\\nrectly set the total document number N = 10, LM-\\npreferred document number K = 2, and the nega-\\ntive mining depth M = 100 in the augmentation-\\nadapted training. We run all experiments on a sin-\\ngle A100 GPU (40G).\\n4.3\\nBaselines\\nZero-shot Setting. We compare our method with\\nthe state-of-the-art zero-shot baselines. Standalone\\nLMs, including Flan-T5 (Chung et al., 2022), In-\\nstructGPT (Ouyang et al., 2022), GAL (Taylor\\net al., 2022) and OPT-IML-Max (Iyer et al., 2022),\\nare prompted by a natural language instruction that\\ndescribes the desired task and question. Adaptive\\nretrieval (Mallen et al., 2022) selectively utilizes\\nnon-parametric memory (retrieval augmentation)\\nand parametric memory (the knowledge obtained\\nfrom pre-training) based on questions’ popularity.\\nIn our main experiment, we select the optimal com-\\nbination in their paper, which consists of Contriever\\nas the non-parametric memory and GenRead (Yu\\net al., 2023) as the parametric memory.\\nFew-shot Setting. We also include the results of\\nprevious few-shot models for reference. Flan-T5,\\nInstructGPT, Chinchilla (Hoffmann et al., 2022)\\nand OPT-IML-Max adopt few-shot demonstrations,\\nwhich provide the LMs with a limited number of\\ntask examples. This enables the models to gener-\\nalize from these examples and generate accurate\\nresponses (Gao et al., 2021). Atlas (Izacard et al.,\\n2022) is a state-of-the-art retrieval-augmented LM,\\nwhich jointly pre-trains the retriever with the LM\\n1We use the GPT-3text-davinci-002 December 2022 version.\\nSettings\\nMethods\\n# Parameters\\nMMLU\\nPopQA\\nAll\\nHum.\\nSoc. Sci.\\nSTEM\\nOther\\nAll\\nBase Setting: T5 Base Size\\nFew-shot\\nFlan-T5Base (Chung et al., 2022)\\n250M\\n35.8\\n39.6\\n39.8\\n26.3\\n41.2\\n8.0\\nZero-shot\\nFlan-T5Base\\n250M\\n36.1\\n40.4\\n39.8\\n27.0\\n40.6\\n8.8\\nFlan-T5Base w/ AR (Mallen et al., 2022)\\n250M\\n42.8\\n43.5\\n44.0\\n35.8\\n50.0\\n29.4\\nFlan-T5Base w/ AARContriever (Ours)\\n250M\\n44.4\\n44.7\\n47.7\\n35.8\\n52.2\\n31.9\\nFlan-T5Base w/ AARANCE (Ours)\\n250M\\n44.8\\n42.2\\n46.4\\n39.0\\n53.2\\n37.7\\nLarge Setting: T5 Large Size\\nFew-shot\\nAtlasLarge FT (Izacard et al., 2022)\\n770M\\n38.9\\n37.3\\n41.7\\n32.3\\n44.9\\nn.a.\\nFlan-T5Large\\n780M\\n45.1\\n47.7\\n53.5\\n34.4\\n49.2\\n9.3\\nZero-shot\\nFlan-T5Large\\n780M\\n44.8\\n46.3\\n51.4\\n34.8\\n50.6\\n7.2\\nFlan-T5Large w/ AR\\n780M\\n49.8\\n50.0\\n55.6\\n38.4\\n59.5\\n29.6\\nFlan-T5Large w/ AARContriever (Ours)\\n780M\\n51.8\\n50.8\\n59.7\\n39.4\\n61.8\\n33.4\\nFlan-T5Large w/ AARANCE (Ours)\\n780M\\n50.4\\n48.0\\n58.1\\n39.3\\n60.2\\n39.3\\nXL Setting: T5 XL Size\\nFew-shot\\nAtlasXL FT\\n3B\\n42.3\\n40.0\\n46.8\\n35.0\\n48.1\\nn.a.\\nFlan-T5XL\\n3B\\n51.6\\n55.0\\n61.1\\n36.8\\n59.5\\n11.1\\nZero-shot\\nFlan-T5XL\\n3B\\n51.2\\n55.5\\n57.4\\n38.1\\n58.7\\n11.3\\nFlan-T5XL w/ AR\\n3B\\n55.5\\n56.7\\n64.5\\n43.0\\n62.6\\n33.7\\nFlan-T5XL w/ AARContriever (Ours)\\n3B\\n56.7\\n57.7\\n65.4\\n43.6\\n65.1\\n31.5\\nFlan-T5XL w/ AARANCE (Ours)\\n3B\\n56.2\\n59.4\\n64.8\\n41.5\\n64.9\\n38.0\\nGiant Setting: Over 70B Size\\nFew-shot\\nChinchilla (Hoffmann et al., 2022)\\n70B\\n67.5\\n63.6\\n79.3\\n55.0\\n73.9\\nn.a.\\nOPT-IML-Max (Iyer et al., 2022)\\n175B\\n47.1\\nn.a.\\nn.a.\\nn.a.\\nn.a.\\nn.a.\\nInstructGPT (Ouyang et al., 2022)\\n175B\\n60.5\\n62.0\\n71.8\\n44.3\\n70.1\\n35.2\\nZero-shot\\nGAL (Taylor et al., 2022)\\n120B\\n52.6\\nn.a.\\nn.a.\\nn.a.\\nn.a.\\nn.a.\\nOPT-IML-Max\\n175B\\n49.1\\nn.a.\\nn.a.\\nn.a.\\nn.a.\\nn.a.\\nInstructGPT\\n175B\\n60.2\\n65.7\\n68.0\\n46.1\\n66.5\\n34.7\\nInstructGPT w/ AR\\n175B\\n60.5\\n62.2\\n71.3\\n44.7\\n69.7\\n43.3\\nInstructGPT w/ AARContriever (Ours)\\n175B\\n61.5\\n64.5\\n73.1\\n45.0\\n69.9\\n43.9\\nInstructGPT w/ AARANCE (Ours)\\n175B\\n62.2\\n62.0\\n72.0\\n49.2\\n70.7\\n52.0\\nTable 1: Our main results on MMLU and PopQA dataset. We group the methods mainly by the parameters. Our\\nLs is Flan-T5Base. AARContriever: AAR initialized from Contriever; AARANCE: AAR initialized from ANCE; FT:\\nfine-tuning; AR: adaptive retrieval. Unspecified methods represent direct prompting. The score marked as bold\\nmeans the best performance among the models in the zero-shot setting.\\n2.5\\n5.0\\n7.5\\nTraining FLOPs 1e21\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nMMLU Accuracy\\nAARANCE\\nLs=Flan-T5Base, Lt=Flan-T5Base\\nAARANCE\\nLs=Flan-T5Base, Lt=Flan-T5Large\\nAARANCE\\nLs=Flan-T5Base, Lt=Flan-T5XL\\nAARANCE\\nLs=Lt=Flan-T5Large\\nAARANCE\\nLs=Lt=Flan-T5XL\\nAtlasLarge\\nAtlasXL\\nFigure 3: Training FLOPs of retrieval augmentation\\nmethods.\\nusing unsupervised data and fine-tunes the retriever\\nvia the attention distillation on few-shot data.\\n5\\nEvaluation Results\\nIn this section, we discuss our main results on\\nMMLU and PopQA datasets (§ 5.1) and conduct\\ncomprehensive studies about how (§ 5.2, § 5.3,\\n§ 5.4) and when (§ 5.5, § 5.6) AAR helps.\\n5.1\\nOverall Performance\\nTable 1 demonstrates that, with the assistance of a\\ngeneric AAR, target LMs of different sizes and\\narchitectures can significantly outperform their\\nstandalone baselines in the zero-shot setting. No-\\ntably, AAR even improves powerful InstructGPT\\nby 2% on MMLU and by nearly 20% on PopQA.\\nWe hypothesize that the PopQA dataset mainly\\ncomprises long-tail questions and thus necessitates\\nmore augmentation information to attain high accu-\\nracy. AAR outperforms other augmentation meth-\\nods like few-shot prompting and adaptive retrieval,\\nas they may not offer as extensive evidence text as\\nAAR does.\\nMeanwhile, AAR is a highly efficient augmenta-\\ntion approach since it only relies on a small source\\n250M 780M\\n3B\\n175B\\n# Parameters\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\nMMLU Accuracy\\nANCE\\nAARANCE\\nContriever\\nAARContriever\\n(a) Pre-trained Retrievers.\\n250M 780M\\n3B\\n175B\\n# Parameters\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\nMMLU Accuracy\\nHuman\\nLs=Flan-T5Base LM\\nLs=Flan-T5Base\\nLs=Flan-T5Large\\n(b) Positive docs selection.\\nFigure 4: AAR’s performance when (a) using differ-\\nent pre-trained retrievers and (b) trained with different\\npositive documents, using Flan-T5Base (250M), Flan-\\nT5Large (780M), Flan-T5XL (3B), InstructGPT (175B)\\nas Lt. The retriever in (b) is initialized from ANCE.\\nLM Flan-T5Base (250M) to provide training signals\\nand can generalize well to target LMs of larger ca-\\npacities. Figure 3 illustrates that solely setting the\\nsource LM as the target LM (represented by the in-\\nverted triangles) does not significantly enhance the\\nMMLU accuracy. However, it may triple the train-\\ning budget required. Only using a small source LM\\nis able to outperform the powerful Atlas by large\\nmargins with fewer training FLOPs.\\n5.2\\nAblation Study\\nIn this experiment, we conduct the ablation study of\\naugmentation-adapted training and analyze model\\nbehaviors during the training process.\\nFigure 4a illustrates that augmentation-adapted\\ntraining can bring additional improvements com-\\npared to the pre-trained retrievers.\\nIn general,\\nANCE benefits more from augmentation-adapted\\ntraining than Contriever. This may be due to the\\nfact that Contriever has been already intensively\\npre-trained on massive data augmentations as well\\nas MS MARCO whereas ANCE is trained only on\\nMS MARCO. We provide exact numbers in Table 7\\nand PopQA results in Figure 8, which yield similar\\nobservations as MMLU.\\nIn Figure 4b, we compare retrievers trained with\\ndifferent positive documents, including human-\\npreferred documents annotated by search users (the\\nblue bar), LM-preferred documents obtained by\\nthe source LM (the orange bar), and their combi-\\nnations (the green bar and the red bar). Since the\\nretriever has been pre-trained on user-annotated\\nMS MARCO, simply using human-preferred docu-\\nments to train it may be meaningless and therefore\\nperforms the worst among all approaches. Only\\nusing LM-preferred documents demonstrates no-\\ntable gains over only using human-preferred doc-\\n0 10K\\n30K\\n50K\\n70K\\nTraining step\\n1.0\\n1.1\\n1.2\\n1.3\\n1.4\\nLoss\\n28\\n30\\n32\\n34\\n36\\nMS MARCO MRR@10\\nTrain Loss\\nEval Loss\\nMS MARCO\\n(a) Retriever’s performance.\\n0 10K\\n30K\\n50K\\n70K\\nTraining step\\n42\\n43\\n44\\n45\\n46\\n47\\nMMLU Accuracy\\nMMLU\\n28.0\\n28.4\\n28.8\\n29.2\\n29.6\\n30.0\\nMSMARCO QA Rouge-L\\nMSMARCO QA\\n(b) Lt’s performance.\\nFigure 5: AAR’s training process. (a) exhibits the re-\\ntriever’s (ANCE) performance on MS MARCO. (b)\\npresents the Lt’s (Flan-T5Base) performance on MS-\\nMARCO QA and MMLU.\\numents, and merging both human-preferred and\\nLM-preferred documents (our main setup) further\\nenhances the retriever’s performance. Finally, us-\\ning Flan-T5Base as source LM yields better results\\ncompared to using Flan-T5Large when the target\\nLMs are relatively small. However, as the target\\nLM’s size increases, both approaches achieve com-\\nparable performance. Hence, our choice to utilize\\na small source LM in the augmentation-adapted\\ntraining is reasonable and effective.\\nFigure 5a and Figure 5b plot the retriever’s and\\nLM’s performance during augmentation-adapted\\ntraining, respectively. At the beginning of the train-\\ning, the retriever’s MRR@10 on the MS MARCO\\ndrops dramatically, indicating a large distribution\\ngap between human-preferred and LM-preferred\\ndocuments. As the retriever’s train and dev loss\\ncontinually decline, the retrieval-augmented LM\\ngradually performs better on MSMARCO QA and\\neventually, on MMLU. This result implies that LMs\\non different task may share common preferences,\\nmaking AAR generalize well from single source\\ntask to heterogeneous target tasks.\\n5.3\\nAnalysis of LM-preferred Documents\\nWe highlight the necessity of adapting existing re-\\ntrievers to LMs by comparing the preferred docu-\\nments between search users and LMs. In general,\\nwe discover that LM-preferred documents can as-\\nsist LM from alternative perspectives rather than\\nthe full information favored by search users.\\nFirst, we define the set overlap O between two\\npositive documents set D+\\n1 and D+\\n2 as:\\nO = D+\\n1 ∩ D+\\n2\\nD+\\n1 ∪ D+\\n2\\n.\\n(8)\\nAs illustrated in Figure 6a, the set overlaps of the\\npositive document sets annotated by human users\\nQuestion\\nHuman-preferred Document\\nLM-preferred Document\\nwhat happens if you miss\\nyour cruise ship\\nIf you do miss the ship, go into the\\ncruise terminal and talk with the port\\nagents, who are in contact with both\\nshipboard and shoreside personnel.\\nThey can help you decide the best way\\nto meet your ...\\nThe cruise line is not financially respon-\\nsible for getting passengers to the next\\nport if they miss the ship. Your travel\\nto the subsequent port, or home, is on\\nyour dime, as are any necessary hotel\\nstays and meals...\\nwhat is annexation?\\nAnnexation is an activity in which two\\nthings are joined together, usually with\\na subordinate or lesser thing being at-\\ntached to a larger thing. In strict legal\\nterms, annexation simply involves...\\nAnnexation (Latin ad, to, and nexus,\\njoining) is the administrative action and\\nconcept in international law relating to\\nthe forcible transition of one state’s ter-\\nritory by another state. It is generally\\nheld to be an illegal act...\\nTable 2: Cases study on MSMARCO QA dataset. We show Top-1 document annotated by human users and FiDAtt\\nscores. Red texts are the gold answer spans.\\nBase Large XL Human\\nBase\\nLarge\\nXL\\nHuman\\n100.0%\\n60.6%\\n55.2%\\n13.2%\\n60.6%\\n100.0%\\n69.2%\\n13.3%\\n55.2%\\n69.2%\\n100.0%\\n13.1%\\n13.2%\\n13.3%\\n13.1%\\n100.0%\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSet Overlap\\n(a) Positive docs overlap.\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nMSMARCO QA Rouge-L\\nStandalone LM\\nHuman\\nHuman (Ans-Deletion)\\nLM\\nLM (Ans-Deletion)\\n(b) Answer-deletion test.\\nFigure 6: Analysis of LM-preferred documents. (a)\\nshows the overlaps of positive document sets, where\\nused LMs are Flan-T5 series. (b) presents the answer-\\ndeletion experiments on the MSMARCO QA dataset.\\nThe retriever is initialized from ANCE.\\n(Dh+) and LMs (Top-KSa\\ni ,Da) are quite low (near\\n13%), demonstrating their distinct tendencies in\\nselecting valuable documents. On the contrary, the\\noverlaps between different LMs are relatively high\\n(over 55%). This evidence provides a strong ratio-\\nnale for the generalization ability of AAR since\\nLMs with different sizes tend to annotate simi-\\nlar positive documents. Furthermore, LMs whose\\nsizes are closer generally possess higher overlaps.\\nThis implies a better generalization ability of the\\nAAR to the LMs whose capacity is near the source\\nLM. The findings further validate the results illus-\\ntrated in Figure 4b.\\nTo give an in-depth analysis of how human-\\npreferred and LM-preferred documents differ, we\\nshow two representative cases sampled from the\\nMSMARCO QA in Table 2. We observe that the\\nhuman-preferred document can always present the\\ngold answer at the beginning of the text, while the\\nLM-preferred document may not contain the ex-\\nact answer. However, an LM-preferred document\\n250M\\n780M\\n3B\\n175B\\n# Parameters\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\nMMLU Accuracy\\nTART\\nAARContriever (MSMARCO QA)\\nAARContriever (KILT)\\nAARANCE (MSMARCO QA)\\nAARANCE (KILT)\\nFigure 7:\\nComparison between single-task (MS-\\nMARCO QA) and multi-task (KILT) trained AAR.\\nTART (Asai et al., 2022) is a multi-task instruction-\\nfinetuned retriever that has not been finetuned with LM-\\npreferred signals.\\ncan (1) deliver a new perspective to answer the\\ngiven question, e.g., “the cruise line’s responsibil-\\nity if you miss your cruise ship” and (2) give a\\nspecific explanation instead of an abstract defini-\\ntion, e.g., “forcible transition of one state’s territory\\nby another state”, These characteristics differ from\\nsearch users who want the full information and can\\nfurther assist LMs in knowledge-based reasoning.\\nWe further examine the unique characteristics\\nof LM-preferred documents through the answer-\\ndeletion test (i.e., deleting the exact answer span\\nfrom the retrieved documents).\\nAs shown in\\nFigure 6b, the retriever trained by either human-\\npreferred (i.e., human-preferred retriever) or LM-\\npreferred documents (i.e., LM-preferred retriever)\\ncan help LM answer the given question. Never-\\ntheless, after the answer-deletion, the performance\\nof LM with the human-preferred retriever declines\\nmore significantly than with the LM-preferred re-\\ntriever. Despite having fewer exact match answers\\n(0.6% for LM-preferred documents vs. 13.0% for\\nCorpora\\nMMLU\\nPopQA\\nAll\\nHum.\\nSoc. Sci.\\nSTEM\\nOther\\nAll\\nMS MARCO\\n44.8\\n42.2\\n46.4\\n39.0\\n53.2\\n13.6\\nKILT-Wikipedia\\n42.6\\n42.5\\n45.9\\n34.3\\n50.5\\n37.7\\nStandalone LM\\n36.1\\n40.4\\n39.8\\n27.0\\n40.6\\n8.8\\nTable 3: Ablation of the retrieval corpus, with Flan-\\nT5Base as LM and AARANCE as retriever.\\nhuman-preferred documents), LM-preferred docu-\\nments provide helpful information from alternative\\nperspectives. Therefore, adapting retrievers with\\nLM-preferred documents can in turn make retrieval-\\naugmented LM perform better.\\n5.4\\nMulti-task Training of AAR\\nIn this section, we explore if the multi-task training\\nof AAR can endow the retriever with better gener-\\nalization to the target task. Specifically, we choose\\nKILT (Petroni et al.) as our multi-task data source,\\nwhich consists of 5 categories (Fact Checking, En-\\ntity Linking, Slot Filling, Open Domain QA, and\\nDialogue). We take one representative subtask per\\ncategory to form a mixture of multiple source tasks.\\nFigure 7 illustrates that ANCE trained with\\nmulti-task KILT can consistently outperform the\\nsingle-task MSMARCO QA, proving the bet-\\nter generalization ability brought by multi-task\\naugmentation-adapted training. It is possible that\\nLMs may vary slightly in preferred documents for\\ndifferent tasks and AAR can switch more smoothly\\nto the target task with the help of multi-task train-\\ning. Contriever does not benefit greatly from multi-\\ntask training. We conjecture that this is because\\nContriever has been pre-trained with multiple for-\\nmats of data augmentations and thus generalizes\\nbetter to new data distribution than ANCE. Inter-\\nestingly, multi-task instruction-finetuned retriever\\nTART (Asai et al., 2022) has an overall worse per-\\nformance compared to AAR, highlighting the ben-\\nefits of having LM-preferred documents during the\\nmulti-task training. A more detailed analysis about\\nthe selection of source tasks is in Appendix B.\\n5.5\\nEffect of Retrieval Corpus\\nTable 3 demonstrates that regardless of the retrieval\\ncorpus, AAR results in consistent and substantial\\nperformance gains over the standalone LM.\\nOn MMLU, using MS MARCO as the retrieval\\ncorpus improves the LM more compared to KILT-\\nWikipedia. We hypothesize that the retriever has\\nbeen trained with MS MARCO corpus and thus\\nholds better retrieval performance on it.\\nSettings\\nMethods\\nMMLU\\nPopQA\\nAll\\nAll\\nFew-shot\\nOPT (Zhang et al., 2022)\\n26.0\\n12.3\\nGPT-neo (Black et al., 2021)\\n28.7\\n11.3\\nZero-shot\\nOPT\\n22.7\\n12.0\\nGPT-neo\\n25.3\\n9.9\\nOPT GenRead\\n22.3\\n12.2\\nGPT-neo GenRead\\n24.4\\n11.9\\nOPT w/ AARContriever (Ours)\\n23.2\\n29.1\\nGPT-neo w/ AARContriever (Ours)\\n25.2\\n27.8\\nOPT w/ AARANCE (Ours)\\n23.7\\n32.9\\nGPT-neo w/ AARANCE (Ours)\\n26.6\\n30.1\\nTable 4: Results of OPT and GPT-neo. We use their\\n1.3B version. The score marked as bold means the best\\nperformance in the zero-shot setting.\\nOn PopQA, model performance will drop by\\nlarge margins if we use MS MARCO as the re-\\ntrieval corpus instead of KILT-Wikipedia. The pri-\\nmary reason is that the PopQA dataset is sampled\\nfrom Wikidata and designed for long-tail questions.\\nPartial long-tail knowledge can be only found in\\nKILT-Wikipedia (Mallen et al., 2022) while MS\\nMARCO lacks the indispensable evidence that\\nshould be utilized for answer prediction. For in-\\nstance, given the question “Who is the mother\\nof Melissa Benn?”, there is no document in MS\\nMARCO containing the answer “Caroline Benn”.\\nUnder such circumstances, aligning the retrieval\\ncorpus with the data source can be necessary to\\nleverage AAR’s ability.\\n5.6\\nApplication Scenarios of AAR\\nTo examine if AAR works for unseen LMs that\\nlack zero-shot generalization ability, we also report\\nthe results of OPT (Zhang et al., 2022) and GPT-\\nneo (Black et al., 2021). These models may have\\npoor zero-shot performance due to the lack of multi-\\ntask instruction tuning.\\nFrom Table 4, we find that our AAR improves\\nboth LMs marginally on MMLU while achieving\\nsignificant gains on PopQA. We conjecture that\\nLMs can benefit more easily from retrieval augmen-\\ntation on the knowledge-probing task like PopQA,\\nwhere the answer span can be directly acquired\\nfrom the retrieved documents. MMLU requires the\\nLM to not only comprehend the retrieved pieces of\\nevidence but also perform knowledge-based reason-\\ning over them. OPT and GPT-neo may not possess\\nsuch abilities in zero-shot scenarios.\\nIn summary, although AAR perfectly fits the\\nmulti-task instruction-finetuned LMs such as the\\nFlan-T5 series and InstructGPT, it may not bring\\nsignificant gains for LMs whose zero-shot perfor-\\nmance is sometimes poor, especially on knowledge-\\nbased reasoning. However, we believe that multi-\\ntask instruction-finetuned models will be the foun-\\ndation of future work due to their outstanding zero-\\nshot generalization capabilities, ensuring the wide-\\nranging application scenarios of AAR.\\n6\\nDiscussions\\nLM-preferred Documents. Acquiring discrete\\nfeedback signals from LMs is challenging as it re-\\nquires superior labeling ability, which is not the de-\\nsigned purpose of LMs. Inspired by ADist (Izacard\\nand Grave, 2021a) and Atlas (Izacard et al., 2022),\\nwe utilize the FiDAtt scores to select LM-preferred\\ndocuments for the augmentation-adapted training.\\nHowever, FiDAtt scores may not reflect the actual\\ncontribution of each document faithfully since LM\\nmay prefer attending to readable rather than in-\\nformative documents. Furthermore, the quality of\\nLM-preferred documents depends heavily on the\\ninitial performance of the retrieval-augmented LM.\\nParallel work (Shi et al., 2023) computes the KL\\ndivergence between retrieval likelihood and LM\\nlikelihood to train the retriever. Nevertheless, they\\nrequire a larger source LM, Curie (6.7B), to pro-\\nvide accurate LM likelihood signals. In the future,\\nreinforcement learning could serve as an alterna-\\ntive method to train the retriever, as it optimizes\\nthe retriever by directly leveraging LM’s signals\\nwithout relying on the devised rule.\\nGeneric Retrieval Plug-in.\\nChatgpt-retrieval-\\nplugin2 has recently gained attention in the NLP\\ncommunity as a generic retrieval plug-in. It re-\\ntrieves the most relevant document from users’ data\\nsources and tailor ChatGPT’s response to meet their\\nspecific needs. We believe that techniques such as\\nAAR will enhance the ability of black-box Chat-\\nGPT to generate more reasonable responses based\\non the retrieved information, thereby promoting the\\ndevelopment of human-centered LM design.\\n7\\nConclusion and Future Work\\nThis paper introduces generic retrieval plug-in that\\nutilizes a generic retriever to enhance target LMs\\nthat may be unknown in advance or are unable\\nto be fine-tuned jointly. Our proposed retriever,\\nAAR, can directly support black-box LMs without\\nrequiring any fine-tuning of the LMs. This is ac-\\ncomplished by building the AAR’s training data\\n2https://github.com/openai/chatgpt-retrieval-plugin\\nwith preferred documents from a small source LM\\ntogether with the ground truth.\\nEmpirical results on MMLU and PopQA demon-\\nstrate that AAR-assisted LMs greatly outperform\\nthe standalone ones in zero-shot scenarios, and\\nAAR generalizes well to LMs of different sizes\\nand structures. Analytical results reveal that LM-\\npreferred and human-preferred documents comple-\\nment each other; LM-preferred documents from\\ndifferent LMs overlap significantly, and LMs with\\nsimilar sizes tend to yield closer document sets.\\nWe leave a more detailed explanation of how dif-\\nferent LMs interact with augmentation documents\\nand a more reasonable selection of LM-preferred\\ndocuments for future work. We hope our work\\nshed light on a path to a generic way of treating\\nlarge LMs as black boxes and adapting retrievers\\nto augment them.\\nLimitations\\nDue to the limitation of computational resources,\\nwe have not evaluated the Flan-T5XXL whose num-\\nber of parameters is 11B, and the OPT whose num-\\nber of parameters is greater than 1.3B.\\nSince OPT and GPT-neo perform poorly in the\\nzero-shot setting and separating attention scores of\\neach document in the input is tedious for decoder-\\nonly models, we choose not to use them as source\\nLMs. However, we prove that taking the encoder-\\ndecoder model Flan-T5Base as our source LM is\\nalso robust to augment decoder-only models. We\\nwill explore new methods to annotate LM-preferred\\ndocuments of decoder-only models based on their\\ninherent signals.\\nAcknowledgement\\nZichun Yu, Shi Yu, and Zhiyuan Liu are supported\\nby Institute Guo Qiang at Tsinghua University, Bei-\\njing Academy of Artificial Intelligence (BAAI).\\nAll authors proposed the original idea together.\\nZichun Yu conducted the experiments. Zichun Yu,\\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\\nvided valuable suggestions for the research. We\\nthank Suyu Ge for sharing the ANCE checkpoint\\ninitialized from T5Base.\\nReferences\\nFabien André, Anne-Marie Kermarrec, and Nicolas\\nLe Scouarnec. 2016. Cache locality is not enough:\\nHigh-performance nearest neighbor search with prod-\\nuct quantization fast scan. In VLDB, page 12.\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\\nwith instructions. arXiv preprint arXiv:2211.09260.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\\n2016. Ms marco: A human generated machine read-\\ning comprehension dataset. In CoCo@NeurIPS.\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\\nMajor, and Shmargaret Shmitchell. 2021. On the\\ndangers of stochastic parrots: Can language models\\nbe too big? In Proceedings of ACM FAccT, pages\\n610–623.\\nSid Black, Gao Leo, Phil Wang, Connor Leahy, and\\nStella Biderman. 2021. Gpt-neo: Large scale autore-\\ngressive language modeling with mesh-tensorflow.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste\\nLespiau, Bogdan Damoc, Aidan Clark, Diego\\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\\nElsen, and Laurent Sifre. 2022. Improving language\\nmodels by retrieving from trillions of tokens. In\\nICML, pages 2206–2240.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. In NeurIPS, pages 1877–1901.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\\nScaling language modeling with pathways. arXiv\\npreprint arXiv:2204.02311.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\n2022. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of NAACL, pages 4171–\\n4186.\\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\\nMaking pre-trained language models better few-shot\\nlearners. In Proceedings of ACL, pages 3816–3830.\\nSuyu Ge, Chenyan Xiong, Corby Rosset, Arnold Over-\\nwijk, Jiawei Han, and Paul Bennett. 2023. Augment-\\ning zero-shot dense retrievers with plug-in mixture-\\nof-memories. arXiv preprint arXiv:2302.03754.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. In ICML,\\npages 3929–3938.\\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\\nRethinking with retrieval: Faithful large language\\nmodel inference. arXiv preprint arXiv:2301.00303.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021. Measuring massive multitask language under-\\nstanding. In ICLR.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\\nford, Diego de Las Casas, Lisa Anne Hendricks, Jo-\\nhannes Welbl, Aidan Clark, Thomas Hennigan, Eric\\nNoland, Katherine Millican, George van den Driess-\\nche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\\nKarén Simonyan, Erich Elsen, Oriol Vinyals, Jack\\nRae, and Laurent Sifre. 2022. An empirical analysis\\nof compute-optimal large language model training.\\nIn NeurIPS, pages 30016–30030.\\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\\nand Ves Stoyanov. 2022. Opt-iml: Scaling language\\nmodel instruction meta learning through the lens of\\ngeneralization. arXiv preprint arXiv:2212.12017.\\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\\nand Edouard Grave. 2021. Unsupervised dense infor-\\nmation retrieval with contrastive learning. TMLR.\\nGautier Izacard and Edouard Grave. 2021a. Distilling\\nknowledge from reader to retriever for question an-\\nswering. In ICLR.\\nGautier Izacard and Edouard Grave. 2021b. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of EACL,\\npages 874–880.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2022. Few-shot Learning with Re-\\ntrieval Augmented Language Models. arXiv preprint\\narXiv:2208.03299.\\nJeff Johnson, Matthijs Douze, and Herve Jegou. 2021.\\nBillion-scale similarity search with gpus. IEEE TBD,\\n7(3):535–547.\\nMingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang,\\nand Yanfang Ye. 2022. Grape: Knowledge graph\\nenhanced passage reader for open-domain question\\nanswering. In Findings of EMNLP.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\\nScaling laws for neural language models.\\narXiv\\npreprint arXiv:2001.08361.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020.\\nDense passage retrieval for\\nopen-domain question answering. In Proceedings\\nof EMNLP, pages 6769–6781.\\nOmar Khattab,\\nKeshav Santhanam,\\nXiang Lisa\\nLi, David Hall, Percy Liang, Christopher Potts,\\nand Matei Zaharia. 2022.\\nDemonstrate-search-\\npredict: Composing retrieval and language mod-\\nels for knowledge-intensive nlp.\\narXiv preprint\\narXiv:2212.14024.\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-\\nintensive NLP tasks. In NeurIPS, pages 9459–9474.\\nBill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen\\nTian, and Xiang Ren. 2022. Unsupervised cross-\\ntask generalization via retrieval augmentation. In\\nNeurIPS, pages 22003–22017.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\\n2022. When not to trust language models: Inves-\\ntigating effectiveness and limitations of paramet-\\nric and non-parametric memories. arXiv preprint\\narXiv:2212.10511.\\nAntonis Maronikolakis and Hinrich Schütze. 2021. Mul-\\ntidomain pretrained language models for green NLP.\\nIn Proceedings of AdaptNLP, pages 1–8.\\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\\narXiv:2303.08774.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Gray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. In NeurIPS, pages 27730–27744.\\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\\nRiedel. KILT: a benchmark for knowledge intensive\\nlanguage tasks. In Proceedings of NAACL, pages\\n2523–2544.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. JMLR, 21:140:1–140:67.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the parame-\\nters of a language model? In Proceedings of EMNLP,\\npages 5418–5426.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2022. Learning to retrieve prompts for in-context\\nlearning. In Proceedings of NAACL, pages 2655–\\n2671.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChaffin, Arnaud Stiegler, Arun Raja, and et al. 2022.\\nMultitask prompted training enables zero-shot task\\ngeneralization. In ICLR.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\\nmoyer, and Wen tau Yih. 2023. Replug: Retrieval-\\naugmented black-box language models.\\narXiv\\npreprint arXiv:2301.12652.\\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\\nlum. 2019. Energy and policy considerations for\\ndeep learning in NLP. In Proceedings of ACL, pages\\n3645–3650.\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\\nGalactica: A large language model for science. arXiv\\npreprint arXiv:2211.09085.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nNeurIPS, pages 24824–24837.\\nChaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min\\nChan, Yankai Lin, Zhiyuan Liu, Xiangyang Li,\\nZhonghua Li, Zhao Cao, and Maosong Sun. 2023.\\nPlug-and-play document modules for pre-trained\\nmodels. In Proceedings of ACL.\\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\\nshot dense retrieval with momentum adversarial do-\\nmain invariant representations. In Findings of ACL,\\npages 4008–4020.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In ICLR.\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\\nMichael Zeng, and Meng Jiang. 2023.\\nGenerate\\nrather than retrieve: Large language models are\\nstrong context generators. In ICLR.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\\ntrained transformer language models. arXiv preprint\\narXiv:2205.01068.\\nZhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong\\nWang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan\\nLiu, Peng Li, Maosong Sun, and Jie Zhou. 2023.\\nPlug-and-play knowledge injection for pre-trained\\nlanguage models. In Proceedings of ACL.\\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\\nLifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,\\nPengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,\\nand Lichao Sun. 2023. A comprehensive survey on\\npretrained foundation models: A history from bert to\\nchatgpt. arXiv preprint arXiv:2302.09419.\\nA\\nExperimental Settings\\nA.1\\nTraining Hyperparameters\\nWe take the ANCE initialized from T5Base3 (Xiong\\net al., 2021; Ge et al., 2023) and Contriever4 (Izac-\\nard et al., 2021)’s hyperparameters in the\\naugmentation-adapted training. Specifically, we fix\\nbatch size as 8, learning rate as 5e-6, and epochs as\\n6 for ANCE while taking batch size as 8, learning\\nrate as 1e-5, and epochs as 3 for Contriever. We\\nchoose their best checkpoints based on the perfor-\\nmance of the development set. The information\\nabout our source tasks and target tasks are listed in\\nTable 6.\\nA.2\\nNumber of Augmentation Documents\\nLMs of different sizes, facing various target tasks,\\nmay require indefinite numbers of augmentation\\ndocuments to achieve their best performance.\\nFor MMLU, we analyze how the number of aug-\\nmentation documents affects LMs’ performance.\\nAs illustrated in Figure 9, we discover that LMs of\\nlarger capacity generally benefit more from more\\naugmentation documents. A possible explanation\\nis that larger LMs are more capable of integrating\\ninformation from multiple documents and perform-\\ning complicated reasoning based on them.\\nFor PopQA, using 3 augmentation documents\\nachieves the best performance across all LMs.\\nA.3\\nPrompt Templates\\nThe prompt template for MMLU is:\\nHere’s a problem to solve: {question}\\nAmong the 4 following options, which is\\nthe correct answer?\\n- A: {choice_A}\\n- B: {choice_B}\\n- C: {choice_C}\\n- D: {choice_D}\\nThe prompt template for PopQA is:\\nQ: {question} A:\\nB\\nSelection of Source Task\\nWe provide a detailed selection of the source tasks\\nhere, using a variety of source and target tasks to an-\\nalyze. MSMARCO QA, KILT-TriviaQA, and NQ\\nbelong to Open Domain QA, while KILT-T-REx\\nand zsRE belong to Slot Filling. MMLU belongs\\nto Multi-task Language Understanding, which is\\n3https://huggingface.co/OpenMatch/t5-ance\\n4https://huggingface.co/facebook/contriever-msmarco\\nTs\\nTt\\nMMLU\\nNQ\\nzsRE\\nMSMARCO QA\\n44.8\\n46.7\\n75.1\\nKILT-TriviaQA\\n43.6\\n46.4\\n74.9\\nKILT-T-REx\\n44.1\\n45.9\\n77.2\\nTable 5: Relationship between the selection of source\\ntask Ts and the performance of target task Tt. The\\nmodel is Flan-T5Base w/ AARANCE. As NQ and zsRE\\nare included in the Flan-T5 training data, we only report\\ntheir F1 results here for reference.\\ncloser to the Open Domain QA in terms of the task\\nobjective. As shown in Table 5, when we align the\\ncategory of the source task with the target task, the\\nLM w/ AAR can generally achieve the best results.\\nWe suppose that this is because LM may share sim-\\nilar document preferences on the tasks from the\\nsame dataset category, making AAR easier to gen-\\neralize. Furthermore, taking MSMARCO QA as\\nthe source task performs the best on MMLU. This\\nvalidates the rationality to set Ts as MSMARCO\\nQA in our main experimental settings.\\nC\\nAAR’s Improvements on PopQA\\n250M\\n780M\\n3B\\n175B\\n# Parameters\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nPopQA Accuracy\\nANCE\\nAARANCE\\nContriever\\nAARContriever\\nFigure 8: AAR’s improvements on PopQA, using Flan-\\nT5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),\\nInstructGPT (175B) as target LMs.\\nD\\nFine-tuning Results\\nWe also report the fine-tuning results of Flan-\\nT5Base and Flan-T5Large on MMLU auxiliary train-\\ning data (Hendrycks et al., 2021) in Table 7. Due to\\nthe limitation of the computational resources, we\\ndo not include the fine-tuning result of Flan-T5XL.\\nWe take batch size as 32, learning rate as 5e-5, and\\nepochs as 3 in fine-tuning. In general, the LM that\\nhas already been massively multi-task instruction-\\nfinetuned, such as Flan-T5, improves little from\\nfine-tuning on extra tasks but benefits greatly from\\nour AAR. The results further validate the power of\\nzero-shot retrieval augmentation.\\n1 2 3 4 5 6 7 8 9 10\\nNumber of documents\\n36\\n38\\n40\\n42\\n44\\nMMLU Accuracy\\nStandalone LM\\nLt=Flan-T5Base\\n(a) Flan-T5Base w/ AARANCE.\\n1 2 3 4 5 6 7 8 9 10\\nNumber of documents\\n44\\n46\\n48\\n50\\n52\\nMMLU Accuracy\\nStandalone LM\\nLt=Flan-T5Large\\n(b) Flan-T5Large w/ AARANCE.\\n1 2 3 4 5 6 7 8 9 10\\nNumber of documents\\n50\\n52\\n54\\n56\\n58\\nMMLU Accuracy\\nStandalone LM\\nLt=Flan-T5XL\\n(c) Flan-T5XL w/ AARANCE.\\nFigure 9: Relationship between LM’s performance and the number of augmentation documents.\\nCategory\\nNumber\\nTs\\nMSMARCO QA\\nOpen Domain QA\\n148122\\nKILT-FEVER\\nFact Checking\\n10444\\nKILT-WNED\\nEntity Linking\\n3396\\nKILT-T-REx\\nSlot Filling\\n5000\\nKILT-TriviaQA\\nOpen Domain QA\\n5359\\nKILT-Wizard of Wikipedia\\nDialogue\\n3054\\nTt\\nMMLU\\nMulti-task Language Understanding\\n1531\\nPopQA\\nOpen Domain QA\\n14267\\nTable 6: Configurations of our source tasks and target tasks.\\nMethods\\nMMLU\\nAll\\nHum.\\nSoc. Sci.\\nSTEM\\nOther\\nFlan-T5Base\\n36.1\\n40.4\\n39.8\\n27.0\\n40.6\\nFlan-T5Base Fine-tuning\\n36.1\\n38.9\\n41.2\\n27.9\\n39.9\\nFlan-T5Base w/ Contriever\\n43.7\\n44.4\\n45.0\\n36.4\\n51.1\\nFlan-T5Base w/ ANCE\\n43.0\\n44.2\\n44.3\\n34.5\\n51.9\\nFlan-T5Base w/ AARContriever (Ours)\\n44.4\\n44.7\\n47.7\\n35.8\\n52.2\\nFlan-T5Base w/ AARANCE (Ours)\\n44.8\\n42.2\\n46.4\\n39.0\\n53.2\\nFlan-T5Large\\n45.1\\n47.7\\n53.5\\n34.4\\n49.2\\nFlan-T5Large Fine-tuning\\n45.3\\n47.6\\n54.1\\n35.2\\n48.7\\nFlan-T5Large w/ Contriever\\n50.7\\n50.5\\n56.4\\n38.9\\n61.1\\nFlan-T5Large w/ ANCE\\n49.2\\n49.3\\n56.7\\n38.1\\n57.2\\nFlan-T5Large w/ AARContriever (Ours)\\n51.8\\n50.8\\n59.7\\n39.4\\n61.8\\nFlan-T5Large w/ AARANCE (Ours)\\n50.4\\n48.0\\n58.1\\n39.3\\n60.2\\nFlan-T5XL\\n51.2\\n55.5\\n57.4\\n38.1\\n58.7\\nFlan-T5XL w/ Contriever\\n56.4\\n57.3\\n66.1\\n43.9\\n63.2\\nFlan-T5XL w/ ANCE\\n55.3\\n55.9\\n64.0\\n41.5\\n64.9\\nFlan-T5XL w/ AARContriever (Ours)\\n56.7\\n57.7\\n65.4\\n43.6\\n65.1\\nFlan-T5XL w/ AARANCE (Ours)\\n56.2\\n59.4\\n64.8\\n41.5\\n64.9\\nInstructGPT\\n60.2\\n65.7\\n68.0\\n46.1\\n66.5\\nInstructGPT w/ Contriever\\n60.5\\n62.0\\n71.8\\n44.3\\n70.1\\nInstructGPT w/ ANCE\\n61.6\\n62.4\\n73.4\\n47.6\\n68.6\\nInstructGPT w/ AARContriever (Ours)\\n61.5\\n64.5\\n73.1\\n45.0\\n69.9\\nInstructGPT w/ AARANCE (Ours)\\n62.2\\n62.0\\n72.0\\n49.2\\n70.7\\nTable 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.\\n', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}),\n",
       " Document(page_content='Augmenting Zero-Shot Dense Retrievers with Plug-in\\nMixture-of-Memories\\nSuyu Ge1∗, Chenyan Xiong2, Corby Rosset2, Arnold Overwijk2, Jiawei Han1, Paul Bennett2\\n1 University of Illinois Urbana-Champaign\\n2 Microsoft Research\\n{suyuge2,hanj}@illinois.edu\\n{chenyan.xiong,corbyrosset,arnold.overwijk,paul.n.bennett}@microsoft.com\\nAbstract\\nIn this paper we improve the zero-shot general-\\nization ability of language models via Mixture-\\nOf-Memory Augmentation (MoMA), a mech-\\nanism that retrieves augmentation documents\\nfrom multiple information corpora (“external\\nmemories”), with the option to “plug in” new\\nmemory at inference time. We develop a joint\\nlearning mechanism that trains the augmenta-\\ntion component with latent labels derived from\\nthe end retrieval task, paired with hard nega-\\ntives from the memory mixture. We instan-\\ntiate the model in a zero-shot dense retrieval\\nsetting by augmenting a strong T5-based re-\\ntriever with MoMA. Our model, MoMA, ob-\\ntains strong zero-shot retrieval accuracy on the\\neighteen tasks included in the standard BEIR\\nbenchmark. It outperforms systems that seek\\ngeneralization from increased model parame-\\nters and computation steps. Our analysis fur-\\nther illustrates the necessity of augmenting with\\nmixture-of-memory for robust generalization,\\nthe beneﬁts of augmentation learning, and how\\nMoMA utilizes the plug-in memory at infer-\\nence time without changing its parameters. We\\nplan to open source our code.\\n1\\nIntroduction\\nScaling up language models—with more parameters,\\ncompute, and annotation data—improves model gen-\\neralization ability on downstream applications (Raffel\\net al., 2019; Brown et al., 2020; Smith et al., 2022), but\\nwith diminishing return: linear improvements on down-\\nstream metrics often require exponentially more parame-\\nters and computing cost (Kaplan et al., 2020; Hoffmann\\net al., 2022). Hence, scaling pretrained language mod-\\nels in this way is economically unsustainable (Strubell\\net al., 2020; Bender et al., 2021; Zhang et al., 2022).\\nRetrieval augmented language models provide a\\npromising alternative. They allow language models\\nto efﬁciently access vast resources from an external cor-\\npus (Guu et al., 2020; Borgeaud et al., 2022) that serves\\nas a kind of “memory” they can refer to when making\\npredictions, alleviating the need to memorize as much\\n∗Work partly done during Suyu’s internship at Microsoft.\\ninformation in their own network parameters (Roberts\\net al., 2020). This open-book approach helps language\\nmodels to better generalize on token prediction tasks and\\nmachine translation (Khandelwal et al., 2019; Borgeaud\\net al., 2022), and tasks which already involve a ﬁrst-\\nstage retrieval component, e.g., OpenQA (Borgeaud\\net al., 2022; Izacard et al., 2022). Existing retrieval\\naugmentation methods usually stick to one single re-\\ntrieval corpus throughout training and inference so that\\nthe retrieval component can be indirectly guided by the\\nsupervision from end tasks.\\nIn this paper we improve the zero-shot generalization\\nability of language models using “mixture-of-memory”\\n(MoMA), a new retrieval augmentation mechanism. In-\\nstead of a single corpus, MoMA retrieves documents\\nfrom a “mixture” of multiple external corpora and en-\\njoys the merits of a larger and more comprehensive\\nsource of knowledge. This mechanism also allows re-\\nmoving and/or “plugging-in” new corpora during in-\\nference time, when more information from the target\\ntask is revealed, or as an additional way for users to\\ncontrol the model. Speciﬁcally, we apply MoMA on the\\nzero-shot dense retrieval task, which is the foundation of\\nmany important real-world applications (Thakur et al.,\\n2021a; Kim, 2022) and also the retrieval component of\\nrecent retrieval augmented language models (Guu et al.,\\n2020; Izacard et al., 2022). However, it is not trivial\\nto guide a retrieval model to leverage multiple corpora.\\nWe need to jointly train the augmentation component\\nand dense retriever using supervised relevance signals\\nand self-mined hard negatives.\\nWe instantiate MoMA with a T5 encoder-decoder\\nmodel (Ni et al., 2022) and apply it to the dense retrieval\\ntask (Karpukhin et al., 2020). Our end task retriever uses\\na set of augmenting documents from the mixture-of-\\nmemories to enhance its representation of the query with\\nimportant context; the retriever then uses the enhanced\\nquery representation to retrieve a ﬁnal candidate set.\\nAt inference time, we plug in the target task’s corpus\\nto the memory mixture to introduce in-domain context\\ninformation, without updating any parameter.\\nWe experimented on eighteen zero-shot dense re-\\ntrieval tasks included in BEIR (Thakur et al., 2021a), the\\nstandard ZeroDR benchmark. The results demonstrate\\nthe improved zero-shot ability of MoMA. When paired\\nwith the ANCE (Xiong et al., 2020) training framework\\narXiv:2302.03754v1  [cs.CL]  7 Feb 2023\\non a T5 model, it outperforms counterparts without the\\nMoMA augmentation component, as well as recent state-\\nof-the-art dense retrieval systems of the same scale, by\\nlarge margins. To validate its effectiveness when paired\\nwith advanced models, we further instantiate MoMA\\nwith a contrastively pretrained T5 model. MoMA then\\nachieves comparable or even stronger performance to\\nZeroDR systems with larger model scales and heavier\\ncomputation costs.\\nOur analysis reveals that large and diverse corpora in\\nthe memory leads to the best performance; while only\\nusing a single corpus during training does not improve\\nperformance on unseen target tasks. The learning of\\naugmentation component is also important for MoMA\\nto utilize the diverse information from the mixture. Our\\nanalysis and case studies illustrate how MoMA lever-\\nages the plug-in memory at testing time to enrich its\\nquery representations with in-domain information that\\nwas not available in training.\\n2\\nRelated Work\\n2.1\\nRetrieval Augmentation\\nRecent research has explored two common ways to\\nconstruct the external memory in retrieval-augmented\\nlanguage models. The ﬁrst is to retrieve similar tokens\\nfor language models to copy from when predicting the\\nnext token (Khandelwal et al., 2019; Zhong et al., 2022).\\nThe second is to retrieve the related documents (text\\nsequences) from an in-domain corpus as additional in-\\nput (Guu et al., 2020; Borgeaud et al., 2022). Our work\\nfalls into this category as document-based models bet-\\nter align with knowledge-intensive tasks (Petroni et al.,\\n2020), such as retrieval and OpenQA (Chen et al., 2017).\\nLearning to retrieve useful documents to augment the\\nlanguage model is a challenging task, since human anno-\\ntations on the usefulness of augmentation documents are\\ncostly and seldom available. The most straightforward\\nway is to use representations from raw pretrained lan-\\nguage models to ﬁnd documents similar to the task input,\\ni.e., as unsupervised dense retrieval (Guu et al., 2020;\\nBorgeaud et al., 2022). Adapting dense retrieval mod-\\nels trained for relevance matching is another common\\nchoice (Izacard and Grave, 2020b; Lewis et al., 2020;\\nYu et al., 2021). A more formal solution is to jointly\\nlearn the augmentation components end-to-end using\\nsupervision from the ﬁnal task, for example, treating the\\naugmentation as latent variables and applying EM (Zhao\\net al., 2021), or distilling the augmentation component\\nfrom feedback of the ﬁnal model (Izacard and Grave,\\n2020a). In a parallel work, Izacard et al. (2022) found\\nthe most effective one is attention distillation method\\n(ADist), which trains the augmentation component us-\\ning soft labels derived from the end model’s attention\\non augmentation documents.\\nThe motivation for query augmentation coincides\\nwith the query expansion methods in the traditional\\nIR community, whereby the user’s original query\\nis augmented by new features with similar mean-\\nings (Carpineto and Romano, 2012). As feature selec-\\ntion usually requires additional semantic analysis, the\\nefﬁciency and usability of traditional query expansion\\nmethods remain limited when faced with a new domain.\\nTo overcome this, recent work relies on dense retrieval\\nresults to expand the query (Yu et al., 2021). The re-\\ntrieved relevant documents serve as pseudo relevance\\nfeedback signals for the model, which are concatenated\\nwith the original query as the augmented model input.\\nOur work augments queries with feedback from multi-\\nple corpora and learns to select important augmentation\\ndocuments automatically.\\n2.2\\nZero-shot Dense Retrieval\\nDense retrieval models trained on a resource rich source\\ntasks, e.g., web search, usually do not perform as well\\nwhen zero-shot transferred to other domains (Thakur\\net al., 2021b). This is concerning since many impor-\\ntant real-world scenarios do not have the luxury of web\\ncorpus training signals and must rely on near zero-shot\\ntransfer, e.g., the medical domains (Kim, 2022). Xin\\net al. (2021) analyzed the challenge of shifting between\\ntraining and testing domains, and leveraged domain-\\ninvariant learning to mitigate the gap. Another common\\napproach is to ﬁrst generate domain-speciﬁc pseudo\\nlabels for each task, and then use them to train dense\\nretriever (Thakur et al., 2021b; Wang et al., 2022). Ad-\\nditionally, continuous pretraining the language model\\nalso improves its generalization ability in ZeroDR (Izac-\\nard et al., 2021; Gao and Callan, 2022; Yu et al., 2022).\\nFollowing works (Izacard et al., 2021; Yu et al., 2022)\\nfurther contrastively pretrained the retriever on source\\nor target corpus with a sentence matching loss. Other\\nmethods seek better generalization ability in ZeroDR\\nfrom various resources, for example, combining with\\nsparse retrieval to introduce exact match signals (For-\\nmal et al., 2021), using multiple vectors per documents\\nfor term-level matching (Khattab and Zaharia, 2020a),\\nor scaling up the retrieval model using larger language\\nmodels (Ni et al., 2021; Neelakantan et al., 2022).\\n3\\nMethod\\nIn this section we ﬁrst describe our Mixture-of-Memory\\nAugmentation. Then we discuss how it is jointly learned\\nwith the end system and enables plug-in memory at\\ninference time.\\n3.1\\nMixture-of-Memory Augmentation\\nBefore going to the details of MoMA, we ﬁrst recap\\nsome preliminaries in ZeroDR.\\nPreliminaries. The dense retrieval (DR) task aims to\\nﬁnd relevant documents d from a corpus C for the given\\nquery q by representing them in a shared embedding\\nspace. Speciﬁcally, the retrieval score in DR is often\\ncalculated as:\\nf(q, d) = q · d; q = g(q); d = g(d).\\n(1)\\nIt uses dot product as the scoring function to match the\\nembeddings q and d, which is known to support efﬁcient\\nnearest neighbor search (ANN) (Johnson et al., 2019). A\\npretrained language model is often the encoder of choice\\ng(). We use the ST5-EncDec variant of Sentence-T5 (Ni\\net al., 2022):\\ng(x) = Dec(Enc(x)),\\n(2)\\nwhich feeds in the text sequence (prepended by a special\\n[CLS] tokens) to the encoder of T5, Enc(), and uses\\nthe output representation of the [CLS] token from the\\ndecoder, Dec(), as the text representation. This naturally\\nleverages the attention from decoder to encoder at all\\nTransformer layers (Raffel et al., 2019), as a ﬁne-grained\\ninformation gathering mechanism.\\nThe training of dense retrieval systems often applies\\nstandard ranking loss and pairs the relevant documents\\nd+ ∈ D+ for each query q with hard negatives d− ∈\\nD−:\\nL =\\nX\\nq\\nX\\nd+∈D+\\nX\\nd−∈D−\\nl(f(q, d+), f(q, d−));\\nD− ∼ ANNC\\nf(q,◦) \\\\ D+.\\n(3)\\nEqn. 3 uses ANCE hard negatives, which are the top-\\nretrieved documents from C using the retriever it-\\nself (Xiong et al., 2020). The loss function l() can\\nbe any standard ranking loss such as cross entropy. A\\nZeroDR model is trained on qs and documents ds ∈ Cs\\nfrom a source task, often web search, and tested on tar-\\nget tasks qt and Ct; supervision signals are only present\\nfrom the source.\\nMixture-of-Memory Augmentation. The key idea\\nof (document-based) retrieval augmented language mod-\\nels is to enrich the representation g(q) with additional\\ncontextual input for the model, i.e., augmentation doc-\\numents da retrieved from an external memory M. In-\\nstead of using a single document corpus, MoMA uses\\nmultiple corpora to provide richer and more diverse ex-\\nternal resources for augmentation. For example, M\\ncan be composed by the source corpus Cs, a general\\nencyclopedia, a domain speciﬁc knowledge graph, etc.\\nThen we can retrieve the augmentation documents Da :\\nDa = ANNM\\nf a(x,◦); M = {C1, ..., CM}.\\n(4)\\nThis augmentation component uses another dense re-\\ntriever f a() (also a Sentence T5 model), with param-\\neters distinct from those in g(). Note that instead of\\nretrieving Da separately from M different ANN mem-\\nory sources and merging results, Eqn. 4 combines them\\ninto one ANN index. This requires the augmentation\\ncomponent f a() to be ﬂexible enough handle various\\ncorpora in the mixture.\\nUsing the encoder-decoder architecture for g() in\\nEqn. 2 enables a simple extension to incorporate the\\naugmentation documents using the fusion-in-decoder\\n(FiD) mechanism (Izacard and Grave, 2020b):\\ngMoMA(q) = Dec(Enc(q), Enc(da\\n1), ..., Enc(da\\nK));\\nDa = {da\\n1, ..., da\\nK}.\\n(5)\\n𝑑%\\n!\\n𝑑)\\n!\\nMedical KG\\nPlug-in\\nCorpus\\n𝑞\\n𝑓!(𝑞,∘)\\n𝒒𝒂\\nEnc\\nDec\\n[CLS]\\nMixture of Memory\\n𝑓+,+-(𝑞!,∘)\\nAug\\nAug\\nEnc\\nEnc\\nEnc\\nAttention\\nFusing\\nDec\\nFigure 1: Illustraion of the Mixture-of-Memory Aug-\\nmentation.\\nIt feeds in the K augmentation documents separately\\nto the T5 encoder of g(). Then it fuses the encoded\\ndocuments together with Enc(q) using one decoder that\\nattends to all encoded vectors, as illustrated in Figure 1.\\nThe FiD approach in Eqn 5 is a nice balance of ef-\\nﬁciency and capacity when modeling multiple text se-\\nquences (Izacard and Grave, 2020b). It is more efﬁcient\\nthan concatenating all text pieces together, while also\\nremaining expressive enough to model the nuances from\\nmany sequences. (Izacard and Grave, 2020a; Izacard\\net al., 2022).\\nWhen instantiating MoMA in the dense retrieval set-\\nting, we focus on augmenting the query representation\\nq, as queries are often short, ambiguous, and beneﬁt\\nmore from additional contextual information (Lavrenko\\nand Croft, 2017; Yu et al., 2021). This leads to the\\nfollowing deﬁnition of MoMA:\\nf MoMA(q, d) =qa · d;\\nqa = gMoMA(q),d = g(d),\\n(6)\\nusing the construction of gMoMA() in Eqn. 5 upon the\\naugmentation documents deﬁned in Eqn. 4.\\n3.2\\nJoint Learning in MoMA and Inference with\\nPlug In Memory\\nMoMA has two sets of parameters to learn, in the main\\nmodel f MoMA() and the augmentation component f a().\\nBoth have their own T5 encoder-decoder parameters.\\nThe two components are bridged by the augmentation\\ndocuments, which are retrieved by f a() from M and\\nused by f MoMA() to produce query representation qa.\\nMain Model Learning. Given the relevance labels\\nfrom the source task and an augmentation model, train-\\ning f MoMA() is straightforward. We can use the standard\\ndense retrieval training to ﬁnetune the enriched query\\nencoder gMoMA() and the document encoder g():\\nLMoMA =\\nX\\nqs\\nX\\nd+\\nX\\nd−\\nl(f MoMA(qs, d+), f MoMA(qs, d−));\\nd+ ∈ Ds+, d− ∈ Ds−\\n(7)\\nDs− ∼ ANNCs\\nf MoMA(qs,◦) \\\\ Ds+.\\n(8)\\nThe training signals come from the source task, includ-\\ning qs, its relevant documents Ds+, and ANCE hard\\nnegatives Ds− retrieved from the source corpus Cs.\\nAugmentation Learning. Training f a() is challeng-\\ning as it is hard to label whether an augmentation docu-\\nment is useful. Propagating gradients from the ﬁnal loss\\nto f a() is also prohibitive as the retrieval operation in\\nEqn. 4 is discrete. Fortunately, recent research found the\\nattention scores from the FiD decoder to each encoded\\ninputs (Eqn. 5) are good approximations to the useful-\\nness of augmentation documents (Izacard and Grave,\\n2020a):\\nFidAtt(da\\ni ) =\\nX\\nlayers\\nX\\npositions\\nX\\nheads\\nAttDec→Enc(gMoMA(da\\ni )).\\n(9)\\nIt sums the attentions from gMoMA()’s special token at\\nthe decoder’s [CLS] position over all layers, input po-\\nsitions, and attention heads. Ideally, higher FidAtt() is\\nassigned to da\\ni that provides useful contextual informa-\\ntion.\\nPreviously, FidAtt scores are often used as soft labels\\nfor the augmentation model (Izacard and Grave, 2020a;\\nIzacard et al., 2022). Doing so with memory mixtures\\nis risky as it is too sparse and overﬁts memory resource\\nthat appears earlier in the training, which are the only\\nones available for the decoder to attend on. To improve\\nthe learning robustness, we introduce ANCE-style hard\\nnegative mining to train the augmentation component\\nas well.\\nFirst, we formulate the positive set of augmentation\\ndocuments as:\\nDa+ = Ds+ ∪ Top-NFidAtt(da\\ni ),Da.\\n(10)\\nwhich combines relevant documents Ds+ and the aug-\\nmenting ones that received N-highest attention scores\\nfrom gMoMA(). Then we pair them with hard negatives\\nto formulate the training of f a() as:\\nLa =\\nX\\nqs\\nX\\nd+∈Da+\\nX\\nd−∈Da−\\nl(f a(qs, d+), f a(qs, d−));\\n(11)\\nDa− ∼ ANNM\\nf a(qs,◦) \\\\ Da+.\\n(12)\\nNotice the negatives for f a() have comprehensive cov-\\nerage from multiple corpora.\\nIterative Training. The learning of f MoMA() and\\nf a() is an iterative process that ﬁts naturally into the\\ntraining procedure of dense retrieval training with hard\\nnegatives. We follow the standard iterations in ANCE\\nand construct the t-th training episode of MoMA:\\n1. Construct hard negatives Ds− via Eqn. 8 using\\nweights f MoMA\\nt−1\\n() from the last episode;\\n2. Retrieve augmentation Da via Eqn. 4 using\\nweights f a\\nt−1() from the last episode;\\n3. Train f MoMA\\nt\\n() as Eqn. 7;\\n4. Formulate new positive augmentation docu-\\nments Da+, using updated attention scores from\\nf MoMA\\nt\\n(), and mine negative augmentation docu-\\nments Da− using f a\\nt−1();\\n5. Train f a\\nt () following Eqn. 11.\\nBoth f MoMA\\n0\\n() and f a\\n0 () can be initialized with a BM25\\nwarmed-up T5 retriever. Steps 1 and 3 above are in-\\nherited from standard dense retrieval training. The rest\\nare introduced by MoMA. The additional computation\\nin the training side mainly resides updating the index\\nfor the memory mixture, a standard cost in retrieval-\\naugmented language models (Guu et al., 2020; Izacard\\net al., 2022).\\nZero-Shot Retrieval with Plug in Memories. To\\nperform zero-shot retrieval on unseen tasks, MoMA\\nﬁrst retrieves augmented documents using f a() from M\\nfor the target query qt, and retrieves target documents\\ndt ∈ Ct with the augmented model f MoMA() without\\nchanging any model parameters. MoMA allows f a()\\nto attend over the target corpus as well if it is plugged\\nin: M = M ∪ Ct \\\\ Cs, which conveys in-domain\\ninformation. The augmenting corpus can also be engi-\\nneered by users manually to inject their preference or\\ndomain knowledge, e.g., as “memory engineering”. In\\nthis work we focus on swapping out the source corpus\\nfor the target corpus; we leave other explorations for\\nfuture work.\\n4\\nExperimental Methodologies\\nDatasets.\\nWe choose the MS MARCO passage\\ndataset (Bajaj et al., 2016) as the source domain dataset,\\nwhereas the target domains are from the 18 datasets\\nin BEIR (Thakur et al., 2021b) benchmark, which in-\\nclude including biomedical, scientiﬁc and ﬁnancial texts.\\nMore details can be found in Appendix A.1. The evalu-\\nation metric NDCG@10 is the same with BEIR bench-\\nmark, which measures Normalized Discounted Cumula-\\ntive Gain (Wang et al., 2013) of top 10 prediction. The\\nhigher NDCG@10 value indicates better performance.\\nAugmenting Corpora. During training, the mixture-\\nof-memory is composed of source training corpus\\n(MARCO), Wikipedia and a medical knowledge\\ngraph.\\nWe use the Wikipedia chunk prepossessed\\nby (Karpukhin et al., 2020) without further process-\\ning1. The medical knowledge graph is extracted from\\nthe Medical Subject Headings (MeSH)2, an open-source\\ndatabase for indexing and cataloging of biomedical and\\nhealth-related information. Since it is hierarchical in\\nstructure, we linearize it by concatenating spans with\\ntext information. During testing, we directly replace\\nMARCO with the corresponding document sets from\\nBEIR. Each task from BEIR is augmented indepen-\\ndently. More dataset and preprocessing details can be\\nfound in Appendix A.1.\\nBaselines and Model Choices. We compare our\\nMoMA with standard sparse and dense retrieval mod-\\nels on BEIR. We also compare MoMA with advanced\\n1https://huggingface.co/datasets/wiki_dpr\\n2https://www.ncbi.nlm.nih.gov/mesh/\\nTable 1: NDCG@10 on the BEIR benchmark. We also include an averaged score on datasets used by Contriever\\nfor a fair comparison. The best result each task is marked bold. An ∗ denotes unfair comparison, as NQ is used in\\ntraining for GTR. †: GenQ generated pseudo labels to train an independent model for each task. ‡: Larger models\\nBM25\\nDPR\\nANCE\\nT5-ANCE\\ncoCondenser\\nGenQ†\\nColBERT\\nContriever\\nGTRbase∗\\nGTRlarge∗‡\\nMoMA\\n(T5-ANCE)\\nMoMA\\n(COCO)\\nParameters#\\n—\\n110M\\n110M\\n110M*2\\n110M\\n66M*18\\n110M\\n110M\\n110M\\n335M\\n110M*2\\n110M*2\\nTREC-COVID\\n0.656\\n0.575\\n0.654\\n0.653\\n0.715\\n0.619\\n0.677\\n0.596\\n0.539\\n0.557\\n0.762\\n0.761\\nBioASQ\\n0.465\\n0.232\\n0.306\\n0.322\\n0.318\\n0.398\\n0.474\\n—\\n0.271\\n0.320\\n0.372\\n0.371\\nNFCorpus\\n0.325\\n0.210\\n0.237\\n0.275\\n0.307\\n0.319\\n0.305\\n0.328\\n0.308\\n0.329\\n0.307\\n0.333\\nNQ\\n0.329\\n0.398\\n0.446\\n0.452\\n0.494\\n0.358\\n0.524\\n0.498\\n0.495\\n0.547\\n0.490\\n0.544\\nHotpotQA\\n0.603\\n0.371\\n0.456\\n0.487\\n0.566\\n0.534\\n0.593\\n0.638\\n0.535\\n0.579\\n0.539\\n0.589\\nFiQA-2018\\n0.236\\n0.274\\n0.295\\n0.294\\n0.285\\n0.308\\n0.317\\n0.329\\n0.349\\n0.424\\n0.320\\n0.329\\nSignal-1M\\n0.330\\n0.238\\n0.249\\n0.246\\n0.274\\n0.281\\n0.274\\n—\\n0.261\\n0.265\\n0.258\\n0.264\\nTREC-NEWS\\n0.398\\n0.366\\n0.382\\n0.379\\n0.389\\n0.396\\n0.393\\n—\\n0.337\\n0.343\\n0.413\\n0.453\\nRobust04\\n0.408\\n0.344\\n0.392\\n0.412\\n0.399\\n0.362\\n0.391\\n—\\n0.437\\n0.470\\n0.469\\n0.475\\nArguAna\\n0.414\\n0.414\\n0.415\\n0.415\\n0.411\\n0.493\\n0.233\\n0.446\\n0.511\\n0.525\\n0.438\\n0.463\\nTouché-2020\\n0.367\\n0.208\\n0.240\\n0.312\\n0.190\\n0.182\\n0.202\\n0.230\\n0.205\\n0.219\\n0.271\\n0.299\\nQuora\\n0.789\\n0.842\\n0.852\\n0.836\\n0.863\\n0.830\\n0.854\\n0.865\\n0.881\\n0.890\\n0.847\\n0.843\\nDBPedia-entity\\n0.313\\n0.236\\n0.281\\n0.290\\n0.356\\n0.328\\n0.392\\n0.413\\n0.347\\n0.391\\n0.347\\n0.383\\nSCIDOCS\\n0.158\\n0.107\\n0.122\\n0.115\\n0.140\\n0.143\\n0.145\\n0.165\\n0.149\\n0.158\\n0.143\\n0.145\\nFever\\n0.753\\n0.589\\n0.669\\n0.655\\n0.678\\n0.669\\n0.771\\n0.758\\n0.660\\n0.712\\n0.723\\n0.745\\nClimate-Fever\\n0.213\\n0.176\\n0.198\\n0.194\\n0.184\\n0.175\\n0.184\\n0.237\\n0.241\\n0.262\\n0.235\\n0.233\\nSciFact\\n0.665\\n0.475\\n0.507\\n0.566\\n0.600\\n0.644\\n0.671\\n0.677\\n0.600\\n0.639\\n0.632\\n0.630\\nCQADupStack\\n0.299\\n0.281\\n0.296\\n0.283\\n0.330\\n0.347\\n0.350\\n0.345\\n0.357\\n0.384\\n0.283\\n0.294\\nContriever Sub Avg\\n0.437\\n0.368\\n0.408\\n0.416\\n0.438\\n0.425\\n0.445\\n0.466\\n0.442\\n0.471\\n0.453\\n0.471\\nAvg\\n0.428\\n0.352\\n0.391\\n0.399\\n0.417\\n0.410\\n0.431\\n—\\n0.416\\n0.444\\n0.436\\n0.453\\nTable 2: Computational analysis in the pretraining stage\\nof different models.\\nModel\\nPretraining Corpus\\nBatch Size\\nTraining Steps\\nMoMA (T5-ANCE)\\n0\\n0\\n0\\nMoMA (COCO)\\nMARCO\\n128\\n50k\\nGTR\\nNQ, CQA\\n2048\\n800k\\nContriever\\nCCNet\\n2048\\n500k\\nWikipedia\\n2048\\n200k\\napproaches that are speciﬁcally designed for zero-shot\\ngeneralization. They involve techniques that are not di-\\nrectly comparable with this paper, including pretraining\\non extra data, in-domain continuous pretraining, and\\ngenerating target pairs using another pretrained gener-\\native model. Besides, some baselines use larger scale\\nlanguage model as their backbone. We list the details of\\nbaselines in Appendix A.2.\\nAs a plug-in-and-play method, MoMA can be com-\\nbined with other techniques. We initiate MoMA on\\ntwo versions of T5 model checkpoints.\\nThe primi-\\ntive MoMA (T5-ANCE) is built on the original T5\\nmodel checkpoint. By comparing it with T5-ANCE,\\nwe can clearly observe the performance gain brought\\nby MoMA. To demonstrate it can integrate techniques\\nfrom other models to achieve higher performances, we\\napply MoMA with a better pretrained T5-based model.\\nFollowing previous work (Gao and Callan, 2022; Yu\\net al., 2022), we continuously trained the T5 model on\\nthe MARCO corpus using a sentence-level contrastive\\nloss, combined with the original masked language mod-\\neling loss. We then performed the same MoMA training\\non top of the continuously pretrained T5 checkpoint\\nand denoted it as MoMA (COCO). Both MoMA (T5-\\nANCE) and MoMA (COCO) are trained iteratively\\nwith ANCE-style (Xiong et al., 2020) hard negatives,\\nthe only difference is the initialized model start point.\\nWe compare their pretraining details with other models\\nin Table 2. Unlike previous work (Yu et al., 2022), we\\ndid not include target datasets and augmenting corpora\\nin the COCO pretraining stage. Since MARCO contains\\nonly 0.5M documents, it adds fewer computational over-\\nhead compared to other methods listed in the table, e.g.,\\nContriever.\\nImplementation Details. For MoMA, we use the T5-\\nbase (Raffel et al., 2019) architecture (12-layer Trans-\\nformer, 768 hidden size) by directly loading the check-\\npoint from HuggingFace3. To warm up the language\\nmodel for dense retrieval, we followed (Xiong et al.,\\n2020) to further train it using BM25 negatives for 10\\nepochs. After warming up, we jointly trained the two\\ncomponents for three episodes, each episode including\\nthree training epochs. After three joint episodes, the end\\nretriever reaches the best performance on MSMARCO,\\nso we select this checkpoint for evaluation. The ratio\\nbetween positive and hard negative pairs is 1:7 for both\\nmodels. The main hyperparameters in MoMA include\\nthe total number of grounding documents K and the at-\\ntention threshold number N in Equation 10. We directly\\nset K=10 and N=5 without any parameter tuning. More\\ndetails on hyperparameters and experimental settings\\ncan be found in Appendix A.3.\\n5\\nEvaluation Results\\nOur experiments evaluate the zero-shot ability of\\nMoMA, its performance with different memory sources,\\nthe inﬂuence of memory mixture learning, and the ben-\\neﬁts of plug-in memory.\\n5.1\\nZero-Shot Retrieval Accuracy and Efﬁciency\\nThe retrieval accuracy of MoMA and baselines are listed\\nin Table 1. Besides baselines of similar parameter count,\\nwe also include larger models (GTRlarge) or those us-\\ning multiple vectors per document (ColBERT). MoMA\\n(COCO) shows the strongest zero-shot accuracy against\\nprevious state-of-the-art methods that do continuous\\ncontrastive pretraining (coCondenser), generate pseudo\\nlabels (GenQ), or consume additional training signals\\n3https://huggingface.co/t5-base\\nTable 3: Efﬁciency of MoMA search and training.\\nOperation\\nOfﬂine\\nOnline\\nBM25 Index Build\\n1.8h\\n—\\nBM25 Retrieval Per Query\\n—\\n43ms\\nMoMA Inference\\nEncoding of Corpus/Per Doc\\n1.5h/4.5ms\\n—\\nQuery Encoding\\n—\\n55ms\\nANN Retrieval (batched q)\\n—\\n9ms\\nDense Retrieval Total\\n—\\n64ms\\nMoMA Training\\nEncoding of Corpus/Per Doc\\n1.5h/4.5ms\\n—\\nANN Index Build\\n10s\\n—\\nNeg Construction Per Batch (32 queries)\\n45ms\\n—\\nBack Propagation Per Batch (32 queries)\\n330ms\\n—\\nin both continuous pretraining and ﬁnetuning phrases\\n(GTRbase). MoMA (T5-ANCE) also achieved nearly\\ncomparable zero-shot accuracy against larger models\\nlike GTRlarge, and ColBERT, which scales up the num-\\nber of vectors per documents (one per token). This\\nconﬁrms that retrieval-augmentation provides another\\npath to improve language models’ generalization ability\\nbesides scaling up. MoMA (T5-ANCE) also outper-\\nforms T5-ANCE, which MoMA (T5-ANCE) uses as\\na subroutine for retrieval augmentation, on all but one\\nretrieval task, showing the robustly improved general-\\nization ability from plug-in mixture of memory.\\nWe evaluate the efﬁciency of MoMA in two stages:\\nofﬂine model training and online inference. In ofﬂine\\ntraining from Table 2, MoMA (T5-ANCE) is signiﬁ-\\ncantly cheaper than other methods as we do not re-\\nquire pretraining on large external corpora, which saves\\nhundreds of hours training time. MoMA (COCO) addi-\\ntionally pretrain on MARCO for 50k steps, which is far\\nfewer than the other compared methods. In online in-\\nference, similar with other retrieval enhanced language\\nmodels, MoMA imposes a necessary cost of retrieval\\naugmented model upon the baseline T5-ANCE. We fur-\\nther provide detailed efﬁciency analysis on MoMA in\\nTable 3. The online latency is measured on one query\\nand 100 retrieved documents. Due to the query augmen-\\ntation, query encoding is more costly and takes about\\n55ms per query. Even with the augmentation cost, the\\nfull dense retrieval total online inference cost is 64ms,\\nonly slightly above the BM25 retrieval latency. The\\nANN retrieval is very efﬁcient, only takes 9ms. In ad-\\ndition, the complexity of ANN retrieval is sub-linear\\nto the corpus size, in most ANN framework such as\\nFAISS. Thus the extra round of ANN retrieval operation\\nin MoMA is not the bottleneck even when the size of\\nmemory mixture scales up.\\n5.2\\nPerformance with Different Memories\\nTable 4 evaluates how MoMA behaves under different\\ncombinations of external memories. Compared with the\\nMoMA (T5-ANCE), MoMA (COCO) may lean towards\\nthe MARCO corpus since it is continuously pretrained\\non it. To avoid unfair comparison between MARCO\\nand other corpora, we choose MoMA (T5-ANCE) as\\nthe Full model version for ablation studies. Unsurpris-\\ningly, using a single out-of-domain memory for retrieval\\naugmentation does not help, for example, even though\\nMARCO is the source domain corpus, solely grounding\\non it reduces zero-shot accuracy. MeSH as the sole aug-\\nmenting corpus also lowers performance, even on some\\nmedical retrieval tasks such as BioASQ. Interestingly,\\nwhen we expand the memory to include MARCO, Wiki,\\nand MeSH, but keep the target corpus excluded (w/o\\nTarget), MoMA exhibits better accuracy compared to\\nthe no-memory T5-ANCE. Our conclusion is that more\\nmemory sources achieves better generalization, espe-\\ncially when no target domain information is available.\\nIn the Full setting, the 3-memory mixture of MARCO,\\nWiki, and MeSH is jointly learned with ﬁnal task at\\ntraining time. At test time, MARCO is swapped out for\\nthe target corpus. The Full improves zero-shot accuracy\\nover both the w/o Target setting (where the target corpus\\nis excluded at test time), and the w/o Learning setting\\n(wherein the augmentation component is not learned).\\nAs expected, plugging in the target corpus at test time\\nis the most valuable source of generalization power. It\\nis also the most realistic, as access to the target corpus\\nmay only be available at testing time.\\n5.3\\nEffect of Memory Mixture Learning\\nTo study the effect of our joint learning mechanism on\\nthe memory mixture, we compare it with recent state-\\nof-the-art Attention Distillation (ADist), which is ﬁrst\\nused in Izacard and Grave (2020a) and recently updated\\nin a parallel work Izacard et al. (2022). It jointly trains\\nthe augmentation model using attention scores from the\\nend language model as pseudo-labels. We also enrich\\nADist with relevance labels from MARCO for more\\ndirect supervision, which was shown to be effective in\\ndistilling a dense retriever from stronger cross-encoder\\nranking model (Hofstätter et al., 2021). Similar to previ-\\nous section, to exclude the performance gain brought by\\ncontrastive pretraining, we choose MoMA (T5-ANCE)\\nas our own method for comparison. The performances\\nof these joint learning methods are listed in Table 5. We\\npick six BEIR tasks whose domains are closely related\\nto the augmentation corpora: TREC-COVID, BIOASQ,\\nand NFCorpus are medical search and closely related to\\nMeSH. NQ, HotpotQA, and FEVER are all Wikipedia\\nbased. The results show that ADist, either standalone\\nor enriched with MARCO labels, does not improve the\\nﬁnal accuracy compared to using a supervised dense\\nretriever as the augmentation component without joint\\nlearning. The main difference is that the supervised\\nretriever has been trained effectively using hard neg-\\native sampling (Xiong et al., 2020). Jointly learning\\nusing soft labels without hard negatives downgraded\\nthe augmentation accuracy. Hence, MoMA is a simple\\ntechnique to learn the end task signals via the attention\\nscores together with hard negatives, which improves\\nquality over a supervised retriever alone.\\nTo further illustrate the joint training process, we\\ntrack the attention scores of documents from different\\nTable 4: NDCG@10 of MoMA (T5-ANCE) under different memory compositions: no memory, single memory,\\nand a mixture of memories. w/o Learning uses the end retriever to select augmenting documents without use of an\\naugmentation component. w/o Target excludes the target from memory.\\nNo Memory\\nSingle Memory\\nMemory Mixture\\nT5-ANCE\\nMARCO\\nWiki\\nMeSH\\nTarget\\nw/o Learning\\nw/o Target\\nFull\\nTREC-COVID\\n0.653\\n0.576\\n0.592\\n0.669\\n0.731\\n0.759\\n0.664\\n0.762\\nBioASQ\\n0.322\\n0.247\\n0.262\\n0.219\\n0.361\\n0.359\\n0.271\\n0.372\\nNFCorpus\\n0.275\\n0.295\\n0.302\\n0.282\\n0.319\\n0.317\\n0.301\\n0.307\\nNQ\\n0.452\\n0.472\\n0.486\\n0.393\\n0.483\\n0.510\\n0.484\\n0.490\\nHotpotQA\\n0.487\\n0.481\\n0.519\\n0.462\\n0.538\\n0.539\\n0.520\\n0.539\\nFiQA-2018\\n0.294\\n0.296\\n0.286\\n0.280\\n0.320\\n0.304\\n0.285\\n0.320\\nSignal-1M\\n0.246\\n0.239\\n0.225\\n0.238\\n0.250\\n0.248\\n0.240\\n0.258\\nTREC-NEWS\\n0.379\\n0.381\\n0.391\\n0.372\\n0.416\\n0.410\\n0.398\\n0.413\\nRobust04\\n0.412\\n0.435\\n0.443\\n0.428\\n0.483\\n0.446\\n0.452\\n0.469\\nArguAna\\n0.415\\n0.439\\n0.438\\n0.442\\n0.439\\n0.427\\n0.438\\n0.438\\nTouché-2020\\n0.312\\n0.281\\n0.281\\n0.252\\n0.331\\n0.275\\n0.272\\n0.271\\nQuora\\n0.836\\n0.809\\n0.798\\n0.835\\n0.781\\n0.813\\n0.812\\n0.847\\nDBPedia-entity\\n0.290\\n0.340\\n0.341\\n0.287\\n0.335\\n0.331\\n0.342\\n0.347\\nSCIDOCS\\n0.115\\n0.128\\n0.121\\n0.130\\n0.146\\n0.134\\n0.127\\n0.143\\nFever\\n0.655\\n0.663\\n0.735\\n0.610\\n0.694\\n0.718\\n0.737\\n0.723\\nClimate-Fever\\n0.194\\n0.231\\n0.238\\n0.231\\n0.228\\n0.222\\n0.240\\n0.235\\nSciFact\\n0.566\\n0.583\\n0.587\\n0.585\\n0.624\\n0.618\\n0.598\\n0.632\\nCQADupStack\\n0.283\\n0.207\\n0.218\\n0.203\\n0.283\\n0.235\\n0.215\\n0.283\\nAvg\\n0.399\\n0.395\\n0.403\\n0.384\\n0.431\\n0.426\\n0.411\\n0.436\\nTable 5: Zero-shot Performances of different distillation methods. We observe consistent trend on all BEIR datasets.\\nWe present results on 6 representative datasets from Wikipedia or medical domains.\\nDistillation Method\\nTREC-COVID\\nBIOASQ\\nNFCorpus\\nNQ\\nHotpotQA\\nFEVER\\nAvg\\nSoft Attention Distill\\nADist (Izacard et al., 2022)\\n0.609\\n0.185\\n0.227\\n0.351\\n0.387\\n0.615\\n0.396\\nADist + MSMARCO rel\\n0.664\\n0.220\\n0.255\\n0.397\\n0.394\\n0.624\\n0.426\\nw/o Distilling (Fixed)\\n0.741\\n0.361\\n0.301\\n0.472\\n0.513\\n0.684\\n0.512\\nMoMA (T5-ANCE)\\n0.762\\n0.372\\n0.307\\n0.490\\n0.539\\n0.723\\n0.532\\nmemory sources as well as their ratio in the augmenta-\\ntion set in Figure 2. We also split MARCO documents\\nby whether they are labeled as Relevant (Rel) for the\\ncorresponding query.\\nFirstly, MoMA learns to increasingly attend to, and\\nretrieve, relevant documents from the memory mixture\\nthroughout training. In Figure 2a, more attention is\\npaid to MARCO Relevant documents than to any other\\ntype in the memory. Although the number of MARCO\\nRelevant documents is not signiﬁcant as a percentage of\\nthe augmenting set in Figure 2c, a query level analysis\\nconﬁrms that percentage of queries having at least one\\nrelevant document in the augmenting set increases from\\n46% in Epi-0 to 62% in Epi-2.\\nThis apparent discrepancy can be explained by the\\nfact that MARCO has only one relevant label per query\\non average, leaving plenty of room for other types of\\ndocuments to be included in the augmenting set.\\nSecondly, the amount of attention paid to certain\\ntypes of documents by MoMA is positively correlated\\nwith their representation in the augmenting set. This\\nconﬁrms that the joint learning effectively conveys the\\nfeedback signals from the end model to the augmenta-\\ntion component. For instance, in Figure 2a, MoMA pays\\na high level of attention to MARCO Other documents, a\\nsignal reﬂected in the composition of its augmentation\\nset in Figure 2c. Even though MARCO Other doc-\\numents were not labeled relevant for the query, they\\ncan still prove to be valuable as an augmenting docu-\\nment because they may contain partial information that\\nhelps query understanding (Lavrenko and Croft, 2017)\\nor it was simply not annotated in MARCO’s sparse\\nlabels (Bajaj et al., 2016). In comparison, the correla-\\ntion of the two in ADist is weak as the model seems to\\ninclude 60% augmenting documents from MeSH, far\\ngreater than the fraction of medical queries in MARCO.\\n5.4\\nGeneralization of Plug-In Memory\\nIn the previous section, we observed how MoMA learns\\nto attend to, and retrieve, informative documents from\\nmemories on which it was trained. In this section, we\\nexamine the zero-shot behavior of MoMA (T5-ANCE)\\non new corpora plugged-in at test time (keeping Wiki\\nand MeSH as before).\\nFigure 3 compares documents from the plugged-in\\ntarget versus the remaining memory mixture in terms of\\nmembership in the augmenting set (Doc Ratio) and at-\\ntention. Again, on all tasks, MoMA (T5-ANCE) heavily\\nattends to – and successfully retrieves – in-domain doc-\\numents, even if those in-domain documents were only\\njust plugged in. This conﬁrms that the augmentation\\nmodel achieves the zero-shot ability to capture relevant\\ninformation from unseen corpora.\\nIn the medical domain, the model pays more attention\\nEpi-0\\nEpi-1\\nEpi-2\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nMeSH\\nWiki\\nMarco Rel\\nMarco Others\\n(a) MoMA Att. Score.\\nEpi-0\\nEpi-1\\nEpi-2\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n(b) ADist Att. Score.\\nEpi-0\\nEpi-1\\nEpi-2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(c) MoMA Doc Ratio.\\nEpi-0\\nEpi-1\\nEpi-2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(d) ADist Doc Ratio.\\nFigure 2: Grounding component breakdown for different distillation methods in each learning iteration. We display\\nthe regularized doc and att. score ratio of documents from different augmentation sources.\\nNQ\\nHotpotQA\\nFEVER\\n0\\n20\\n40\\n60\\n80\\n100\\nTarget\\nWiki\\nMeSH\\n(a) Doc Ratio. (Wiki)\\nNFCorpus\\nTREC-Covid\\nBIOASQ\\n0\\n20\\n40\\n60\\n80\\n100\\n(b) Doc Ratio. (Med)\\nNQ\\nHotpotQA\\nFEVER\\n0\\n20\\n40\\n60\\n80\\n100\\n(c) Att. Score Ratio. (Wiki)\\nNFCorpus\\nTREC-Covid\\nBIOASQ\\n0\\n20\\n40\\n60\\n80\\n100\\n(d) Att. Score Ratio. (Med)\\nFigure 3: The inclusion of Plug-In memory during testing (grouped by the Wiki and Medical domains).\\nto MeSH documents, especially on TREC-Covid task\\nsince MeSH includes high quality updated information\\nrelated to COVID-19. Wikipedia documents received\\nmore attention on the Wiki-centric tasks like FEVER, as\\nexpected. Some tasks may need a small amount of pre-\\ncise information from Wikipedia to answer the detailed\\nquestion, e.g. in HotpotQA. Similar with the training\\nprocess, there is a non-trivial correspondence between\\nattention score of a memory and its membership in the\\naugmentation set.\\n5.5\\nCase Studies\\nTable 6 shows examples of how augmenting documents\\nchosen by MoMA can provide valuable contextual in-\\nformation for the query. The ﬁrst example is a training\\nquery from MARCO, where the augmenting documents\\nhelp disambiguate the query word \"rating\". In the sec-\\nond one, documents from the ofﬁcial Wiki and Hot-\\npotQA’s Wiki corpus are descriptions of the two entities\\nin HotpotQA’s comparison question. It illustrates how\\nMoMA provides more comprehensive augmentation by\\nincorporating information from different sources.\\n6\\nConclusion\\nIn this paper we propose a new plug-in mixture-of-\\nmemory mechanism for the retrieval augmented lan-\\nguage models to improve their zero-shot ability on the\\ndense retrieval task. To learn the memory mixture we\\ndevelop a new joint learning approach that trains the\\naugmentation component using the positive signals from\\nthe end task, the language model’s attention scores, and\\nTable 6: MoMA retrieves augmenting documents during\\ntraining (Marco) and testing (BEIR).\\nQueries\\nAugmentation Docs\\nTraining\\n[Marco]\\nWhat\\nis\\nhotel\\ntran-\\nsylvania\\nrated\\n[Marco] Why is Hotel Transylvania 2 rated\\nPG? It is rated PG for some scary images,\\naction and rude humor. [Wiki] Another re-\\nview aggregate calculated an average score\\nof 47 out of 100, indicating “mixed or av-\\nerage reviews”.\\nZero-Shot Testing\\n[HotpotQA]\\nWere Scott\\nDerrickson\\nand\\nEd\\nWood\\nof\\nthe\\nsame\\nnationality?\\n[Wiki] Scott Derrickson (born July 16,\\n1966) is an American director, screenwriter\\nand producer. [HotpotQA] Edward Davis\\nWood Jr. (October 10, December 10, 1978)\\nwas an American ﬁlmmaker, actor, writer,\\nproducer, and director.\\nhard negatives retrieved from the mixture of augmen-\\ntation corpora. This leads to our ﬁnal model MoMA\\n(T5-ANCE) and MoMA (COCO) that achieve strong\\nzero-shot accuracy on 18 retrieval tasks included in\\nBEIR. Our analysis shows the importance of augment-\\ning with diverse memory sources and in-domain infor-\\nmation for robust generalization. We also share our\\nobservations and insights on how the model learns to\\nleverage the augmentation information from multiple\\ncorpora during training and testing. We hope our ﬁnd-\\nings and illustrations can inspire more future research in\\nbetter augmenting language models, to provide other al-\\nternatives to achieve generalization ability beyond solely\\nrelying on model scale.\\nLimitations\\nAlthough MoMA (T5-ANCE) and MoMA (COCO)\\nachieve strong zero-shot performances, we mainly ver-\\nify their efﬁcacy from the empirical performances\\non BEIR tasks, where the target corpora, Wiki and\\nMARCO serve as readily available retrieval sources.\\nIn a real-world scenario, the grounding corpora usually\\nneed to be customized according to query domains and\\nuser needs. Thus, how to choose effective grounding\\ncorpora and efﬁciently evaluate their relative contribu-\\ntion remain an open problem. These analyses will go\\nbeyond our empirical settings and reveal a wider appli-\\ncation scenario of MoMA.\\nEthics Statement\\nAll data in this study are publicly available and used\\nunder ethical considerations. Text and ﬁgures in the\\npaper are used for illustration only, they do not represent\\nthe ethical attitude of the authors.\\nReferences\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\\net al. 2016. MS MARCO: A human generated ma-\\nchine reading comprehension dataset. arXiv preprint\\narXiv:1611.09268.\\nEmily M Bender, Timnit Gebru, Angelina McMillan-\\nMajor, and Shmargaret Shmitchell. 2021. On the\\ndangers of stochastic parrots: Can language models\\nbe too big? In Proceedings of the 2021 ACM Confer-\\nence on Fairness, Accountability, and Transparency,\\npages 610–623.\\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\\nPanchenko, Chris Biemann, Benno Stein, Henning\\nWachsmuth, Martin Potthast, and Matthias Hagen.\\n2020.\\nOverview of Touché 2020: Argument Re-\\ntrieval. In Working Notes Papers of the CLEF 2020\\nEvaluation Labs, volume 2696 of CEUR Workshop\\nProceedings.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste\\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\\nImproving language models by retrieving from tril-\\nlions of tokens. In International Conference on Ma-\\nchine Learning, pages 2206–2240. PMLR.\\nVera Boteva, Demian Gholipour, Artem Sokolov, and\\nStefan Riezler. 2016. A full-text learning to rank\\ndataset for medical information retrieval. In Euro-\\npean Conference on Information Retrieval, pages\\n716–722. Springer.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nClaudio Carpineto and Giovanni Romano. 2012. A\\nsurvey of automatic query expansion in information\\nretrieval. Acm Computing Surveys (CSUR), 44(1):1–\\n50.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to Answer Open-\\nDomain Questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 1870–1879.\\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\\nDowney, and Daniel Weld. 2020.\\nSPECTER:\\nDocument-level\\nrepresentation\\nlearning\\nusing\\ncitation-informed transformers.\\nIn Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2270–2282, Online.\\nAssociation for Computational Linguistics.\\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leippold.\\n2020. CLIMATE-FEVER: A dataset for veriﬁca-\\ntion of real-world climate claims. arXiv preprint\\narXiv:2012.00614.\\nThibault Formal, Benjamin Piwowarski, and Stéphane\\nClinchant. 2021. Splade: Sparse lexical and expan-\\nsion model for ﬁrst stage ranking. In Proceedings\\nof the 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval,\\npages 2288–2292.\\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\\npus aware language model pre-training for dense pas-\\nsage retrieval. In ACL 2022.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\\nand Ming-Wei Chang. 2020. REALM: Retrieval-\\naugmented language model pre-training. In ICML.\\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisz-\\ntian Balog, Svein Erik Bratsberg, Alexander Kotov,\\nand Jamie Callan. 2017. DBpedia-Entity v2: A test\\ncollection for entity search. In Proceedings of the\\n40th International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval,\\nSIGIR ’17, page 1265–1268, New York, NY, USA.\\nAssociation for Computing Machinery.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\\nford, Diego de Las Casas, Lisa Anne Hendricks,\\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\\ning compute-optimal large language models. arXiv\\npreprint arXiv:2203.15556.\\nSebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\\nYang, Jimmy Lin, and Allan Hanbury. 2021.\\nEf-\\nﬁciently teaching an effective dense retriever with\\nbalanced topic aware sampling. In Proceedings of\\nSIGIR 2021, pages 113–122.\\nSebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\\nYang, Jimmy Lin, and Allan Hanbury. 2021.\\nEf-\\nﬁciently teaching an effective dense retriever with\\nbalanced topic aware sampling. In Proceedings of\\nthe 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval,\\npage 113–122. Association for Computing Machin-\\nery.\\nDoris Hoogeveen, Karin M. Verspoor, and Timothy\\nBaldwin. 2015. CQADupStack: A benchmark data\\nset for community question-answering research. In\\nProceedings of the 20th Australasian Document Com-\\nputing Symposium, ADCS ’15, New York, NY, USA.\\nAssociation for Computing Machinery.\\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\\nbastian Riedel, Piotr Bojanowski, Armand Joulin, and\\nEdouard Grave. 2021. Towards unsupervised dense\\ninformation retrieval with contrastive learning. arXiv\\npreprint arXiv:2112.09118.\\nGautier Izacard and Edouard Grave. 2020a. Distilling\\nknowledge from reader to retriever for question an-\\nswering. arXiv preprint arXiv:2012.04584.\\nGautier Izacard and Edouard Grave. 2020b. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2022. Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint\\narXiv:2208.03299.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\\nBillion-scale similarity search with gpus.\\nIEEE\\nTransactions on Big Data, 7(3):535–547.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\\nScaling laws for neural language models.\\narXiv\\npreprint arXiv:2001.08361.\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020.\\nDense passage retrieval for\\nopen-domain question answering.\\narXiv preprint\\narXiv:2004.04906.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. arXiv preprint arXiv:1911.00172.\\nOmar Khattab and Matei Zaharia. 2020a. Colbert: Efﬁ-\\ncient and effective passage search via contextualized\\nlate interaction over bert. In Proceedings of the 43rd\\nInternational ACM SIGIR conference on research and\\ndevelopment in Information Retrieval, pages 39–48.\\nOmar Khattab and Matei Zaharia. 2020b. Colbert: Ef-\\nﬁcient and effective passage search via contextual-\\nized late interaction over bert. In Proceedings of\\nthe 43rd International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval,\\npage 39–48, New York, NY, USA. Association for\\nComputing Machinery.\\nYubin Kim. 2022. Applications and future of dense\\nretrieval in industry. In Proceedings of the 45th In-\\nternational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval, pages 3373–\\n3374.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: A benchmark for question answering\\nresearch. Transactions of the Association for Compu-\\ntational Linguistics, 7:452–466.\\nVictor Lavrenko and W Bruce Croft. 2017. Relevance-\\nbased language models. In ACM SIGIR Forum, vol-\\nume 51, pages 260–267. ACM New York, NY, USA.\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. arXiv preprint\\narXiv:2005.11401.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa:\\nA Robustly Optimized BERT Pretraining Approach.\\narXiv preprint arXiv:1907.11692.\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\\nweight decay regularization. In International Confer-\\nence on Learning Representations.\\nJing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni,\\nand Yinfei Yang. 2021. Multi-stage training with im-\\nproved negative contrast for neural passage retrieval.\\nIn Proceedings of the 2021 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n6091–6103, Online and Punta Cana, Dominican Re-\\npublic. Association for Computational Linguistics.\\nMacedo Maia, Siegfried Handschuh, André Freitas,\\nBrian Davis, Ross McDermott, Manel Zarrouk, and\\nAlexandra Balahur. 2018. WWW’18 open challenge:\\nFinancial opinion mining and question answering. In\\nCompanion Proceedings of the The Web Conference\\n2018, WWW ’18, page 1941–1942, Republic and\\nCanton of Geneva, CHE. International World Wide\\nWeb Conferences Steering Committee.\\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\\nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\\n2022. Text and code embeddings by contrastive pre-\\ntraining. arXiv preprint arXiv:2201.10005.\\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant,\\nJi Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.\\nSentence-t5: Scalable sentence encoders from pre-\\ntrained text-to-text models. In Findings of the As-\\nsociation for Computational Linguistics: ACL 2022,\\npages 1864–1874.\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\\ntavo Hernández Ábrego, Ji Ma, Vincent Y Zhao,\\nYi Luan, Keith B Hall, Ming-Wei Chang, et al.\\n2021. Large dual encoders are generalizable retriev-\\ners. arXiv preprint arXiv:2112.07899.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam\\nLerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\\nAntiga, et al. 2019. Pytorch: An imperative style,\\nhigh-performance deep learning library. Advances in\\nneural information processing systems, 32.\\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\\nYacine Jernite, Vladimir Karpukhin, Jean Mail-\\nlard, et al. 2020.\\nKilt: a benchmark for knowl-\\nedge intensive language tasks.\\narXiv preprint\\narXiv:2009.02252.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\\nand Haifeng Wang. 2021. RocketQA: An optimized\\ntraining approach to dense passage retrieval for open-\\ndomain question answering. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, pages 5835–5847, On-\\nline. Association for Computational Linguistics.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2019. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. Journal of Machine Learning Research.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the parame-\\nters of a language model? In EMNLP.\\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\\nprobabilistic relevance framework: Bm25 and be-\\nyond. Foundations and Trends in Information Re-\\ntrieval, 3(4):333–389.\\nShaden Smith, Mostofa Patwary, Brandon Norick,\\nPatrick LeGresley, Samyam Rajbhandari, Jared\\nCasper, Zhun Liu, Shrimai Prabhumoye, George\\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\\nspeed and megatron to train megatron-turing nlg\\n530b, a large-scale generative language model. arXiv\\npreprint arXiv:2201.11990.\\nIan Soboroff, Shudong Huang, and Donna Harman.\\n2018. Trec 2018 news track overview.\\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\\nlum. 2020.\\nEnergy and policy considerations for\\nmodern deep learning research. In Proceedings of\\nthe AAAI Conference on Artiﬁcial Intelligence, vol-\\nume 34, pages 13693–13696.\\nAxel Suarez, Dyaa Albakour, David Corney, Miguel\\nMartinez, and José Esquivel. 2018. A data collection\\nfor evaluating the retrieval of related tweets to news\\narticles. In European Conference on Information\\nRetrieval, pages 780–786. Springer.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishek Srivastava, and Iryna Gurevych. 2021a. Beir:\\nA heterogenous benchmark for zero-shot evalua-\\ntion of information retrieval models. arXiv preprint\\narXiv:2104.08663.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishek Srivastava, and Iryna Gurevych. 2021b. BEIR:\\nA heterogenous benchmark for zero-shot evalua-\\ntion of information retrieval models. arXiv preprint\\narXiv:2104.08663.\\nJames\\nThorne,\\nAndreas\\nVlachos,\\nChristos\\nChristodoulopoulos,\\nand\\nArpit\\nMittal.\\n2018.\\nFEVER: a large-scale dataset for fact extraction\\nand VERiﬁcation.\\nIn Proceedings of the 2018\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers),\\npages 809–819, New Orleans, Louisiana. Association\\nfor Computational Linguistics.\\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\\nMichael R Alvers, Dirk Weissenborn, Anastasia\\nKrithara, Sergios Petridis, Dimitris Polychronopou-\\nlos, et al. 2015. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question an-\\nswering competition. BMC bioinformatics, 16(1):1–\\n28.\\nEllen Voorhees, Tasmeer Alam, Steven Bedrick, Dina\\nDemner-Fushman, William R. Hersh, Kyle Lo, Kirk\\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\\nTREC-COVID: Constructing a pandemic informa-\\ntion retrieval test collection. SIGIR Forum, 54(1).\\nEllen M Voorhees et al. 2004. Overview of the trec\\n2004 robust retrieval track. In Trec, pages 69–77.\\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein.\\n2018. Retrieval of the best counterargument without\\nprior topic knowledge. In Proceedings of the 56th\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 241–251,\\nMelbourne, Australia. Association for Computational\\nLinguistics.\\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\\nWang, Madeleine van Zuylen, Arman Cohan, and\\nHannaneh Hajishirzi. 2020. Fact or ﬁction: Verifying\\nscientiﬁc claims. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 7534–7550, Online. As-\\nsociation for Computational Linguistics.\\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna\\nGurevych. 2022. GPL: Generative pseudo labeling\\nfor unsupervised domain adaptation of dense retrieval.\\nIn Proceedings of the 2022 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nSeattle, United States. Association for Computational\\nLinguistics.\\nYining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei\\nChen, and Tie-Yan Liu. 2013. A theoretical analysis\\nof ndcg ranking measures. In Proceedings of the 26th\\nannual conference on learning theory (COLT 2013),\\nvolume 8, page 6. Citeseer.\\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\\nmand Joulin, and Edouard Grave. 2020.\\nCCNet:\\nExtracting high quality monolingual datasets from\\nweb crawl data. In Proceedings of the 12th Lan-\\nguage Resources and Evaluation Conference, pages\\n4003–4012, Marseille, France. European Language\\nResources Association.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\\nformers: State-of-the-art natural language processing.\\nIn Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations, pages 38–45, Online. Association\\nfor Computational Linguistics.\\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\\nshot dense retrieval with momentum adversarial do-\\nmain invariant representations. In Findings of the As-\\nsociation for Computational Linguistics: ACL 2022,\\npages 4008–4020, Dublin, Ireland. Association for\\nComputational Linguistics.\\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\\nSharma, Damien Jose, and Paul N Bennett. 2021.\\nZero-shot dense retrieval with momentum adversar-\\nial domain invariant representations. arXiv preprint\\narXiv:2110.07581.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\\nOverwijk. 2020. Approximate nearest neighbor nega-\\ntive contrastive learning for dense text retrieval. arXiv\\npreprint arXiv:2007.00808.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\\ngio, William W. Cohen, Ruslan Salakhutdinov, and\\nChristopher D. Manning. 2018.\\nHotpotQA: A\\nDataset for Diverse, Explainable Multi-hop Ques-\\ntion Answering. In Proceedings of the Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 2369–2380.\\nHongChien Yu, Chenyan Xiong, and Jamie Callan. 2021.\\nImproving query representations for dense retrieval\\nwith pseudo relevance feedback.\\narXiv preprint\\narXiv:2108.13454.\\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\\nArnold Overwijk. 2022. Coco-dr: Combating dis-\\ntribution shifts in zero-shot dense retrieval with con-\\ntrastive and distributionally robust learning. arXiv\\npreprint arXiv:2210.15212.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\\nOpt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068.\\nChen Zhao, Chenyan Xiong, Jordan Boyd-Graber, and\\nHal Daumé III. 2021. Distantly-supervised evidence\\nretrieval enables question answering without evidence\\nannotation. arXiv preprint arXiv:2110.04889.\\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\\ning language models with memory augmentation.\\narXiv preprint arXiv:2205.12674.\\nA\\nAppendix\\nA.1\\nDatasets Details\\nEvaluation Datasets\\nTarget domain datasets used\\nin our experiments are collected in the BEIR bench-\\nmark (Thakur et al., 2021b)4 and include the following\\ndomains:\\n• Open-domain Question Answering (QA): Hot-\\npotQA (Yang et al., 2018), NQ (Kwiatkowski et al.,\\n2019), and FiQA (Maia et al., 2018).\\n• Bio-Medical\\nInformation\\nRetrieval:\\nTREC-\\nCOVID (Voorhees et al., 2021), NFCorpus (Boteva\\net al., 2016), and BioASQ (Tsatsaronis et al., 2015).\\n• Argument Retrieval:\\nWebis-Touché2020 (Bon-\\ndarenko et al., 2020) and ArguAna (Wachsmuth et al.,\\n2018).\\n• News Retrieval: TREC-NEWS (Soboroff et al., 2018)\\nand Robust04 (Voorhees et al., 2004).\\n• Tweet Retrieval: Signal-1m (Suarez et al., 2018).\\n• Duplicate Question Retrieval: Quora (Thakur et al.,\\n2021b) and CQADupStack (Hoogeveen et al., 2015).\\n• Entity Retrieval: DBPedia (Hasibi et al., 2017)\\n• Citation Prediction: SCIDOCS (Cohan et al., 2020)\\n• Fact Checking:\\nSciFact (Wadden et al., 2020),\\nFEVER\\n(Thorne\\net\\nal.,\\n2018),\\nand\\nClimate-\\nFEVER (Diggelmann et al., 2020)\\nWe list the statistics of the BEIR benchmark in Table 7.\\nAugmenting Corpora\\nCorpus size We ﬁrst introduce\\nmore details on how we preprocessed the Medical Sub-\\nject Headings (MeSH) Database. We select text in-\\nformation from the Qualiﬁer Record Set and Descrip-\\ntor Record Set. Each set contains multiple <Concept>\\nelements, which is composed of three sub-elecments,\\ni.e., <ConceptName>, <ScopeNote> and <TermList>.\\nAmong the sub-elecments, <ScopeNote> is the major\\ntextual information source, which is usually a short de-\\nscription to a medical term or phenomenon. We directly\\nconsider each <ScopeNote> as a document entry and\\nconcatenate it with corresponding <ConceptName>.\\nWe list the statistics of the augmenting corpora in\\nTable 8.\\nA.2\\nBaselines\\nWe use the baselines from the current BEIR leader-\\nboard (Thakur et al., 2021b) and recent papers. These\\nbaselines can be divided into four groups: dense re-\\ntrieval, dense retrieval with generated queries5, lexical\\nretrieval and late interaction.\\n4https://github.com/beir-cellar/beir\\n5We separate them from dense retrieval since they usually\\nrely on Seq2seq models to generate pseudo query-document\\npairs, and they train a model for each dataset independently\\ninstead of using a single model for all datasets.\\nDense Retrieval\\nFor dense retrieval, the baselines\\nare the same dual-tower model as ours. We consider\\nDPR (Karpukhin et al., 2020), ANCE (Xiong et al.,\\n2020), T5-ANCE, coCondenser (Gao and Callan,\\n2022) and one recently-proposed model GTR (Ni et al.,\\n2021) with different size conﬁguration in this paper.\\n• DPR uses a single BM25 retrieval example and in-\\nbatch examples as hard negative examples to train\\nthe model. Different from the original paper (Thakur\\net al., 2021b) that train the DPR on QA datasets, we\\ntrain DPR on MS MARCO (Bajaj et al., 2016) Dataset\\nfor fair comparison. Notice that this also lead to better\\nresults according to Xin et al. (2022).\\n• ANCE constructs hard negative examples from an\\nANN index of the corpus. The hard negative training\\ninstances are updated in parallel during ﬁne-tuning of\\nthe model. The model is a RoBERTa (Liu et al., 2019)\\nmodel trained on MS MARCO for 600k steps.\\n• T5-ANCE Different with default ANCE setting, we\\nreplace the backbone language model RoBERTa with\\nT5-base. All the other model settings are the same\\nwith the original ANCE. We include this baseline\\nbecause as a subroutine for MoMA, it could be viewed\\nas an ablation without memory augmentation. We\\ncan directly observe the impact of plug-in mixture of\\nmemory by comparing T5-ANCE with MoMA.\\n• coCondenser is a continuous pre-trained model\\nbased on BERT, with the equivalent amount of param-\\neters to BERT-base. It enhances the representation\\nability of [CLS] token by changing the connections\\nbetween different layers of Transformer blocks. Fine-\\ntuning of coCondenser uses BM25 and self-mined\\nnegatives.\\n• Contriever conducts unsupervised contrastive pre-\\ntraining with data augmentations and momentum\\nqueues on Wikipedia and the larger CC-Net (Wen-\\nzek et al., 2020) corpora for 500k steps.\\n• GTR initializes the dual encoders from the T5 mod-\\nels (Raffel et al., 2019). It is ﬁrst pre-trained on Com-\\nmunity QA6 with 2 billion question-answer pairs then\\nﬁne-tuned on NQ and MS Marco dataset. In addition,\\nthey use the hard negatives released by RocketQA (Qu\\net al., 2021) when ﬁnetuning with MS Marco data and\\nthe hard negatives release by (Lu et al., 2021) for Nat-\\nural Questions. GTRbase leverages the same T5-base\\nmodel as MoMA, while GTRlarge is based on T5-large,\\nwhich is not directly comparable to our method as it\\ntriples the parameters.\\nDense Retrieval with Generated Queries\\nGenQ\\nﬁrst ﬁne-tunes a T5-base (Raffel et al., 2019) model on\\nMS MARCO for 2 epochs and then generate 5 queries\\n6Unfortunately, this corpus has not been released by the\\nauthors.\\nTable 7: Statistics of datasets in the BEIR benchmark. The table is taken from the original BEIR benchmark\\npaper (Thakur et al., 2021b).\\nSplit (→)\\nTrain\\nDev\\nTest\\nAvg. Word Lengths\\nTask (↓)\\nDomain (↓)\\nDataset (↓)\\nTitle\\nRelevancy\\n#Pairs\\n#Query\\n#Query\\n#Corpus\\nAvg. D / Q\\nQuery\\nDocument\\nPassage-Retrieval\\nMisc.\\nMS MARCO\\n\\x17\\nBinary\\n532,761\\n—-\\n6,980\\n8,841,823\\n1.1\\n5.96\\n55.98\\nBio-Medical\\nBio-Medical\\nTREC-COVID\\n\\x13\\n3-level\\n—-\\n—-\\n50\\n171,332\\n493.5\\n10.60\\n160.77\\nInformation\\nBio-Medical\\nNFCorpus\\n\\x13\\n3-level\\n110,575\\n324\\n323\\n3,633\\n38.2\\n3.30\\n232.26\\nRetrieval (IR)\\nBio-Medical\\nBioASQ\\n\\x13\\nBinary\\n32,916\\n—-\\n500\\n14,914,602\\n4.7\\n8.05\\n202.61\\nQuestion\\nWikipedia\\nNQ\\n\\x13\\nBinary\\n132,803\\n—-\\n3,452\\n2,681,468\\n1.2\\n9.16\\n78.88\\nAnswering\\nWikipedia\\nHotpotQA\\n\\x13\\nBinary\\n170,000\\n5,447\\n7,405\\n5,233,329\\n2.0\\n17.61\\n46.30\\n(QA)\\nFinance\\nFiQA-2018\\n\\x17\\nBinary\\n14,166\\n500\\n648\\n57,638\\n2.6\\n10.77\\n132.32\\nTweet-Retrieval\\nTwitter\\nSignal-1M (RT)\\n\\x17\\n3-level\\n—-\\n—-\\n97\\n2,866,316\\n19.6\\n9.30\\n13.93\\nNews\\nNews\\nTREC-NEWS\\n\\x13\\n5-level\\n—-\\n—-\\n57\\n594,977\\n19.6\\n11.14\\n634.79\\nRetrieval\\nNews\\nRobust04\\n\\x17\\n3-level\\n—-\\n—-\\n249\\n528,155\\n69.9\\n15.27\\n466.40\\nArgument\\nMisc.\\nArguAna\\n\\x13\\nBinary\\n—-\\n—-\\n1,406\\n8,674\\n1.0\\n192.98\\n166.80\\nRetrieval\\nMisc.\\nTouché-2020\\n\\x13\\n3-level\\n—-\\n—-\\n49\\n382,545\\n19.0\\n6.55\\n292.37\\nDuplicate-Question\\nStackEx.\\nCQADupStack\\n\\x13\\nBinary\\n—-\\n—-\\n13,145\\n457,199\\n1.4\\n8.59\\n129.09\\nRetrieval\\nQuora\\nQuora\\n\\x17\\nBinary\\n—-\\n5,000\\n10,000\\n522,931\\n1.6\\n9.53\\n11.44\\nEntity-Retrieval\\nWikipedia\\nDBPedia\\n\\x13\\n3-level\\n—-\\n67\\n400\\n4,635,922\\n38.2\\n5.39\\n49.68\\nCitation-Prediction\\nScientiﬁc\\nSCIDOCS\\n\\x13\\nBinary\\n—-\\n—-\\n1,000\\n25,657\\n4.9\\n9.38\\n176.19\\nWikipedia\\nFEVER\\n\\x13\\nBinary\\n140,085\\n6,666\\n6,666\\n5,416,568\\n1.2\\n8.13\\n84.76\\nFact Checking\\nWikipedia\\nClimate-FEVER\\n\\x13\\nBinary\\n—-\\n—-\\n1,535\\n5,416,593\\n3.0\\n20.13\\n84.76\\nScientiﬁc\\nSciFact\\n\\x13\\nBinary\\n920\\n—-\\n300\\n5,183\\n1.1\\n12.37\\n213.63\\nTable 8: Statistics of the augmenting corpora.\\nDatasets\\nCorpus Size\\nAvg. Doc Length\\nMS MARCO\\n502,939\\n56.0\\nMeSH\\n32,326\\n16.8\\nWiki\\n21,015,324\\n100.0\\nfor each passage as additional training data for the target\\ndomain to continue to ﬁne-tune the TAS-B (Hofstätter\\net al., 2021) model.\\nLexical Retrieval\\nLexical retrieval is a score func-\\ntion for token matching calculated between two\\nhigh-dimensional sparse vectors with token weights.\\nBM25 (Robertson et al., 2009) is the most commonly\\nused lexical retrieval function. We use the BM25 results\\nreported in Thakur et al. (2021b) for comparison.\\nLate Interaction\\nWe also consider a late interac-\\ntion baseline, namely ColBERT (Khattab and Zaharia,\\n2020b). The model computes multiple contextualized\\nembeddings for each token of queries and documents,\\nand then uses a maximum similarity function to retrieve\\nrelevant documents. This type of matching requires sig-\\nniﬁcantly more disk space for indexes and has a higher\\nlatency.\\nA.3\\nDetailed Experimental Settings and\\nhyperparameters\\nOur implementation uses PyTorch (Paszke et al., 2019)\\nwith Hugging Face Transformers (Wolf et al., 2020).\\nWe optimize the model using AdamW (Loshchilov and\\nHutter, 2019) with a peak learning rate at 5e-6, weight\\ndecay of 0.01, and linear learning rate decay. The global\\nbatch size is set to 256. The maximum length of query\\nand passage are set to 32 and 128 respectively. We\\nsummarize all hyperparameter settings in Table 9. The\\nmodel is trained with 8 Nvidia A100 80GB GPUs and\\nTable 9: The hyperparameters of MoMA.\\nHyperparameters\\nSettings\\nGrounding document number\\n10\\nAttention threshold number\\n5\\nNegative mining depth\\n200\\nGlobal batch size (query size per batch)\\n256\\nPositive number per query\\n1\\nNegative number per query\\n7\\nPeak learnig rate\\n5e-6\\nLearnig rate decay\\n0.01\\nOptimizer\\nAdamW\\nScheduler\\nLinear\\nMARCO Maximum query length\\n32\\nMARCO Maximum document length\\n128\\nFP16 mixed-precision training. The total running time\\nis 6.6 hrs for three episodes of augmentation component\\ntraining and 6.3 hrs for end retriever training. We detail\\nthe training time of each episode in Table 10.\\nWhen evaluating on the BEIR benchmark, we fol-\\nlow the setting in GTR (Ni et al., 2021), which use\\nsequences of 64 tokens for the questions and 512 for the\\ndocuments in all datasets except Trec-News, Robust-04\\nand ArguAna. In particular, we set the document length\\nto 768 for Trec-News and Robust-04. For ArguAna,\\nwe set both question and document length to 128. The\\nabove length setting is in accordance to the average\\nquery and document lengths in these datasets.\\nTable 10: Training time for MoMA with three training\\nepisodes. We use 8 Nvidia A100 80GB GPUs with\\nFP16 mixed-precision training.\\nStage\\nAugmentation Component\\nEnd Retriever\\nEpi-1\\n0.8h\\n1.5h\\nEpi-2\\n0.8h\\n1.5h\\nEpi-3\\n0.8h\\n1.5h\\nIndex refresh\\n1.4h\\n0.6h\\nRefresh number\\n3\\n3\\nOverall\\n6.6h\\n6.3h\\n', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an Index\n",
    "use a naive index creation strategy of just using RecursiveCharacterTextSplitter on the documents and embedding each into the VectorStore using OpenAIEmbeddings()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meron/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x7fe140bbfc10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Instantiate OpenAIEmbeddings \n",
    "openai_embeddings = OpenAIEmbeddings(openai_api_key='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')\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "\n",
    "# Split the documents \n",
    "docs = text_splitter.split_documents(base_docs)\n",
    "\n",
    "# Create the Chroma vectorstore with the documents and the OpenAI embeddings instance\n",
    "vectorstore = Chroma.from_documents(docs, openai_embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4756"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "print(max([len(chunk.page_content) for chunk in docs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert our Chroma vectorstore into a retriever with the .as_retriever() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to give it a test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = base_retriever.get_relevant_documents(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Retrieval Augmented Generation Prompt\n",
    "we can set up a prompt template that will be used to provide the LLM with the necessary contexts, user query, and instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\\n\\n### CONTEXT\\n{context}\\n\\n### QUESTION\\nQuestion: {question}\\n\"))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the Basic QA Chain\n",
    "instantiate the basic RAG chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meron/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "api_key = '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'\n",
    "\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=api_key)\n",
    "\n",
    "\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': AIMessage(content='RAG stands for Retrieval Augmented Generation.'), 'context': [Document(page_content='2020). RAG consists of three primary components:\\nTool Retrieval, Plan Generation, and Execution.1\\nIn this study, we focus on enhancing tool retrieval,\\nwith the goal of achieving subsequent improve-\\nments in plan generation.', metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}), Document(page_content='markable ability to solve new tasks with just a\\nfew examples, but they need access to the right\\ntools. Retrieval Augmented Generation (RAG)\\naddresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG’s', metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'})]}\n"
     ]
    }
   ],
   "source": [
    "question = \"What is RAG?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground Truth Dataset Creation Using GPT-3.5-turbo and GPT-4\n",
    "the evaluation dataset is provided.\n",
    "\n",
    "The basic idea is that we can use LangChain to create questions based on our contexts, and then answer those questions.\n",
    "\n",
    "Let's look at how that works in the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"a question about the context.\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "format_instructions = question_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Replace with your actual OpenAI API key\n",
    "api_key = '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'\n",
    "\n",
    "question_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", openai_api_key=api_key)\n",
    "bare_prompt_template = \"{content}\"\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\n",
    "\n",
    "question: a question about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "question\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=docs[0],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "output_dict = question_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "What is the main focus of this paper?\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "qac_triples = []\n",
    "\n",
    "for text in tqdm(docs[:10]):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=text,\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = question_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = question_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  output_dict[\"context\"] = text\n",
    "  qac_triples.append(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the focus of this paper?',\n",
       " 'context': Document(page_content='thisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community.\\nCompared', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'})}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qac_triples[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "# Other necessary imports...\n",
    "\n",
    "api_key = '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'\n",
    "\n",
    "answer_generation_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=api_key)\n",
    "\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=qac_triples[0][\"context\"],\n",
    "    question=qac_triples[0][\"question\"],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n",
    "\n",
    "response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "output_dict = answer_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "The advantages of retrieval-augmented text generation compared to conventional generation models include the ability to incorporate external knowledge, which can lead to more informative and contextually relevant outputs. This approach has achieved state-of-the-art performance in various NLP tasks by leveraging retrieved information to enhance the generation process. Additionally, it can improve the diversity and factual accuracy of the generated text, as it can draw upon a wide range of sources for content enrichment.\n",
      "question\n",
      "What are the advantages of retrieval-augmented text generation compared to conventional generation models?\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:55<00:00,  5.59s/it]\n"
     ]
    }
   ],
   "source": [
    "for triple in tqdm(qac_triples):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=triple[\"context\"],\n",
    "      question=triple[\"question\"],\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = answer_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  triple[\"answer\"] = output_dict[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meron/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
    "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
    "\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'ground_truth'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the advantages of retrieval-augmented text generation compared to conventional generation models?',\n",
       " 'context': 'A Survey on Retrieval-Augmented Text Generation\\nHuayang Li♥,∗\\nYixuan Su♠,∗\\nDeng Cai♦,∗\\nYan Wang♣,∗\\nLemao Liu♣,∗\\n♥Nara Institute of Science and Technology\\n♠University of Cambridge\\n♦The Chinese University of Hong Kong\\n♣Tencent AI Lab',\n",
       " 'ground_truth': 'The advantages of retrieval-augmented text generation compared to conventional generation models include the ability to incorporate external knowledge, which can lead to more informative and contextually relevant outputs. This approach has shown state-of-the-art performance in various NLP tasks by leveraging retrieved content to enhance the generation process. Additionally, it can improve the diversity and factual accuracy of the generated text, as it can draw upon a wide range of sources for information.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.55ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7836"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_csv(\"groundtruth_eval_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# eval_dataset = Dataset.from_csv(\"groundtruth_eval_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'ground_truth'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Using RAGAS\n",
    "\n",
    "The set-up is fairly straightforward - we simply need to create a dataset with our generated answers and our contexts, and then evaluate using the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "openai.api_key=OPENAI_API_KEY\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "# openai_embeddings = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
    "  rag_dataset = []\n",
    "  for row in tqdm(eval_dataset):\n",
    "    answer = rag_pipeline.invoke({\"question\" : row[\"question\"]})\n",
    "    rag_dataset.append(\n",
    "        {\"question\" : row[\"question\"],\n",
    "         \"answer\" : answer[\"response\"].content,\n",
    "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
    "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
    "         }\n",
    "    )\n",
    "  rag_df = pd.DataFrame(rag_dataset)\n",
    "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "  return rag_eval_dataset\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
